{"instance_id": "astropy__astropy-6938", "found_files": ["astropy/io/fits/fitsrec.py", "astropy/io/fits/column.py", "astropy/io/fits/util.py"], "additional_artifact_loc_file": {}, "file_traj": {}, "found_related_locs": {"astropy/io/fits/fitsrec.py": ["function: FITS_rec._scale_back_ascii\nfunction: encode_ascii"], "astropy/io/fits/column.py": [""], "astropy/io/fits/util.py": [""]}, "additional_artifact_loc_related": [{"raw_output_loc": "```\nastropy/io/fits/fitsrec.py\nfunction: FITS_rec._scale_back_ascii\nfunction: encode_ascii\n```"}], "related_loc_traj": [{"response": "```\nastropy/io/fits/fitsrec.py\nfunction: FITS_rec._scale_back_ascii\nfunction: encode_ascii\n```", "usage": {"completion_tokens": 29, "prompt_tokens": 4629}, "prompt": "\nPlease look through the following GitHub Problem Description and the Skeleton of Relevant Files.\nIdentify all locations that need inspection or editing to fix the problem, including directly related areas as well as any potentially related global variables, functions, and classes.\nFor each location you provide, either give the name of the class, the name of a method in a class, the name of a function, or the name of a global variable.\n\n### GitHub Problem Description ###\nPossible bug in io.fits related to D exponents\nI came across the following code in ``fitsrec.py``:\r\n\r\n```python\r\n        # Replace exponent separator in floating point numbers\r\n        if 'D' in format:\r\n            output_field.replace(encode_ascii('E'), encode_ascii('D'))\r\n```\r\n\r\nI think this may be incorrect because as far as I can tell ``replace`` is not an in-place operation for ``chararray`` (it returns a copy). Commenting out this code doesn't cause any tests to fail so I think this code isn't being tested anyway.\n\n\n### Skeleton of Relevant Files ###\n\n### File: astropy/io/fits/fitsrec.py ###\n```python\n# Licensed under a 3-clause BSD style license - see PYFITS.rst\n\n\n\nclass FITS_record:\n\n    def __init__(self, input, row=0, start=None, end=None, step=None,\n                 base=None, **kwargs):\n...\n\n    def __getitem__(self, key):\n...\n\n    def __setitem__(self, key, value):\n...\n\n    def __len__(self):\n...\n\n    def __repr__(self):\n...\n\n    def field(self, field):\n...\n\n    def setfield(self, field, value):\n...\n\n    @lazyproperty\n    def _bases(self):\n...\n\n    def _get_index(self, indx):\n...\n\n\nclass FITS_rec(np.recarray):\n\n    _record_type = FITS_record\n    _character_as_bytes = False\n\n    def __new__(subtype, input):\n...\n\n    def __setstate__(self, state):\n...\n\n    def __reduce__(self):\n...\n\n    def __array_finalize__(self, obj):\n...\n\n    def _init(self):\n...\n\n    @classmethod\n    def from_columns(cls, columns, nrows=0, fill=False, character_as_bytes=False):\n...\n\n    def __repr__(self):\n...\n\n    def __getitem__(self, key):\n...\n\n    def __setitem__(self, key, value):\n...\n\n    def copy(self, order='C'):\n...\n\n    @property\n    def columns(self):\n...\n\n    @property\n    def _coldefs(self):\n...\n\n    @_coldefs.setter\n    def _coldefs(self, cols):\n...\n\n    @_coldefs.deleter\n    def _coldefs(self):\n...\n\n    def __del__(self):\n...\n\n    @property\n    def names(self):\n...\n\n    @property\n    def formats(self):\n...\n\n    @property\n    def _raw_itemsize(self):\n...\n\n    def field(self, key):\n...\n\n    def _cache_field(self, name, field):\n...\n\n    def _update_column_attribute_changed(self, column, idx, attr, old_value,\n                                         new_value):\n...\n\n    def _update_column_name(self, column, idx, old_name, name):\n...\n\n    def _convert_x(self, field, recformat):\n...\n\n    def _convert_p(self, column, field, recformat):\n...\n\n    def _convert_ascii(self, column, field):\n...\n\n    def _convert_other(self, column, field, recformat):\n...\n\n    def _get_heap_data(self):\n...\n\n    def _get_raw_data(self):\n...\n\n    def _get_scale_factors(self, column):\n...\n\n    def _scale_back(self, update_heap_pointers=True):\n...\n\n    def _scale_back_strings(self, col_idx, input_field, output_field):\n...\n\n    def _scale_back_ascii(self, col_idx, input_field, output_field):\n...\n\n\ndef _get_recarray_field(array, key):\n...\n\n\nclass _UnicodeArrayEncodeError(UnicodeEncodeError):\n    def __init__(self, encoding, object_, start, end, reason, index):\n...\n\n\ndef _ascii_encode(inarray, out=None):\n...\n\n\ndef _has_unicode_fields(array):\n...\n\n```\n\n### File: astropy/io/fits/column.py ###\n```python\n# Licensed under a 3-clause BSD style license - see PYFITS.rst\n\n\n__all__ = ['Column', 'ColDefs', 'Delayed']\n\n\n# mapping from TFORM data type to numpy data type (code)\n# L: Logical (Boolean)\n# B: Unsigned Byte\n# I: 16-bit Integer\n# J: 32-bit Integer\n# K: 64-bit Integer\n# E: Single-precision Floating Point\n# D: Double-precision Floating Point\n# C: Single-precision Complex\n# M: Double-precision Complex\n# A: Character\nFITS2NUMPY = {'L': 'i1', 'B': 'u1', 'I': 'i2', 'J': 'i4', 'K': 'i8', 'E': 'f4',\n              'D': 'f8', 'C': 'c8', 'M': 'c16', 'A': 'a'}\n\n# the inverse dictionary of the above\nNUMPY2FITS = {val: key for key, val in FITS2NUMPY.items()}\n# Normally booleans are represented as ints in Astropy, but if passed in a numpy\n# boolean array, that should be supported\nNUMPY2FITS['b1'] = 'L'\n# Add unsigned types, which will be stored as signed ints with a TZERO card.\nNUMPY2FITS['u2'] = 'I'\nNUMPY2FITS['u4'] = 'J'\nNUMPY2FITS['u8'] = 'K'\n# Add half precision floating point numbers which will be up-converted to\n# single precision.\nNUMPY2FITS['f2'] = 'E'\n\n# This is the order in which values are converted to FITS types\n# Note that only double precision floating point/complex are supported\nFORMATORDER = ['L', 'B', 'I', 'J', 'K', 'D', 'M', 'A']\n\n# Convert single precision floating point/complex to double precision.\nFITSUPCONVERTERS = {'E': 'D', 'C': 'M'}\n\n# mapping from ASCII table TFORM data type to numpy data type\n# A: Character\n# I: Integer (32-bit)\n# J: Integer (64-bit; non-standard)\n# F: Float (64-bit; fixed decimal notation)\n# E: Float (64-bit; exponential notation)\n# D: Float (64-bit; exponential notation, always 64-bit by convention)\nASCII2NUMPY = {'A': 'a', 'I': 'i4', 'J': 'i8', 'F': 'f8', 'E': 'f8', 'D': 'f8'}\n\n# Maps FITS ASCII column format codes to the appropriate Python string\n# formatting codes for that type.\nASCII2STR = {'A': '', 'I': 'd', 'J': 'd', 'F': 'f', 'E': 'E', 'D': 'E'}\n\n# For each ASCII table format code, provides a default width (and decimal\n# precision) for when one isn't given explicitly in the column format\nASCII_DEFAULT_WIDTHS = {'A': (1, 0), 'I': (10, 0), 'J': (15, 0),\n                        'E': (15, 7), 'F': (16, 7), 'D': (25, 17)}\n\n\n# tuple of column/field definition common names and keyword names, make\n# sure to preserve the one-to-one correspondence when updating the list(s).\n# Use lists, instead of dictionaries so the names can be displayed in a\n# preferred order.\nKEYWORD_NAMES = ('TTYPE', 'TFORM', 'TUNIT', 'TNULL', 'TSCAL', 'TZERO',\n                 'TDISP', 'TBCOL', 'TDIM', 'TCTYP', 'TCUNI', 'TCRPX',\n                 'TCRVL', 'TCDLT', 'TRPOS')\nKEYWORD_ATTRIBUTES = ('name', 'format', 'unit', 'null', 'bscale', 'bzero',\n                      'disp', 'start', 'dim', 'coord_type', 'coord_unit',\n                      'coord_ref_point', 'coord_ref_value', 'coord_inc',\n                      'time_ref_pos')\n\n\nKEYWORD_TO_ATTRIBUTE = OrderedDict(zip(KEYWORD_NAMES, KEYWORD_ATTRIBUTES))\n\nATTRIBUTE_TO_KEYWORD = OrderedDict(zip(KEYWORD_ATTRIBUTES, KEYWORD_NAMES))\n\n\n# TODO: Define a list of default comments to associate with each table keyword\n\n# TFORMn regular expression\nTFORMAT_RE = re.compile(r'(?P<repeat>^[0-9]*)(?P<format>[LXBIJKAEDCMPQ])'\n                        r'(?P<option>[!-~]*)', re.I)\n\n# TFORMn for ASCII tables; two different versions depending on whether\n# the format is floating-point or not; allows empty values for width\n# in which case defaults are used\nTFORMAT_ASCII_RE = re.compile(r'(?:(?P<format>[AIJ])(?P<width>[0-9]+)?)|'\n                              r'(?:(?P<formatf>[FED])'\n                              r'(?:(?P<widthf>[0-9]+)\\.'\n                              r'(?P<precision>[0-9]+))?)')\n\nTTYPE_RE = re.compile(r'[0-9a-zA-Z_]+')\n\n# table definition keyword regular expression\nTDEF_RE = re.compile(r'(?P<label>^T[A-Z]*)(?P<num>[1-9][0-9 ]*$)')\n\n# table dimension keyword regular expression (fairly flexible with whitespace)\nTDIM_RE = re.compile(r'\\(\\s*(?P<dims>(?:\\d+,\\s*)+\\s*\\d+)\\s*\\)\\s*')\n\n# value for ASCII table cell with value = TNULL\n# this can be reset by user.\nASCIITNULL = 0\n\n# The default placeholder to use for NULL values in ASCII tables when\n# converting from binary to ASCII tables\nDEFAULT_ASCII_TNULL = '---'\n\n\nclass Delayed:\n\n    def __init__(self, hdu=None, field=None):\n...\n\n    def __getitem__(self, key):\n...\n\n\nclass _BaseColumnFormat(str):\n\n    def __eq__(self, other):\n...\n\n    def __hash__(self):\n...\n\n    @lazyproperty\n    def dtype(self):\n...\n\n    @classmethod\n    def from_column_format(cls, format):\n...\n\n\nclass _ColumnFormat(_BaseColumnFormat):\n\n    def __new__(cls, format):\n...\n\n    @classmethod\n    def from_recformat(cls, recformat):\n...\n\n    @lazyproperty\n    def recformat(self):\n...\n\n    @lazyproperty\n    def canonical(self):\n...\n\n\nclass _AsciiColumnFormat(_BaseColumnFormat):\n\n    def __new__(cls, format, strict=False):\n...\n\n    @classmethod\n    def from_column_format(cls, format):\n...\n\n    @classmethod\n    def from_recformat(cls, recformat):\n...\n\n    @lazyproperty\n    def recformat(self):\n...\n\n    @lazyproperty\n    def canonical(self):\n...\n\n\nclass _FormatX(str):\n\n    def __new__(cls, repeat=1):\n...\n\n    def __getnewargs__(self):\n...\n\n    @property\n    def tform(self):\n...\n\n\n# TODO: Table column formats need to be verified upon first reading the file;\n# as it is, an invalid P format will raise a VerifyError from some deep,\n# unexpected place\nclass _FormatP(str):\n\n    # As far as I can tell from my reading of the FITS standard, a type code is\n    # *required* for P and Q formats; there is no default\n    _format_re_template = (r'(?P<repeat>\\d+)?{}(?P<dtype>[LXBIJKAEDCM])'\n                           r'(?:\\((?P<max>\\d*)\\))?')\n    _format_code = 'P'\n    _format_re = re.compile(_format_re_template.format(_format_code))\n    _descriptor_format = '2i4'\n\n    def __new__(cls, dtype, repeat=None, max=None):\n...\n\n    def __getnewargs__(self):\n...\n\n    @classmethod\n    def from_tform(cls, format):\n...\n\n    @property\n    def tform(self):\n...\n\n\nclass _FormatQ(_FormatP):\n\n    _format_code = 'Q'\n    _format_re = re.compile(_FormatP._format_re_template.format(_format_code))\n    _descriptor_format = '2i8'\n\n\nclass ColumnAttribute:\n\n    def __init__(self, keyword):\n...\n\n    def __get__(self, obj, objtype=None):\n...\n\n    def __set__(self, obj, value):\n...\n\n    def __call__(self, func):\n...\n\n    def __repr__(self):\n...\n\n\nclass Column(NotifierMixin):\n\n    def __init__(self, name=None, format=None, unit=None, null=None,\n                 bscale=None, bzero=None, disp=None, start=None, dim=None,\n                 array=None, ascii=None, coord_type=None, coord_unit=None,\n                 coord_ref_point=None, coord_ref_value=None, coord_inc=None,\n                 time_ref_pos=None):\n...\n\n    def __repr__(self):\n...\n\n    def __eq__(self, other):\n...\n\n    def __hash__(self):\n...\n\n    @property\n    def array(self):\n...\n\n    @array.setter\n    def array(self, array):\n...\n\n    @array.deleter\n    def array(self):\n...\n\n    @ColumnAttribute('TTYPE')\n    def name(col, name):\n...\n\n    @ColumnAttribute('TCTYP')\n    def coord_type(col, coord_type):\n...\n\n    @ColumnAttribute('TCUNI')\n    def coord_unit(col, coord_unit):\n...\n\n    @ColumnAttribute('TCRPX')\n    def coord_ref_point(col, coord_ref_point):\n...\n\n    @ColumnAttribute('TCRVL')\n    def coord_ref_value(col, coord_ref_value):\n...\n\n    @ColumnAttribute('TCDLT')\n    def coord_inc(col, coord_inc):\n...\n\n    @ColumnAttribute('TRPOS')\n    def time_ref_pos(col, time_ref_pos):\n...\n\n    format = ColumnAttribute('TFORM')\n    unit = ColumnAttribute('TUNIT')\n    null = ColumnAttribute('TNULL')\n    bscale = ColumnAttribute('TSCAL')\n    bzero = ColumnAttribute('TZERO')\n    disp = ColumnAttribute('TDISP')\n    start = ColumnAttribute('TBCOL')\n    dim = ColumnAttribute('TDIM')\n\n    @lazyproperty\n    def ascii(self):\n...\n\n    @lazyproperty\n    def dtype(self):\n...\n\n    def copy(self):\n...\n\n    @staticmethod\n    def _convert_format(format, cls):\n...\n\n    @classmethod\n    def _verify_keywords(cls, name=None, format=None, unit=None, null=None,\n                         bscale=None, bzero=None, disp=None, start=None,\n                         dim=None, ascii=None, coord_type=None, coord_unit=None,\n                         coord_ref_point=None, coord_ref_value=None,\n                         coord_inc=None, time_ref_pos=None):\n...\n\n    @classmethod\n    def _determine_formats(cls, format, start, dim, ascii):\n...\n\n    @classmethod\n    def _guess_format(cls, format, start, dim):\n...\n\n    def _convert_to_valid_data_type(self, array):\n...\n\n\nclass ColDefs(NotifierMixin):\n\n    _padding_byte = '\\x00'\n    _col_format_cls = _ColumnFormat\n\n    def __new__(cls, input, ascii=False):\n...\n\n    def __getnewargs__(self):\n...\n\n    def __init__(self, input, ascii=False):\n...\n\n    def _init_from_coldefs(self, coldefs):\n...\n\n    def _init_from_sequence(self, columns):\n...\n\n    def _init_from_array(self, array):\n...\n\n    def _init_from_table(self, table):\n...\n\n    def __copy__(self):\n...\n\n    def __deepcopy__(self, memo):\n...\n\n    def _copy_column(self, column):\n...\n\n    def __getattr__(self, name):\n...\n\n    @lazyproperty\n    def dtype(self):\n...\n\n    @lazyproperty\n    def names(self):\n...\n\n    @lazyproperty\n    def formats(self):\n...\n\n    @lazyproperty\n    def _arrays(self):\n...\n\n    @lazyproperty\n    def _recformats(self):\n...\n\n    @lazyproperty\n    def _dims(self):\n...\n\n    def __getitem__(self, key):\n...\n\n    def __len__(self):\n...\n\n    def __repr__(self):\n...\n\n    def __add__(self, other, option='left'):\n...\n\n    def __radd__(self, other):\n...\n\n    def __sub__(self, other):\n...\n\n    def _update_column_attribute_changed(self, column, attr, old_value,\n                                         new_value):\n...\n\n    def add_col(self, column):\n...\n\n    def del_col(self, col_name):\n...\n\n    def change_attrib(self, col_name, attrib, new_value):\n...\n\n    def change_name(self, col_name, new_name):\n...\n\n    def change_unit(self, col_name, new_unit):\n...\n\n    def info(self, attrib='all', output=None):\n...\n\n\nclass _AsciiColDefs(ColDefs):\n\n    _padding_byte = ' '\n    _col_format_cls = _AsciiColumnFormat\n\n    def __init__(self, input, ascii=True):\n...\n\n    @lazyproperty\n    def dtype(self):\n...\n\n    @property\n    def spans(self):\n...\n\n    @lazyproperty\n    def _recformats(self):\n...\n\n    def add_col(self, column):\n...\n\n    def del_col(self, col_name):\n...\n\n    def _update_field_metrics(self):\n...\n\n\n# Utilities\n\n\nclass _VLF(np.ndarray):\n\n    def __new__(cls, input, dtype='a'):\n...\n\n    def __array_finalize__(self, obj):\n...\n\n    def __setitem__(self, key, value):\n...\n\n\ndef _get_index(names, key):\n...\n\n\ndef _unwrapx(input, output, repeat):\n...\n\n\ndef _wrapx(input, output, repeat):\n...\n\n\ndef _makep(array, descr_output, format, nrows=None):\n...\n\n\ndef _parse_tformat(tform):\n...\n\n\ndef _parse_ascii_tformat(tform, strict=False):\n...\n\n\ndef _parse_tdim(tdim):\n...\n\n\ndef _scalar_to_format(value):\n...\n\n\ndef _cmp_recformats(f1, f2):\n...\n\n\ndef _convert_fits2record(format):\n...\n\n\ndef _convert_record2fits(format):\n...\n\n\ndef _dtype_to_recformat(dtype):\n...\n\n\ndef _convert_format(format, reverse=False):\n...\n\n\ndef _convert_ascii_format(format, reverse=False):\n...\n\n```\n\n### File: astropy/io/fits/util.py ###\n```python\n# Licensed under a 3-clause BSD style license - see PYFITS.rst\n\n\n\ncmp = lambda a, b: (a > b) - (a < b)\n\nall_integer_types = (int, np.integer)\n\n\nclass NotifierMixin:\n\n    _listeners = None\n\n    def _add_listener(self, listener):\n...\n\n    def _remove_listener(self, listener):\n...\n\n    def _notify(self, notification, *args, **kwargs):\n...\n\n    def __getstate__(self):\n...\n\n\ndef first(iterable):\n...\n\n\ndef itersubclasses(cls, _seen=None):\n...\n\n\ndef ignore_sigint(func):\n...\n\n\ndef pairwise(iterable):\n...\n\n\ndef encode_ascii(s):\n...\n\n\ndef decode_ascii(s):\n...\n\n\ndef isreadable(f):\n...\n\n\ndef iswritable(f):\n...\n\n\ndef isfile(f):\n...\n\n\ndef fileobj_open(filename, mode):\n...\n\n\ndef fileobj_name(f):\n...\n\n\ndef fileobj_closed(f):\n...\n\n\ndef fileobj_mode(f):\n...\n\n\ndef _fileobj_normalize_mode(f):\n...\n\n\ndef fileobj_is_binary(f):\n...\n\n\ndef translate(s, table, deletechars):\n...\n\n\ndef fill(text, width, **kwargs):\n...\n\n\n# On MacOS X 10.8 and earlier, there is a bug that causes numpy.fromfile to\n# fail when reading over 2Gb of data. If we detect these versions of MacOS X,\n# we can instead read the data in chunks. To avoid performance penalties at\n# import time, we defer the setting of this global variable until the first\n# time it is needed.\nCHUNKED_FROMFILE = None\n\n\ndef _array_from_file(infile, dtype, count):\n...\n\n\n_OSX_WRITE_LIMIT = (2 ** 32) - 1\n_WIN_WRITE_LIMIT = (2 ** 31) - 1\n\n\ndef _array_to_file(arr, outfile):\n...\n\n\ndef _array_to_file_like(arr, fileobj):\n...\n\n\ndef _write_string(f, s):\n...\n\n\ndef _convert_array(array, dtype):\n...\n\n\ndef _unsigned_zero(dtype):\n...\n\n\ndef _is_pseudo_unsigned(dtype):\n...\n\n\ndef _is_int(val):\n...\n\n\ndef _str_to_num(val):\n...\n\n\ndef _words_group(input, strlen):\n...\n\n\ndef _tmp_name(input):\n...\n\n\ndef _get_array_mmap(array):\n...\n\n\n@contextmanager\ndef _free_space_check(hdulist, dirname=None):\n...\n\n\ndef _extract_number(value, default):\n...\n\n\ndef get_testdata_filepath(filename):\n...\n\n\ndef _rstrip_inplace(array):\n...\n\n```\n\n\n###\n\nPlease provide the complete set of locations as either a class name, a function name, or a variable name.\nNote that if you include a class, you do not need to list its specific methods.\nYou can include either the entire class or don't include the class name and instead include specific methods in the class.\n### Examples:\n```\nfull_path1/file1.py\nfunction: my_function_1\nclass: MyClass1\nfunction: MyClass2.my_method\n\nfull_path2/file2.py\nvariable: my_var\nfunction: MyClass3.my_method\n\nfull_path3/file3.py\nfunction: my_function_2\nfunction: my_function_3\nfunction: MyClass4.my_method_1\nclass: MyClass5\n```\n\nReturn just the locations wrapped with ```.\n"}], "found_edit_locs": {"astropy/io/fits/fitsrec.py": ["\nfunction: FitsRec._scale_back_ascii\nline: 1264"]}, "additional_artifact_loc_edit_location": [{"raw_output_loc": ["```\nastropy/io/fits/fitsrec.py\nfunction: FitsRec._scale_back_ascii\nline: 1264\n```", "```\nastropy/io/fits/fitsrec.py\nfunction: _scale_back_ascii\nline: 1264\n```", "```\nastropy/io/fits/fitsrec.py\nfunction: FITS_rec._scale_back_ascii\nline: 1264\n```", "```\nastropy/io/fits/fitsrec.py\nline: 1264\n```"]}], "edit_loc_traj": {"prompt": "\nPlease review the following GitHub problem description and relevant files, and provide a set of locations that need to be edited to fix the issue.\nThe locations can be specified as class names, function or method names, or exact line numbers that require modification.\n\n### GitHub Problem Description ###\nPossible bug in io.fits related to D exponents\nI came across the following code in ``fitsrec.py``:\r\n\r\n```python\r\n        # Replace exponent separator in floating point numbers\r\n        if 'D' in format:\r\n            output_field.replace(encode_ascii('E'), encode_ascii('D'))\r\n```\r\n\r\nI think this may be incorrect because as far as I can tell ``replace`` is not an in-place operation for ``chararray`` (it returns a copy). Commenting out this code doesn't cause any tests to fail so I think this code isn't being tested anyway.\n\n\n###\n### astropy/io/fits/fitsrec.py\n...\n1186|            # Otherwise go ahead and do a direct copy into--if both are type\n1187|            # 'U' we'll handle encoding later\n1188|            input_field = input_field.flatten().view(output_field.dtype)\n1189|            output_field.flat[:] = input_field\n1190|\n1191|        # Ensure that blanks at the end of each string are\n1192|        # converted to nulls instead of spaces, see Trac #15\n1193|        # and #111\n1194|        _rstrip_inplace(output_field)\n1195|\n1196|    def _scale_back_ascii(self, col_idx, input_field, output_field):\n1197|        \"\"\"\n1198|        Convert internal array values back to ASCII table representation.\n1199|\n1200|        The ``input_field`` is the internal representation of the values, and\n1201|        the ``output_field`` is the character array representing the ASCII\n1202|        output that will be written.\n1203|        \"\"\"\n1204|\n1205|        starts = self._coldefs.starts[:]\n1206|        spans = self._coldefs.spans\n1207|        format = self._coldefs[col_idx].format\n1208|\n1209|        # The the index of the \"end\" column of the record, beyond\n1210|        # which we can't write\n1211|        end = super().field(-1).itemsize\n1212|        starts.append(end + starts[-1])\n1213|\n1214|        if col_idx > 0:\n1215|            lead = starts[col_idx] - starts[col_idx - 1] - spans[col_idx - 1]\n1216|        else:\n1217|            lead = 0\n1218|\n1219|        if lead < 0:\n1220|            warnings.warn('Column {!r} starting point overlaps the previous '\n1221|                          'column.'.format(col_idx + 1))\n1222|\n1223|        trail = starts[col_idx + 1] - starts[col_idx] - spans[col_idx]\n1224|\n1225|        if trail < 0:\n1226|            warnings.warn('Column {!r} ending point overlaps the next '\n1227|                          'column.'.format(col_idx + 1))\n1228|\n1229|        # TODO: It would be nice if these string column formatting\n1230|        # details were left to a specialized class, as is the case\n1231|        # with FormatX and FormatP\n1232|        if 'A' in format:\n1233|            _pc = '{:'\n1234|        else:\n1235|            _pc = '{:>'\n1236|\n1237|        fmt = ''.join([_pc, format[1:], ASCII2STR[format[0]], '}',\n1238|                       (' ' * trail)])\n1239|\n1240|        # Even if the format precision is 0, we should output a decimal point\n1241|        # as long as there is space to do so--not including a decimal point in\n1242|        # a float value is discouraged by the FITS Standard\n1243|        trailing_decimal = (format.precision == 0 and\n1244|                            format.format in ('F', 'E', 'D'))\n1245|\n1246|        # not using numarray.strings's num2char because the\n1247|        # result is not allowed to expand (as C/Python does).\n1248|        for jdx, value in enumerate(input_field):\n1249|            value = fmt.format(value)\n1250|            if len(value) > starts[col_idx + 1] - starts[col_idx]:\n1251|                raise ValueError(\n1252|                    \"Value {!r} does not fit into the output's itemsize of \"\n1253|                    \"{}.\".format(value, spans[col_idx]))\n1254|\n1255|            if trailing_decimal and value[0] == ' ':\n1256|                # We have some extra space in the field for the trailing\n1257|                # decimal point\n1258|                value = value[1:] + '.'\n1259|\n1260|            output_field[jdx] = value\n1261|\n1262|        # Replace exponent separator in floating point numbers\n1263|        if 'D' in format:\n1264|            output_field.replace(encode_ascii('E'), encode_ascii('D'))\n1265|\n1266|\n1267|def _get_recarray_field(array, key):\n1268|    \"\"\"\n1269|    Compatibility function for using the recarray base class's field method.\n1270|    This incorporates the legacy functionality of returning string arrays as\n1271|    Numeric-style chararray objects.\n1272|    \"\"\"\n1273|\n1274|    # Numpy >= 1.10.dev recarray no longer returns chararrays for strings\n...\n\n\n\n\n###\n\nPlease provide the class name, function or method name, or the exact line numbers that need to be edited.\nThe possible location outputs should be either \"class\", \"function\" or \"line\".\n\n### Examples:\n```\nfull_path1/file1.py\nline: 10\nclass: MyClass1\nline: 51\n\nfull_path2/file2.py\nfunction: MyClass2.my_method\nline: 12\n\nfull_path3/file3.py\nfunction: my_function\nline: 24\nline: 156\n```\n\nReturn just the location(s) wrapped with ```.\n", "response": ["```\nastropy/io/fits/fitsrec.py\nfunction: FitsRec._scale_back_ascii\nline: 1264\n```", "```\nastropy/io/fits/fitsrec.py\nfunction: _scale_back_ascii\nline: 1264\n```", "```\nastropy/io/fits/fitsrec.py\nfunction: FITS_rec._scale_back_ascii\nline: 1264\n```", "```\nastropy/io/fits/fitsrec.py\nline: 1264\n```"], "usage": {"completion_tokens": 102, "prompt_tokens": 1374}}}
{"instance_id": "astropy__astropy-7746", "found_files": ["astropy/wcs/wcs.py", "astropy/wcs/utils.py", "astropy/wcs/docstrings.py"], "additional_artifact_loc_file": {}, "file_traj": {}, "found_related_locs": {"astropy/wcs/wcs.py": ["function: WCS.wcs_pix2world\nfunction: WCS._array_converter\nfunction: WCS._return_list_of_arrays"], "astropy/wcs/utils.py": [""], "astropy/wcs/docstrings.py": [""]}, "additional_artifact_loc_related": [{"raw_output_loc": "```\nastropy/wcs/wcs.py\nfunction: WCS.wcs_pix2world\nfunction: WCS._array_converter\nfunction: WCS._return_list_of_arrays\n```"}], "related_loc_traj": [{"response": "```\nastropy/wcs/wcs.py\nfunction: WCS.wcs_pix2world\nfunction: WCS._array_converter\nfunction: WCS._return_list_of_arrays\n```", "usage": {"completion_tokens": 40, "prompt_tokens": 15316}, "prompt": "\nPlease look through the following GitHub Problem Description and the Skeleton of Relevant Files.\nIdentify all locations that need inspection or editing to fix the problem, including directly related areas as well as any potentially related global variables, functions, and classes.\nFor each location you provide, either give the name of the class, the name of a method in a class, the name of a function, or the name of a global variable.\n\n### GitHub Problem Description ###\nIssue when passing empty lists/arrays to WCS transformations\nThe following should not fail but instead should return empty lists/arrays:\r\n\r\n```\r\nIn [1]: from astropy.wcs import WCS\r\n\r\nIn [2]: wcs = WCS('2MASS_h.fits')\r\n\r\nIn [3]: wcs.wcs_pix2world([], [], 0)\r\n---------------------------------------------------------------------------\r\nInconsistentAxisTypesError                Traceback (most recent call last)\r\n<ipython-input-3-e2cc0e97941a> in <module>()\r\n----> 1 wcs.wcs_pix2world([], [], 0)\r\n\r\n~/Dropbox/Code/Astropy/astropy/astropy/wcs/wcs.py in wcs_pix2world(self, *args, **kwargs)\r\n   1352         return self._array_converter(\r\n   1353             lambda xy, o: self.wcs.p2s(xy, o)['world'],\r\n-> 1354             'output', *args, **kwargs)\r\n   1355     wcs_pix2world.__doc__ = \"\"\"\r\n   1356         Transforms pixel coordinates to world coordinates by doing\r\n\r\n~/Dropbox/Code/Astropy/astropy/astropy/wcs/wcs.py in _array_converter(self, func, sky, ra_dec_order, *args)\r\n   1267                     \"a 1-D array for each axis, followed by an origin.\")\r\n   1268 \r\n-> 1269             return _return_list_of_arrays(axes, origin)\r\n   1270 \r\n   1271         raise TypeError(\r\n\r\n~/Dropbox/Code/Astropy/astropy/astropy/wcs/wcs.py in _return_list_of_arrays(axes, origin)\r\n   1223             if ra_dec_order and sky == 'input':\r\n   1224                 xy = self._denormalize_sky(xy)\r\n-> 1225             output = func(xy, origin)\r\n   1226             if ra_dec_order and sky == 'output':\r\n   1227                 output = self._normalize_sky(output)\r\n\r\n~/Dropbox/Code/Astropy/astropy/astropy/wcs/wcs.py in <lambda>(xy, o)\r\n   1351             raise ValueError(\"No basic WCS settings were created.\")\r\n   1352         return self._array_converter(\r\n-> 1353             lambda xy, o: self.wcs.p2s(xy, o)['world'],\r\n   1354             'output', *args, **kwargs)\r\n   1355     wcs_pix2world.__doc__ = \"\"\"\r\n\r\nInconsistentAxisTypesError: ERROR 4 in wcsp2s() at line 2646 of file cextern/wcslib/C/wcs.c:\r\nncoord and/or nelem inconsistent with the wcsprm.\r\n```\n\n\n### Skeleton of Relevant Files ###\n\n### File: astropy/wcs/wcs.py ###\n```python\n# Licensed under a 3-clause BSD style license - see LICENSE.rst\n\n__all__ = ['FITSFixedWarning', 'WCS', 'find_all_wcs',\n           'DistortionLookupTable', 'Sip', 'Tabprm', 'Wcsprm',\n           'WCSBase', 'validate', 'WcsError', 'SingularMatrixError',\n           'InconsistentAxisTypesError', 'InvalidTransformError',\n           'InvalidCoordinateError', 'NoSolutionError',\n           'InvalidSubimageSpecificationError', 'NoConvergence',\n           'NonseparableSubimageCoordinateSystemError',\n           'NoWcsKeywordsFoundError', 'InvalidTabularParametersError']\n\n\n__doctest_skip__ = ['WCS.all_world2pix']\n\n\n# Additional relax bit flags\nWCSHDO_SIP = 0x80000\n\n# Regular expression defining SIP keyword It matches keyword that starts with A\n# or B, optionally followed by P, followed by an underscore then a number in\n# range of 0-19, followed by an underscore and another number in range of 0-19.\n# Keyword optionally ends with a capital letter.\nSIP_KW = re.compile('''^[AB]P?_1?[0-9]_1?[0-9][A-Z]?$''')\n\n\ndef _parse_keysel(keysel):\n...\n\n\nclass NoConvergence(Exception):\n\n    def __init__(self, *args, best_solution=None, accuracy=None, niter=None,\n                 divergent=None, slow_conv=None, **kwargs):\n...\n\n\nclass FITSFixedWarning(AstropyWarning):\n    pass\n\n\nclass WCS(WCSBase):\n\n    def __init__(self, header=None, fobj=None, key=' ', minerr=0.0,\n                 relax=True, naxis=None, keysel=None, colsel=None,\n                 fix=True, translate_units='', _do_set=True):\n...\n\n    def __copy__(self):\n...\n\n    def __deepcopy__(self, memo):\n...\n\n    def copy(self):\n...\n\n    def deepcopy(self):\n...\n\n    def sub(self, axes=None):\n...\n    if _wcs is not None:\n        sub.__doc__ = _wcs.Wcsprm.sub.__doc__\n\n    def _fix_scamp(self):\n...\n\n    def fix(self, translate_units='', naxis=None):\n...\n\n    def calc_footprint(self, header=None, undistort=True, axes=None, center=True):\n...\n\n    def _read_det2im_kw(self, header, fobj, err=0.0):\n...\n\n    def _read_d2im_old_format(self, header, fobj, axiscorr):\n...\n\n    def _write_det2im(self, hdulist):\n...\n\n    def _read_distortion_kw(self, header, fobj, dist='CPDIS', err=0.0):\n...\n\n    def _write_distortion_kw(self, hdulist, dist='CPDIS'):\n...\n\n    def _remove_sip_kw(self, header):\n...\n\n    def _read_sip_kw(self, header, wcskey=\"\"):\n...\n\n    def _write_sip_kw(self):\n...\n\n    def _denormalize_sky(self, sky):\n...\n\n    def _normalize_sky(self, sky):\n...\n\n    def _array_converter(self, func, sky, *args, ra_dec_order=False):\n...\n\n    def all_pix2world(self, *args, **kwargs):\n...\n    all_pix2world.__doc__ = \"\"\"\n        Transforms pixel coordinates to world coordinates.\n\n        Performs all of the following in series:\n\n            - Detector to image plane correction (if present in the\n              FITS file)\n\n            - `SIP`_ distortion correction (if present in the FITS\n              file)\n...\n            x- and y-coordinate arrays are not the same size.\n\n        InvalidTransformError\n            Invalid coordinate transformation parameters.\n\n        InvalidTransformError\n            Ill-conditioned coordinate transformation parameters.\n        \"\"\".format(__.TWO_OR_MORE_ARGS('naxis', 8),\n                   __.RA_DEC_ORDER(8),\n                   __.RETURNS('sky coordinates, in degrees', 8))\n\n    def wcs_pix2world(self, *args, **kwargs):\n...\n    wcs_pix2world.__doc__ = \"\"\"\n        Transforms pixel coordinates to world coordinates by doing\n        only the basic `wcslib`_ transformation.\n\n        No `SIP`_ or `distortion paper`_ table lookup correction is\n        applied.  To perform distortion correction, see\n        `~astropy.wcs.WCS.all_pix2world`,\n        `~astropy.wcs.WCS.sip_pix2foc`, `~astropy.wcs.WCS.p4_pix2foc`,\n        or `~astropy.wcs.WCS.pix2foc`.\n\n...\n        The order of the axes for the result is determined by the\n        ``CTYPEia`` keywords in the FITS header, therefore it may not\n        always be of the form (*ra*, *dec*).  The\n        `~astropy.wcs.Wcsprm.lat`, `~astropy.wcs.Wcsprm.lng`,\n        `~astropy.wcs.Wcsprm.lattyp` and `~astropy.wcs.Wcsprm.lngtyp`\n        members can be used to determine the order of the axes.\n\n        \"\"\".format(__.TWO_OR_MORE_ARGS('naxis', 8),\n                   __.RA_DEC_ORDER(8),\n                   __.RETURNS('world coordinates, in degrees', 8))\n\n    def _all_world2pix(self, world, origin, tolerance, maxiter, adaptive,\n                       detect_divergence, quiet):\n...\n\n    def all_world2pix(self, *args, tolerance=1e-4, maxiter=20, adaptive=False,\n                      detect_divergence=True, quiet=False, **kwargs):\n...\n\n    all_world2pix.__doc__ = \"\"\"\n        all_world2pix(*arg, accuracy=1.0e-4, maxiter=20,\n        adaptive=False, detect_divergence=True, quiet=False)\n\n        Transforms world coordinates to pixel coordinates, using\n        numerical iteration to invert the full forward transformation\n        `~astropy.wcs.WCS.all_pix2world` with complete\n        distortion model.\n\n\n...\n        >>> raise e\n        Traceback (most recent call last):\n        ...\n        NoConvergence: 'WCS.all_world2pix' failed to converge to the\n        requested accuracy.  After 6 iterations, the solution is\n        diverging at least for one input point.\n\n        \"\"\".format(__.TWO_OR_MORE_ARGS('naxis', 8),\n                   __.RA_DEC_ORDER(8),\n                   __.RETURNS('pixel coordinates', 8))\n\n    def wcs_world2pix(self, *args, **kwargs):\n...\n    wcs_world2pix.__doc__ = \"\"\"\n        Transforms world coordinates to pixel coordinates, using only\n        the basic `wcslib`_ WCS transformation.  No `SIP`_ or\n        `distortion paper`_ table lookup transformation is applied.\n\n        Parameters\n        ----------\n        {0}\n\n            For a transformation that is not two-dimensional, the\n...\n            x- and y-coordinate arrays are not the same size.\n\n        InvalidTransformError\n            Invalid coordinate transformation parameters.\n\n        InvalidTransformError\n            Ill-conditioned coordinate transformation parameters.\n        \"\"\".format(__.TWO_OR_MORE_ARGS('naxis', 8),\n                   __.RA_DEC_ORDER(8),\n                   __.RETURNS('pixel coordinates', 8))\n\n    def pix2foc(self, *args):\n...\n    pix2foc.__doc__ = \"\"\"\n        Convert pixel coordinates to focal plane coordinates using the\n        `SIP`_ polynomial distortion convention and `distortion\n        paper`_ table-lookup correction.\n\n        The output is in absolute pixel coordinates, not relative to\n        ``CRPIX``.\n\n        Parameters\n        ----------\n\n        {0}\n\n        Returns\n        -------\n\n        {1}\n\n        Raises\n        ------\n        MemoryError\n            Memory allocation failed.\n\n        ValueError\n            Invalid coordinate transformation parameters.\n        \"\"\".format(__.TWO_OR_MORE_ARGS('2', 8),\n                   __.RETURNS('focal coordinates', 8))\n\n    def p4_pix2foc(self, *args):\n...\n    p4_pix2foc.__doc__ = \"\"\"\n        Convert pixel coordinates to focal plane coordinates using\n        `distortion paper`_ table-lookup correction.\n\n        The output is in absolute pixel coordinates, not relative to\n        ``CRPIX``.\n\n        Parameters\n        ----------\n\n        {0}\n\n        Returns\n        -------\n\n        {1}\n\n        Raises\n        ------\n        MemoryError\n            Memory allocation failed.\n\n        ValueError\n            Invalid coordinate transformation parameters.\n        \"\"\".format(__.TWO_OR_MORE_ARGS('2', 8),\n                   __.RETURNS('focal coordinates', 8))\n\n    def det2im(self, *args):\n...\n    det2im.__doc__ = \"\"\"\n        Convert detector coordinates to image plane coordinates using\n        `distortion paper`_ table-lookup correction.\n\n        The output is in absolute pixel coordinates, not relative to\n        ``CRPIX``.\n\n        Parameters\n        ----------\n\n        {0}\n\n        Returns\n        -------\n\n        {1}\n\n        Raises\n        ------\n        MemoryError\n            Memory allocation failed.\n\n        ValueError\n            Invalid coordinate transformation parameters.\n        \"\"\".format(__.TWO_OR_MORE_ARGS('2', 8),\n                   __.RETURNS('pixel coordinates', 8))\n\n    def sip_pix2foc(self, *args):\n...\n    sip_pix2foc.__doc__ = \"\"\"\n        Convert pixel coordinates to focal plane coordinates using the\n        `SIP`_ polynomial distortion convention.\n\n        The output is in pixel coordinates, relative to ``CRPIX``.\n\n        FITS WCS `distortion paper`_ table lookup correction is not\n        applied, even if that information existed in the FITS file\n        that initialized this :class:`~astropy.wcs.WCS` object.  To\n        correct for that, use `~astropy.wcs.WCS.pix2foc` or\n        `~astropy.wcs.WCS.p4_pix2foc`.\n\n        Parameters\n        ----------\n\n        {0}\n\n        Returns\n        -------\n\n        {1}\n\n        Raises\n        ------\n        MemoryError\n            Memory allocation failed.\n\n        ValueError\n            Invalid coordinate transformation parameters.\n        \"\"\".format(__.TWO_OR_MORE_ARGS('2', 8),\n                   __.RETURNS('focal coordinates', 8))\n\n    def sip_foc2pix(self, *args):\n...\n    sip_foc2pix.__doc__ = \"\"\"\n        Convert focal plane coordinates to pixel coordinates using the\n        `SIP`_ polynomial distortion convention.\n\n        FITS WCS `distortion paper`_ table lookup distortion\n        correction is not applied, even if that information existed in\n        the FITS file that initialized this `~astropy.wcs.WCS` object.\n\n        Parameters\n        ----------\n\n        {0}\n\n        Returns\n        -------\n\n        {1}\n\n        Raises\n        ------\n        MemoryError\n            Memory allocation failed.\n\n        ValueError\n            Invalid coordinate transformation parameters.\n        \"\"\".format(__.TWO_OR_MORE_ARGS('2', 8),\n                   __.RETURNS('pixel coordinates', 8))\n\n    def to_fits(self, relax=False, key=None):\n...\n\n    def to_header(self, relax=None, key=None):\n...\n\n    def _fix_ctype(self, header, add_sip=True, log_message=True):\n...\n\n    def to_header_string(self, relax=None):\n...\n\n    def footprint_to_file(self, filename='footprint.reg', color='green',\n                          width=2, coordsys=None):\n...\n\n    @property\n    def _naxis1(self):\n...\n\n    @_naxis1.setter\n    def _naxis1(self, value):\n...\n\n    @property\n    def _naxis2(self):\n...\n\n    @_naxis2.setter\n    def _naxis2(self, value):\n...\n\n    def _get_naxis(self, header=None):\n...\n\n    def printwcs(self):\n...\n\n    def __repr__(self):\n...\n\n    def get_axis_types(self):\n...\n\n    def __reduce__(self):\n...\n\n    def dropaxis(self, dropax):\n...\n\n    def swapaxes(self, ax0, ax1):\n...\n\n    def reorient_celestial_first(self):\n...\n\n    def slice(self, view, numpy_order=True):\n...\n\n    def __getitem__(self, item):\n...\n\n    def __iter__(self):\n...\n\n    @property\n    def axis_type_names(self):\n...\n\n    @property\n    def celestial(self):\n...\n\n    @property\n    def is_celestial(self):\n...\n\n    @property\n    def has_celestial(self):\n...\n\n    @property\n    def pixel_scale_matrix(self):\n...\n\n    def _as_mpl_axes(self):\n...\n\n\ndef __WCS_unpickle__(cls, dct, fits_data):\n...\n\n\ndef find_all_wcs(header, relax=True, keysel=None, fix=True,\n                 translate_units='',\n                 _do_set=True):\n...\n\n\ndef validate(source):\n...\n\n```\n\n### File: astropy/wcs/utils.py ###\n```python\n# Licensed under a 3-clause BSD style license - see LICENSE.rst\n\n\n__doctest_skip__ = ['wcs_to_celestial_frame', 'celestial_frame_to_wcs']\n\n__all__ = ['add_stokes_axis_to_wcs', 'celestial_frame_to_wcs',\n           'wcs_to_celestial_frame', 'proj_plane_pixel_scales',\n           'proj_plane_pixel_area', 'is_proj_plane_distorted',\n           'non_celestial_pixel_scales', 'skycoord_to_pixel',\n           'pixel_to_skycoord', 'custom_wcs_to_frame_mappings',\n           'custom_frame_to_wcs_mappings']\n\n\ndef add_stokes_axis_to_wcs(wcs, add_before_ind):\n...\n\n\ndef _wcs_to_celestial_frame_builtin(wcs):\n...\n\n\ndef _celestial_frame_to_wcs_builtin(frame, projection='TAN'):\n...\n\n\nWCS_FRAME_MAPPINGS = [[_wcs_to_celestial_frame_builtin]]\nFRAME_WCS_MAPPINGS = [[_celestial_frame_to_wcs_builtin]]\n\n\nclass custom_wcs_to_frame_mappings:\n    def __init__(self, mappings=[]):\n...\n\n    def __enter__(self):\n...\n\n    def __exit__(self, type, value, tb):\n...\n\n\n# Backward-compatibility\ncustom_frame_mappings = custom_wcs_to_frame_mappings\n\n\nclass custom_frame_to_wcs_mappings:\n    def __init__(self, mappings=[]):\n...\n\n    def __enter__(self):\n...\n\n    def __exit__(self, type, value, tb):\n...\n\n\ndef wcs_to_celestial_frame(wcs):\n...\n\n\ndef celestial_frame_to_wcs(frame, projection='TAN'):\n...\n\n\ndef proj_plane_pixel_scales(wcs):\n...\n\n\ndef proj_plane_pixel_area(wcs):\n...\n\n\ndef is_proj_plane_distorted(wcs, maxerr=1.0e-5):\n...\n\n\ndef _is_cd_orthogonal(cd, maxerr):\n...\n\n\ndef non_celestial_pixel_scales(inwcs):\n...\n\n\ndef _has_distortion(wcs):\n...\n\n\n# TODO: in future, we should think about how the following two functions can be\n# integrated better into the WCS class.\n\ndef skycoord_to_pixel(coords, wcs, origin=0, mode='all'):\n...\n\n\ndef pixel_to_skycoord(xp, yp, wcs, origin=0, mode='all', cls=None):\n...\n\n```\n\n### File: astropy/wcs/docstrings.py ###\n```python\n# Licensed under a 3-clause BSD style license - see LICENSE.rst\n\n# It gets to be really tedious to type long docstrings in ANSI C\n# syntax (since multi-line string literals are not valid).\n# Therefore, the docstrings are written here in doc/docstrings.py,\n# which are then converted by setup.py into docstrings.h, which is\n# included by pywcs.c\n\n\na = \"\"\"\n``double array[a_order+1][a_order+1]`` Focal plane transformation\nmatrix.\n\nThe `SIP`_ ``A_i_j`` matrix used for pixel to focal plane\ntransformation.\n\nIts values may be changed in place, but it may not be resized, without\ncreating a new `~astropy.wcs.Sip` object.\n\"\"\"\n\na_order = \"\"\"\n``int`` (read-only) Order of the polynomial (``A_ORDER``).\n\"\"\"\n\nall_pix2world = \"\"\"\nall_pix2world(pixcrd, origin) -> ``double array[ncoord][nelem]``\n\nTransforms pixel coordinates to world coordinates.\n\nDoes the following:\n\n    - Detector to image plane correction (if present)\n\n    - SIP distortion correction (if present)\n...\n\nValueError\n    x- and y-coordinate arrays are not the same size.\n\nInvalidTransformError\n    Invalid coordinate transformation.\n\nInvalidTransformError\n    Ill-conditioned coordinate transformation parameters.\n\"\"\".format(__.ORIGIN())\n\nalt = \"\"\"\n``str`` Character code for alternate coordinate descriptions.\n\nFor example, the ``\"a\"`` in keyword names such as ``CTYPEia``.  This\nis a space character for the primary coordinate description, or one of\nthe 26 upper-case letters, A-Z.\n\"\"\"\n\nap = \"\"\"\n``double array[ap_order+1][ap_order+1]`` Focal plane to pixel\ntransformation matrix.\n\nThe `SIP`_ ``AP_i_j`` matrix used for focal plane to pixel\ntransformation.  Its values may be changed in place, but it may not be\nresized, without creating a new `~astropy.wcs.Sip` object.\n\"\"\"\n\nap_order = \"\"\"\n``int`` (read-only) Order of the polynomial (``AP_ORDER``).\n\"\"\"\n\naxis_types = \"\"\"\n``int array[naxis]`` An array of four-digit type codes for each axis.\n\n- First digit (i.e. 1000s):\n\n  - 0: Non-specific coordinate type.\n\n  - 1: Stokes coordinate.\n\n  - 2: Celestial coordinate (including ``CUBEFACE``).\n...\n\n    - 1: Latitude coordinate.\n\n    - 2: ``CUBEFACE`` number.\n\n  - For lookup tables: the axis number in a multidimensional table.\n\n``CTYPEia`` in ``\"4-3\"`` form with unrecognized algorithm code will\nhave its type set to -1 and generate an error.\n\"\"\"\n\nb = \"\"\"\n``double array[b_order+1][b_order+1]`` Pixel to focal plane\ntransformation matrix.\n\nThe `SIP`_ ``B_i_j`` matrix used for pixel to focal plane\ntransformation.  Its values may be changed in place, but it may not be\nresized, without creating a new `~astropy.wcs.Sip` object.\n\"\"\"\n\nb_order = \"\"\"\n``int`` (read-only) Order of the polynomial (``B_ORDER``).\n\"\"\"\n\nbounds_check = \"\"\"\nbounds_check(pix2world, world2pix)\n\nEnable/disable bounds checking.\n\nParameters\n----------\npix2world : bool, optional\n    When `True`, enable bounds checking for the pixel-to-world (p2x)\n    transformations.  Default is `True`.\n\nworld2pix : bool, optional\n    When `True`, enable bounds checking for the world-to-pixel (s2x)\n    transformations.  Default is `True`.\n\nNotes\n-----\nNote that by default (without calling `bounds_check`) strict bounds\nchecking is enabled.\n\"\"\"\n\nbp = \"\"\"\n``double array[bp_order+1][bp_order+1]`` Focal plane to pixel\ntransformation matrix.\n\nThe `SIP`_ ``BP_i_j`` matrix used for focal plane to pixel\ntransformation.  Its values may be changed in place, but it may not be\nresized, without creating a new `~astropy.wcs.Sip` object.\n\"\"\"\n\nbp_order = \"\"\"\n``int`` (read-only) Order of the polynomial (``BP_ORDER``).\n\"\"\"\n\ncd = \"\"\"\n``double array[naxis][naxis]`` The ``CDi_ja`` linear transformation\nmatrix.\n\nFor historical compatibility, three alternate specifications of the\nlinear transformations are available in wcslib.  The canonical\n``PCi_ja`` with ``CDELTia``, ``CDi_ja``, and the deprecated\n``CROTAia`` keywords.  Although the latter may not formally co-exist\nwith ``PCi_ja``, the approach here is simply to ignore them if given\nin conjunction with ``PCi_ja``.\n\n`~astropy.wcs.Wcsprm.has_pc`, `~astropy.wcs.Wcsprm.has_cd` and\n`~astropy.wcs.Wcsprm.has_crota` can be used to determine which of\nthese alternatives are present in the header.\n\nThese alternate specifications of the linear transformation matrix are\ntranslated immediately to ``PCi_ja`` by `~astropy.wcs.Wcsprm.set` and\nare nowhere visible to the lower-level routines.  In particular,\n`~astropy.wcs.Wcsprm.set` resets `~astropy.wcs.Wcsprm.cdelt` to unity\nif ``CDi_ja`` is present (and no ``PCi_ja``).  If no ``CROTAia`` is\nassociated with the latitude axis, `~astropy.wcs.Wcsprm.set` reverts\nto a unity ``PCi_ja`` matrix.\n\"\"\"\n\ncdelt = \"\"\"\n``double array[naxis]`` Coordinate increments (``CDELTia``) for each\ncoord axis.\n\nIf a ``CDi_ja`` linear transformation matrix is present, a warning is\nraised and `~astropy.wcs.Wcsprm.cdelt` is ignored.  The ``CDi_ja``\nmatrix may be deleted by::\n\n  del wcs.wcs.cd\n\nAn undefined value is represented by NaN.\n\"\"\"\n\ncdfix = \"\"\"\ncdfix()\n\nFix erroneously omitted ``CDi_ja`` keywords.\n\nSets the diagonal element of the ``CDi_ja`` matrix to unity if all\n``CDi_ja`` keywords associated with a given axis were omitted.\nAccording to Paper I, if any ``CDi_ja`` keywords at all are given in a\nFITS header then those not given default to zero.  This results in a\nsingular matrix with an intersecting row and column of zeros.\n\nReturns\n-------\nsuccess : int\n    Returns ``0`` for success; ``-1`` if no change required.\n\"\"\"\n\ncel_offset = \"\"\"\n``boolean`` Is there an offset?\n\nIf `True`, an offset will be applied to ``(x, y)`` to force ``(x, y) =\n(0, 0)`` at the fiducial point, (phi_0, theta_0).  Default is `False`.\n\"\"\"\n\ncelfix = \"\"\"\nTranslates AIPS-convention celestial projection types, ``-NCP`` and\n``-GLS``.\n\nReturns\n-------\nsuccess : int\n    Returns ``0`` for success; ``-1`` if no change required.\n\"\"\"\n\ncname = \"\"\"\n``list of strings`` A list of the coordinate axis names, from\n``CNAMEia``.\n\"\"\"\n\ncolax = \"\"\"\n``int array[naxis]`` An array recording the column numbers for each\naxis in a pixel list.\n\"\"\"\n\ncolnum = \"\"\"\n``int`` Column of FITS binary table associated with this WCS.\n\nWhere the coordinate representation is associated with an image-array\ncolumn in a FITS binary table, this property may be used to record the\nrelevant column number.\n\nIt should be set to zero for an image header or pixel list.\n\"\"\"\n\ncompare = \"\"\"\ncompare(other, cmp=0, tolerance=0.0)\n\nCompare two Wcsprm objects for equality.\n\nParameters\n----------\n\nother : Wcsprm\n    The other Wcsprm object to compare to.\n...\ntolerance : float, optional\n    The amount of tolerance required.  For example, for a value of\n    1e-6, all floating-point values in the objects must be equal to\n    the first 6 decimal places.  The default value of 0.0 implies\n    exact equality.\n\nReturns\n-------\nequal : bool\n\"\"\"\n\nconvert = \"\"\"\nconvert(array)\n\nPerform the unit conversion on the elements of the given *array*,\nreturning an array of the same shape.\n\"\"\"\n\ncoord = \"\"\"\n``double array[K_M]...[K_2][K_1][M]`` The tabular coordinate array.\n\nHas the dimensions::\n\n    (K_M, ... K_2, K_1, M)\n\n(see `~astropy.wcs.Tabprm.K`) i.e. with the `M` dimension\nvarying fastest so that the `M` elements of a coordinate vector are\nstored contiguously in memory.\n\"\"\"\n\ncopy = \"\"\"\nCreates a deep copy of the WCS object.\n\"\"\"\n\ncpdis1 = \"\"\"\n`~astropy.wcs.DistortionLookupTable`\n\nThe pre-linear transformation distortion lookup table, ``CPDIS1``.\n\"\"\"\n\ncpdis2 = \"\"\"\n`~astropy.wcs.DistortionLookupTable`\n\nThe pre-linear transformation distortion lookup table, ``CPDIS2``.\n\"\"\"\n\ncrder = \"\"\"\n``double array[naxis]`` The random error in each coordinate axis,\n``CRDERia``.\n\nAn undefined value is represented by NaN.\n\"\"\"\n\ncrota = \"\"\"\n``double array[naxis]`` ``CROTAia`` keyvalues for each coordinate\naxis.\n\nFor historical compatibility, three alternate specifications of the\nlinear transformations are available in wcslib.  The canonical\n``PCi_ja`` with ``CDELTia``, ``CDi_ja``, and the deprecated\n``CROTAia`` keywords.  Although the latter may not formally co-exist\nwith ``PCi_ja``, the approach here is simply to ignore them if given\nin conjunction with ``PCi_ja``.\n\n`~astropy.wcs.Wcsprm.has_pc`, `~astropy.wcs.Wcsprm.has_cd` and\n`~astropy.wcs.Wcsprm.has_crota` can be used to determine which of\nthese alternatives are present in the header.\n\nThese alternate specifications of the linear transformation matrix are\ntranslated immediately to ``PCi_ja`` by `~astropy.wcs.Wcsprm.set` and\nare nowhere visible to the lower-level routines.  In particular,\n`~astropy.wcs.Wcsprm.set` resets `~astropy.wcs.Wcsprm.cdelt` to unity\nif ``CDi_ja`` is present (and no ``PCi_ja``).  If no ``CROTAia`` is\nassociated with the latitude axis, `~astropy.wcs.Wcsprm.set` reverts\nto a unity ``PCi_ja`` matrix.\n\"\"\"\n\ncrpix = \"\"\"\n``double array[naxis]`` Coordinate reference pixels (``CRPIXja``) for\neach pixel axis.\n\"\"\"\n\ncrval = \"\"\"\n``double array[naxis]`` Coordinate reference values (``CRVALia``) for\neach coordinate axis.\n\"\"\"\n\ncrval_tabprm = \"\"\"\n``double array[M]`` Index values for the reference pixel for each of\nthe tabular coord axes.\n\"\"\"\n\ncsyer = \"\"\"\n``double array[naxis]`` The systematic error in the coordinate value\naxes, ``CSYERia``.\n\nAn undefined value is represented by NaN.\n\"\"\"\n\nctype = \"\"\"\n``list of strings[naxis]`` List of ``CTYPEia`` keyvalues.\n\nThe `~astropy.wcs.Wcsprm.ctype` keyword values must be in upper case\nand there must be zero or one pair of matched celestial axis types,\nand zero or one spectral axis.\n\"\"\"\n\ncubeface = \"\"\"\n``int`` Index into the ``pixcrd`` (pixel coordinate) array for the\n``CUBEFACE`` axis.\n\nThis is used for quadcube projections where the cube faces are stored\non a separate axis.\n\nThe quadcube projections (``TSC``, ``CSC``, ``QSC``) may be\nrepresented in FITS in either of two ways:\n\n...\n      three-dimensional structure using a ``CUBEFACE`` axis indexed\n      from 0 to 5 as above.\n\nThese routines support both methods; `~astropy.wcs.Wcsprm.set`\ndetermines which is being used by the presence or absence of a\n``CUBEFACE`` axis in `~astropy.wcs.Wcsprm.ctype`.\n`~astropy.wcs.Wcsprm.p2s` and `~astropy.wcs.Wcsprm.s2p` translate the\n``CUBEFACE`` axis representation to the single plane representation\nunderstood by the lower-level projection routines.\n\"\"\"\n\ncunit = \"\"\"\n``list of astropy.UnitBase[naxis]`` List of ``CUNITia`` keyvalues as\n`astropy.units.UnitBase` instances.\n\nThese define the units of measurement of the ``CRVALia``, ``CDELTia``\nand ``CDi_ja`` keywords.\n\nAs ``CUNITia`` is an optional header keyword,\n`~astropy.wcs.Wcsprm.cunit` may be left blank but otherwise is\nexpected to contain a standard units specification as defined by WCS\nPaper I.  `~astropy.wcs.Wcsprm.unitfix` is available to translate\ncommonly used non-standard units specifications but this must be done\nas a separate step before invoking `~astropy.wcs.Wcsprm.set`.\n\nFor celestial axes, if `~astropy.wcs.Wcsprm.cunit` is not blank,\n`~astropy.wcs.Wcsprm.set` uses ``wcsunits`` to parse it and scale\n`~astropy.wcs.Wcsprm.cdelt`, `~astropy.wcs.Wcsprm.crval`, and\n`~astropy.wcs.Wcsprm.cd` to decimal degrees.  It then resets\n`~astropy.wcs.Wcsprm.cunit` to ``\"deg\"``.\n\nFor spectral axes, if `~astropy.wcs.Wcsprm.cunit` is not blank,\n`~astropy.wcs.Wcsprm.set` uses ``wcsunits`` to parse it and scale\n`~astropy.wcs.Wcsprm.cdelt`, `~astropy.wcs.Wcsprm.crval`, and\n`~astropy.wcs.Wcsprm.cd` to SI units.  It then resets\n`~astropy.wcs.Wcsprm.cunit` accordingly.\n\n`~astropy.wcs.Wcsprm.set` ignores `~astropy.wcs.Wcsprm.cunit` for\nother coordinate types; `~astropy.wcs.Wcsprm.cunit` may be used to\nlabel coordinate values.\n\"\"\"\n\ncylfix = \"\"\"\ncylfix()\n\nFixes WCS keyvalues for malformed cylindrical projections.\n\nReturns\n-------\nsuccess : int\n    Returns ``0`` for success; ``-1`` if no change required.\n\"\"\"\n\ndata = \"\"\"\n``float array`` The array data for the\n`~astropy.wcs.DistortionLookupTable`.\n\"\"\"\n\ndata_wtbarr = \"\"\"\n``double array``\n\nThe array data for the BINTABLE.\n\"\"\"\n\ndateavg = \"\"\"\n``string`` Representative mid-point of the date of observation.\n\nIn ISO format, ``yyyy-mm-ddThh:mm:ss``.\n\nSee also\n--------\nastropy.wcs.Wcsprm.dateobs\n\"\"\"\n\ndateobs = \"\"\"\n``string`` Start of the date of observation.\n\nIn ISO format, ``yyyy-mm-ddThh:mm:ss``.\n\nSee also\n--------\nastropy.wcs.Wcsprm.dateavg\n\"\"\"\n\ndatfix = \"\"\"\ndatfix()\n\nTranslates the old ``DATE-OBS`` date format to year-2000 standard form\n``(yyyy-mm-ddThh:mm:ss)`` and derives ``MJD-OBS`` from it if not\nalready set.\n\nAlternatively, if `~astropy.wcs.Wcsprm.mjdobs` is set and\n`~astropy.wcs.Wcsprm.dateobs` isn't, then `~astropy.wcs.Wcsprm.datfix`\nderives `~astropy.wcs.Wcsprm.dateobs` from it.  If both are set but\ndisagree by more than half a day then `ValueError` is raised.\n\nReturns\n-------\nsuccess : int\n    Returns ``0`` for success; ``-1`` if no change required.\n\"\"\"\n\ndelta = \"\"\"\n``double array[M]`` (read-only) Interpolated indices into the coord\narray.\n\nArray of interpolated indices into the coordinate array such that\nUpsilon_m, as defined in Paper III, is equal to\n(`~astropy.wcs.Tabprm.p0` [m] + 1) + delta[m].\n\"\"\"\n\ndet2im = \"\"\"\nConvert detector coordinates to image plane coordinates.\n\"\"\"\n\ndet2im1 = \"\"\"\nA `~astropy.wcs.DistortionLookupTable` object for detector to image plane\ncorrection in the *x*-axis.\n\"\"\"\n\ndet2im2 = \"\"\"\nA `~astropy.wcs.DistortionLookupTable` object for detector to image plane\ncorrection in the *y*-axis.\n\"\"\"\n\ndims = \"\"\"\n``int array[ndim]`` (read-only)\n\nThe dimensions of the tabular array\n`~astropy.wcs.Wtbarr.data`.\n\"\"\"\n\nDistortionLookupTable = \"\"\"\nDistortionLookupTable(*table*, *crpix*, *crval*, *cdelt*)\n\nRepresents a single lookup table for a `distortion paper`_\ntransformation.\n\nParameters\n----------\ntable : 2-dimensional array\n    The distortion lookup table.\n\ncrpix : 2-tuple\n    The distortion array reference pixel\n\ncrval : 2-tuple\n    The image array pixel coordinate\n\ncdelt : 2-tuple\n    The grid step size\n\"\"\"\n\nequinox = \"\"\"\n``double`` The equinox associated with dynamical equatorial or\necliptic coordinate systems.\n\n``EQUINOXa`` (or ``EPOCH`` in older headers).  Not applicable to ICRS\nequatorial or ecliptic coordinates.\n\nAn undefined value is represented by NaN.\n\"\"\"\n\nextlev = \"\"\"\n``int`` (read-only)\n\n``EXTLEV`` identifying the binary table extension.\n\"\"\"\n\nextnam = \"\"\"\n``str`` (read-only)\n\n``EXTNAME`` identifying the binary table extension.\n\"\"\"\n\nextrema = \"\"\"\n``double array[K_M]...[K_2][2][M]`` (read-only)\n\nAn array recording the minimum and maximum value of each element of\nthe coordinate vector in each row of the coordinate array, with the\ndimensions::\n\n    (K_M, ... K_2, 2, M)\n\n(see `~astropy.wcs.Tabprm.K`).  The minimum is recorded\nin the first element of the compressed K_1 dimension, then the\nmaximum.  This array is used by the inverse table lookup function to\nspeed up table searches.\n\"\"\"\n\nextver = \"\"\"\n``int`` (read-only)\n\n``EXTVER`` identifying the binary table extension.\n\"\"\"\n\nfind_all_wcs = \"\"\"\nfind_all_wcs(relax=0, keysel=0)\n\nFind all WCS transformations in the header.\n\nParameters\n----------\n\nheader : str\n    The raw FITS header data.\n...\n\n    - ``WCSHDR_PIXLIST``: Pixel list keywords.\n\n    If zero, there is no restriction.  If -1, `wcspih` is called,\n    rather than `wcstbh`.\n\nReturns\n-------\nwcs_list : list of `~astropy.wcs.Wcsprm` objects\n\"\"\"\n\nfix = \"\"\"\nfix(translate_units='', naxis=0)\n\nApplies all of the corrections handled separately by\n`~astropy.wcs.Wcsprm.datfix`, `~astropy.wcs.Wcsprm.unitfix`,\n`~astropy.wcs.Wcsprm.celfix`, `~astropy.wcs.Wcsprm.spcfix`,\n`~astropy.wcs.Wcsprm.cylfix` and `~astropy.wcs.Wcsprm.cdfix`.\n\nParameters\n----------\n...\n    - `~astropy.wcs.Wcsprm.datfix`\n\n    - `~astropy.wcs.Wcsprm.unitfix`\n\n    - `~astropy.wcs.Wcsprm.celfix`\n\n    - `~astropy.wcs.Wcsprm.spcfix`\n\n    - `~astropy.wcs.Wcsprm.cylfix`\n\"\"\"\n\nget_offset = \"\"\"\nget_offset(x, y) -> (x, y)\n\nReturns the offset as defined in the distortion lookup table.\n\nReturns\n-------\ncoordinate : coordinate pair\n    The offset from the distortion table for pixel point (*x*, *y*).\n\"\"\"\n\nget_cdelt = \"\"\"\nget_cdelt() -> double array[naxis]\n\nCoordinate increments (``CDELTia``) for each coord axis.\n\nReturns the ``CDELT`` offsets in read-only form.  Unlike the\n`~astropy.wcs.Wcsprm.cdelt` property, this works even when the header\nspecifies the linear transformation matrix in one of the alternative\n``CDi_ja`` or ``CROTAia`` forms.  This is useful when you want access\nto the linear transformation matrix, but don't care how it was\nspecified in the header.\n\"\"\"\n\nget_pc = \"\"\"\nget_pc() -> double array[naxis][naxis]\n\nReturns the ``PC`` matrix in read-only form.  Unlike the\n`~astropy.wcs.Wcsprm.pc` property, this works even when the header\nspecifies the linear transformation matrix in one of the alternative\n``CDi_ja`` or ``CROTAia`` forms.  This is useful when you want access\nto the linear transformation matrix, but don't care how it was\nspecified in the header.\n\"\"\"\n\nget_ps = \"\"\"\nget_ps() -> list of tuples\n\nReturns ``PSi_ma`` keywords for each *i* and *m*.\n\nReturns\n-------\nps : list of tuples\n\n    Returned as a list of tuples of the form (*i*, *m*, *value*):\n\n    - *i*: int.  Axis number, as in ``PSi_ma``, (i.e. 1-relative)\n\n    - *m*: int.  Parameter number, as in ``PSi_ma``, (i.e. 0-relative)\n\n    - *value*: string.  Parameter value.\n\nSee also\n--------\nastropy.wcs.Wcsprm.set_ps : Set ``PSi_ma`` values\n\"\"\"\n\nget_pv = \"\"\"\nget_pv() -> list of tuples\n\nReturns ``PVi_ma`` keywords for each *i* and *m*.\n\nReturns\n-------\n\n    Returned as a list of tuples of the form (*i*, *m*, *value*):\n\n    - *i*: int.  Axis number, as in ``PVi_ma``, (i.e. 1-relative)\n\n    - *m*: int.  Parameter number, as in ``PVi_ma``, (i.e. 0-relative)\n\n    - *value*: string. Parameter value.\n\nSee also\n--------\nastropy.wcs.Wcsprm.set_pv : Set ``PVi_ma`` values\n\nNotes\n-----\n\nNote that, if they were not given, `~astropy.wcs.Wcsprm.set` resets\nthe entries for ``PVi_1a``, ``PVi_2a``, ``PVi_3a``, and ``PVi_4a`` for\nlongitude axis *i* to match (``phi_0``, ``theta_0``), the native\nlongitude and latitude of the reference point given by ``LONPOLEa``\nand ``LATPOLEa``.\n\"\"\"\n\nhas_cd = \"\"\"\nhas_cd() -> bool\n\nReturns `True` if ``CDi_ja`` is present.\n\n``CDi_ja`` is an alternate specification of the linear transformation\nmatrix, maintained for historical compatibility.\n\nMatrix elements in the IRAF convention are equivalent to the product\n``CDi_ja = CDELTia * PCi_ja``, but the defaults differ from that of\nthe ``PCi_ja`` matrix.  If one or more ``CDi_ja`` keywords are present\nthen all unspecified ``CDi_ja`` default to zero.  If no ``CDi_ja`` (or\n``CROTAia``) keywords are present, then the header is assumed to be in\n``PCi_ja`` form whether or not any ``PCi_ja`` keywords are present\nsince this results in an interpretation of ``CDELTia`` consistent with\nthe original FITS specification.\n\nWhile ``CDi_ja`` may not formally co-exist with ``PCi_ja``, it may\nco-exist with ``CDELTia`` and ``CROTAia`` which are to be ignored.\n\nSee also\n--------\nastropy.wcs.Wcsprm.cd : Get the raw ``CDi_ja`` values.\n\"\"\"\n\nhas_cdi_ja = \"\"\"\nhas_cdi_ja() -> bool\n\nAlias for `~astropy.wcs.Wcsprm.has_cd`.  Maintained for backward\ncompatibility.\n\"\"\"\n\nhas_crota = \"\"\"\nhas_crota() -> bool\n\nReturns `True` if ``CROTAia`` is present.\n\n``CROTAia`` is an alternate specification of the linear transformation\nmatrix, maintained for historical compatibility.\n\nIn the AIPS convention, ``CROTAia`` may only be associated with the\nlatitude axis of a celestial axis pair.  It specifies a rotation in\nthe image plane that is applied *after* the ``CDELTia``; any other\n``CROTAia`` keywords are ignored.\n\n``CROTAia`` may not formally co-exist with ``PCi_ja``.  ``CROTAia`` and\n``CDELTia`` may formally co-exist with ``CDi_ja`` but if so are to be\nignored.\n\nSee also\n--------\nastropy.wcs.Wcsprm.crota : Get the raw ``CROTAia`` values\n\"\"\"\n\nhas_crotaia = \"\"\"\nhas_crotaia() -> bool\n\nAlias for `~astropy.wcs.Wcsprm.has_crota`.  Maintained for backward\ncompatibility.\n\"\"\"\n\nhas_pc = \"\"\"\nhas_pc() -> bool\n\nReturns `True` if ``PCi_ja`` is present.  ``PCi_ja`` is the\nrecommended way to specify the linear transformation matrix.\n\nSee also\n--------\nastropy.wcs.Wcsprm.pc : Get the raw ``PCi_ja`` values\n\"\"\"\n\nhas_pci_ja = \"\"\"\nhas_pci_ja() -> bool\n\nAlias for `~astropy.wcs.Wcsprm.has_pc`.  Maintained for backward\ncompatibility.\n\"\"\"\n\ni = \"\"\"\n``int`` (read-only)\n\nImage axis number.\n\"\"\"\n\nimgpix_matrix = \"\"\"\n``double array[2][2]`` (read-only) Inverse of the ``CDELT`` or ``PC``\nmatrix.\n\nInverse containing the product of the ``CDELTia`` diagonal matrix and\nthe ``PCi_ja`` matrix.\n\"\"\"\n\nis_unity = \"\"\"\nis_unity() -> bool\n\nReturns `True` if the linear transformation matrix\n(`~astropy.wcs.Wcsprm.cd`) is unity.\n\"\"\"\n\nK = \"\"\"\n``int array[M]`` (read-only) The lengths of the axes of the coordinate\narray.\n\nAn array of length `M` whose elements record the lengths of the axes of\nthe coordinate array and of each indexing vector.\n\"\"\"\n\nkind = \"\"\"\n``str`` (read-only)\n\nCharacter identifying the wcstab array type:\n\n    - ``'c'``: coordinate array,\n    - ``'i'``: index vector.\n\"\"\"\n\nlat = \"\"\"\n``int`` (read-only) The index into the world coord array containing\nlatitude values.\n\"\"\"\n\nlatpole = \"\"\"\n``double`` The native latitude of the celestial pole, ``LATPOLEa`` (deg).\n\"\"\"\n\nlattyp = \"\"\"\n``string`` (read-only) Celestial axis type for latitude.\n\nFor example, \"RA\", \"DEC\", \"GLON\", \"GLAT\", etc. extracted from \"RA--\",\n\"DEC-\", \"GLON\", \"GLAT\", etc. in the first four characters of\n``CTYPEia`` but with trailing dashes removed.\n\"\"\"\n\nlng = \"\"\"\n``int`` (read-only) The index into the world coord array containing\nlongitude values.\n\"\"\"\n\nlngtyp = \"\"\"\n``string`` (read-only) Celestial axis type for longitude.\n\nFor example, \"RA\", \"DEC\", \"GLON\", \"GLAT\", etc. extracted from \"RA--\",\n\"DEC-\", \"GLON\", \"GLAT\", etc. in the first four characters of\n``CTYPEia`` but with trailing dashes removed.\n\"\"\"\n\nlonpole = \"\"\"\n``double`` The native longitude of the celestial pole.\n\n``LONPOLEa`` (deg).\n\"\"\"\n\nM = \"\"\"\n``int`` (read-only) Number of tabular coordinate axes.\n\"\"\"\n\nm = \"\"\"\n``int`` (read-only)\n\nArray axis number for index vectors.\n\"\"\"\n\nmap = \"\"\"\n``int array[M]`` Association between axes.\n\nA vector of length `~astropy.wcs.Tabprm.M` that defines\nthe association between axis *m* in the *M*-dimensional coordinate\narray (1 <= *m* <= *M*) and the indices of the intermediate world\ncoordinate and world coordinate arrays.\n\nWhen the intermediate and world coordinate arrays contain the full\ncomplement of coordinate elements in image-order, as will usually be\nthe case, then ``map[m-1] == i-1`` for axis *i* in the *N*-dimensional\nimage (1 <= *i* <= *N*).  In terms of the FITS keywords::\n\n    map[PVi_3a - 1] == i - 1.\n\nHowever, a different association may result if the intermediate\ncoordinates, for example, only contains a (relevant) subset of\nintermediate world coordinate elements.  For example, if *M* == 1 for\nan image with *N* > 1, it is possible to fill the intermediate\ncoordinates with the relevant coordinate element with ``nelem`` set to\n1.  In this case ``map[0] = 0`` regardless of the value of *i*.\n\"\"\"\n\nmix = \"\"\"\nmix(mixpix, mixcel, vspan, vstep, viter, world, pixcrd, origin)\n\nGiven either the celestial longitude or latitude plus an element of\nthe pixel coordinate, solves for the remaining elements by iterating\non the unknown celestial coordinate element using\n`~astropy.wcs.Wcsprm.s2p`.\n\nParameters\n----------\n...\nnative pole of a projection in which the pole is represented as a\nfinite curve, for example the zenithals and conics.  In such cases two\nor more valid solutions may exist but `~astropy.wcs.Wcsprm.mix` only\never returns one.\n\nBecause of its generality, `~astropy.wcs.Wcsprm.mix` is very\ncompute-intensive.  For compute-limited applications, more efficient\nspecial-case solvers could be written for simple projections, for\nexample non-oblique cylindrical projections.\n\"\"\".format(__.ORIGIN())\n\nmjdavg = \"\"\"\n``double`` Modified Julian Date corresponding to ``DATE-AVG``.\n\n``(MJD = JD - 2400000.5)``.\n\nAn undefined value is represented by NaN.\n\nSee also\n--------\nastropy.wcs.Wcsprm.mjdobs\n\"\"\"\n\nmjdobs = \"\"\"\n``double`` Modified Julian Date corresponding to ``DATE-OBS``.\n\n``(MJD = JD - 2400000.5)``.\n\nAn undefined value is represented by NaN.\n\nSee also\n--------\nastropy.wcs.Wcsprm.mjdavg\n\"\"\"\n\nname = \"\"\"\n``string`` The name given to the coordinate representation\n``WCSNAMEa``.\n\"\"\"\n\nnaxis = \"\"\"\n``int`` (read-only) The number of axes (pixel and coordinate).\n\nGiven by the ``NAXIS`` or ``WCSAXESa`` keyvalues.\n\nThe number of coordinate axes is determined at parsing time, and can\nnot be subsequently changed.\n\nIt is determined from the highest of the following:\n\n  1. ``NAXIS``\n\n  2. ``WCSAXESa``\n\n  3. The highest axis number in any parameterized WCS keyword.  The\n     keyvalue, as well as the keyword, must be syntactically valid\n     otherwise it will not be considered.\n\nIf none of these keyword types is present, i.e. if the header only\ncontains auxiliary WCS keywords for a particular coordinate\nrepresentation, then no coordinate description is constructed for it.\n\nThis value may differ for different coordinate representations of the\nsame image.\n\"\"\"\n\nnc = \"\"\"\n``int`` (read-only) Total number of coord vectors in the coord array.\n\nTotal number of coordinate vectors in the coordinate array being the\nproduct K_1 * K_2 * ... * K_M.\n\"\"\"\n\nndim = \"\"\"\n``int`` (read-only)\n\nExpected dimensionality of the wcstab array.\n\"\"\"\n\nobsgeo = \"\"\"\n``double array[3]`` Location of the observer in a standard terrestrial\nreference frame.\n\n``OBSGEO-X``, ``OBSGEO-Y``, ``OBSGEO-Z`` (in meters).\n\nAn undefined value is represented by NaN.\n\"\"\"\n\np0 = \"\"\"\n``int array[M]`` Interpolated indices into the coordinate array.\n\nVector of length `~astropy.wcs.Tabprm.M` of interpolated\nindices into the coordinate array such that Upsilon_m, as defined in\nPaper III, is equal to ``(p0[m] + 1) + delta[m]``.\n\"\"\"\n\np2s = \"\"\"\np2s(pixcrd, origin)\n\nConverts pixel to world coordinates.\n\nParameters\n----------\n\npixcrd : double array[ncoord][nelem]\n    Array of pixel coordinates.\n...\n    Invalid coordinate transformation parameters.\n\nInvalidTransformError\n    Ill-conditioned coordinate transformation parameters.\n\nSee also\n--------\nastropy.wcs.Wcsprm.lat, astropy.wcs.Wcsprm.lng\n    Definition of the latitude and longitude axes\n\"\"\".format(__.ORIGIN())\n\np4_pix2foc = \"\"\"\np4_pix2foc(*pixcrd, origin*) -> double array[ncoord][nelem]\n\nConvert pixel coordinates to focal plane coordinates using `distortion\npaper`_ lookup-table correction.\n\nParameters\n----------\npixcrd : double array[ncoord][nelem].\n    Array of pixel coordinates.\n\n{0}\n\nReturns\n-------\nfoccrd : double array[ncoord][nelem]\n    Returns an array of focal plane coordinates.\n\nRaises\n------\nMemoryError\n    Memory allocation failed.\n\nValueError\n    Invalid coordinate transformation parameters.\n\"\"\".format(__.ORIGIN())\n\npc = \"\"\"\n``double array[naxis][naxis]`` The ``PCi_ja`` (pixel coordinate)\ntransformation matrix.\n\nThe order is::\n\n  [[PC1_1, PC1_2],\n   [PC2_1, PC2_2]]\n\nFor historical compatibility, three alternate specifications of the\nlinear transformations are available in wcslib.  The canonical\n``PCi_ja`` with ``CDELTia``, ``CDi_ja``, and the deprecated\n``CROTAia`` keywords.  Although the latter may not formally co-exist\nwith ``PCi_ja``, the approach here is simply to ignore them if given\nin conjunction with ``PCi_ja``.\n\n`~astropy.wcs.Wcsprm.has_pc`, `~astropy.wcs.Wcsprm.has_cd` and\n`~astropy.wcs.Wcsprm.has_crota` can be used to determine which of\nthese alternatives are present in the header.\n\nThese alternate specifications of the linear transformation matrix are\ntranslated immediately to ``PCi_ja`` by `~astropy.wcs.Wcsprm.set` and\nare nowhere visible to the lower-level routines.  In particular,\n`~astropy.wcs.Wcsprm.set` resets `~astropy.wcs.Wcsprm.cdelt` to unity\nif ``CDi_ja`` is present (and no ``PCi_ja``).  If no ``CROTAia`` is\nassociated with the latitude axis, `~astropy.wcs.Wcsprm.set` reverts\nto a unity ``PCi_ja`` matrix.\n\"\"\"\n\nphi0 = \"\"\"\n``double`` The native latitude of the fiducial point.\n\nThe point whose celestial coordinates are given in ``ref[1:2]``.  If\nundefined (NaN) the initialization routine, `~astropy.wcs.Wcsprm.set`,\nwill set this to a projection-specific default.\n\nSee also\n--------\nastropy.wcs.Wcsprm.theta0\n\"\"\"\n\npix2foc = \"\"\"\npix2foc(*pixcrd, origin*) -> double array[ncoord][nelem]\n\nPerform both `SIP`_ polynomial and `distortion paper`_ lookup-table\ncorrection in parallel.\n\nParameters\n----------\npixcrd : double array[ncoord][nelem]\n    Array of pixel coordinates.\n\n{0}\n\nReturns\n-------\nfoccrd : double array[ncoord][nelem]\n    Returns an array of focal plane coordinates.\n\nRaises\n------\nMemoryError\n    Memory allocation failed.\n\nValueError\n    Invalid coordinate transformation parameters.\n\"\"\".format(__.ORIGIN())\n\npiximg_matrix = \"\"\"\n``double array[2][2]`` (read-only) Matrix containing the product of\nthe ``CDELTia`` diagonal matrix and the ``PCi_ja`` matrix.\n\"\"\"\n\nprint_contents = \"\"\"\nprint_contents()\n\nPrint the contents of the `~astropy.wcs.Wcsprm` object to stdout.\nProbably only useful for debugging purposes, and may be removed in the\nfuture.\n\nTo get a string of the contents, use `repr`.\n\"\"\"\n\nprint_contents_tabprm = \"\"\"\nprint_contents()\n\nPrint the contents of the `~astropy.wcs.Tabprm` object to\nstdout.  Probably only useful for debugging purposes, and may be\nremoved in the future.\n\nTo get a string of the contents, use `repr`.\n\"\"\"\n\nradesys = \"\"\"\n``string`` The equatorial or ecliptic coordinate system type,\n``RADESYSa``.\n\"\"\"\n\nrestfrq = \"\"\"\n``double`` Rest frequency (Hz) from ``RESTFRQa``.\n\nAn undefined value is represented by NaN.\n\"\"\"\n\nrestwav = \"\"\"\n``double`` Rest wavelength (m) from ``RESTWAVa``.\n\nAn undefined value is represented by NaN.\n\"\"\"\n\nrow = \"\"\"\n``int`` (read-only)\n\nTable row number.\n\"\"\"\n\ns2p = \"\"\"\ns2p(world, origin)\n\nTransforms world coordinates to pixel coordinates.\n\nParameters\n----------\nworld : double array[ncoord][nelem]\n    Array of world coordinates, in decimal degrees.\n\n...\n   Invalid coordinate transformation parameters.\n\nInvalidTransformError\n    Ill-conditioned coordinate transformation parameters.\n\nSee also\n--------\nastropy.wcs.Wcsprm.lat, astropy.wcs.Wcsprm.lng\n    Definition of the latitude and longitude axes\n\"\"\".format(__.ORIGIN())\n\nsense = \"\"\"\n``int array[M]`` +1 if monotonically increasing, -1 if decreasing.\n\nA vector of length `~astropy.wcs.Tabprm.M` whose elements\nindicate whether the corresponding indexing vector is monotonically\nincreasing (+1), or decreasing (-1).\n\"\"\"\n\nset = \"\"\"\nset()\n\nSets up a WCS object for use according to information supplied within\nit.\n\nNote that this routine need not be called directly; it will be invoked\nby `~astropy.wcs.Wcsprm.p2s` and `~astropy.wcs.Wcsprm.s2p` if\nnecessary.\n\n...\n\nValueError\n    Invalid parameter value.\n\nInvalidTransformError\n    Invalid coordinate transformation parameters.\n\nInvalidTransformError\n    Ill-conditioned coordinate transformation parameters.\n\"\"\"\n\nset_tabprm = \"\"\"\nset()\n\nAllocates memory for work arrays.\n\nAlso sets up the class according to information supplied within it.\n\nNote that this routine need not be called directly; it will be invoked\nby functions that need it.\n\nRaises\n------\nMemoryError\n    Memory allocation failed.\n\nInvalidTabularParameters\n    Invalid tabular parameters.\n\"\"\"\n\nset_ps = \"\"\"\nset_ps(ps)\n\nSets ``PSi_ma`` keywords for each *i* and *m*.\n\nParameters\n----------\nps : sequence of tuples\n\n    The input must be a sequence of tuples of the form (*i*, *m*,\n    *value*):\n\n    - *i*: int.  Axis number, as in ``PSi_ma``, (i.e. 1-relative)\n\n    - *m*: int.  Parameter number, as in ``PSi_ma``, (i.e. 0-relative)\n\n    - *value*: string.  Parameter value.\n\nSee also\n--------\nastropy.wcs.Wcsprm.get_ps\n\"\"\"\n\nset_pv = \"\"\"\nset_pv(pv)\n\nSets ``PVi_ma`` keywords for each *i* and *m*.\n\nParameters\n----------\npv : list of tuples\n\n    The input must be a sequence of tuples of the form (*i*, *m*,\n    *value*):\n\n    - *i*: int.  Axis number, as in ``PVi_ma``, (i.e. 1-relative)\n\n    - *m*: int.  Parameter number, as in ``PVi_ma``, (i.e. 0-relative)\n\n    - *value*: float.  Parameter value.\n\nSee also\n--------\nastropy.wcs.Wcsprm.get_pv\n\"\"\"\n\nsip = \"\"\"\nGet/set the `~astropy.wcs.Sip` object for performing `SIP`_ distortion\ncorrection.\n\"\"\"\n\nSip = \"\"\"\nSip(*a, b, ap, bp, crpix*)\n\nThe `~astropy.wcs.Sip` class performs polynomial distortion correction\nusing the `SIP`_ convention in both directions.\n\nParameters\n----------\na : double array[m+1][m+1]\n    The ``A_i_j`` polynomial for pixel to focal plane transformation.\n...\n\ncrpix : double array[2]\n    The reference pixel.\n\nNotes\n-----\nShupe, D. L., M. Moshir, J. Li, D. Makovoz and R. Narron.  2005.\n\"The SIP Convention for Representing Distortion in FITS Image\nHeaders.\"  ADASS XIV.\n\"\"\"\n\nsip_foc2pix = \"\"\"\nsip_foc2pix(*foccrd, origin*) -> double array[ncoord][nelem]\n\nConvert focal plane coordinates to pixel coordinates using the `SIP`_\npolynomial distortion convention.\n\nParameters\n----------\nfoccrd : double array[ncoord][nelem]\n    Array of focal plane coordinates.\n\n{0}\n\nReturns\n-------\npixcrd : double array[ncoord][nelem]\n    Returns an array of pixel coordinates.\n\nRaises\n------\nMemoryError\n    Memory allocation failed.\n\nValueError\n    Invalid coordinate transformation parameters.\n\"\"\".format(__.ORIGIN())\n\nsip_pix2foc = \"\"\"\nsip_pix2foc(*pixcrd, origin*) -> double array[ncoord][nelem]\n\nConvert pixel coordinates to focal plane coordinates using the `SIP`_\npolynomial distortion convention.\n\nParameters\n----------\npixcrd : double array[ncoord][nelem]\n    Array of pixel coordinates.\n\n{0}\n\nReturns\n-------\nfoccrd : double array[ncoord][nelem]\n    Returns an array of focal plane coordinates.\n\nRaises\n------\nMemoryError\n    Memory allocation failed.\n\nValueError\n    Invalid coordinate transformation parameters.\n\"\"\".format(__.ORIGIN())\n\nspcfix = \"\"\"\nspcfix() -> int\n\nTranslates AIPS-convention spectral coordinate types.  {``FREQ``,\n``VELO``, ``FELO``}-{``OBS``, ``HEL``, ``LSR``} (e.g. ``FREQ-LSR``,\n``VELO-OBS``, ``FELO-HEL``)\n\nReturns\n-------\nsuccess : int\n    Returns ``0`` for success; ``-1`` if no change required.\n\"\"\"\n\nspec = \"\"\"\n``int`` (read-only) The index containing the spectral axis values.\n\"\"\"\n\nspecsys = \"\"\"\n``string`` Spectral reference frame (standard of rest), ``SPECSYSa``.\n\nSee also\n--------\nastropy.wcs.Wcsprm.ssysobs, astropy.wcs.Wcsprm.velosys\n\"\"\"\n\nsptr = \"\"\"\nsptr(ctype, i=-1)\n\nTranslates the spectral axis in a WCS object.\n\nFor example, a ``FREQ`` axis may be translated into ``ZOPT-F2W`` and\nvice versa.\n\nParameters\n----------\n...\n\nInvalidTransformError\n    Invalid coordinate transformation parameters.\n\nInvalidTransformError\n    Ill-conditioned coordinate transformation parameters.\n\nInvalidSubimageSpecificationError\n    Invalid subimage specification (no spectral axis).\n\"\"\"\n\nssysobs = \"\"\"\n``string`` Spectral reference frame.\n\nThe spectral reference frame in which there is no differential\nvariation in the spectral coordinate across the field-of-view,\n``SSYSOBSa``.\n\nSee also\n--------\nastropy.wcs.Wcsprm.specsys, astropy.wcs.Wcsprm.velosys\n\"\"\"\n\nssyssrc = \"\"\"\n``string`` Spectral reference frame for redshift.\n\nThe spectral reference frame (standard of rest) in which the redshift\nwas measured, ``SSYSSRCa``.\n\"\"\"\n\nsub = \"\"\"\nsub(axes)\n\nExtracts the coordinate description for a subimage from a\n`~astropy.wcs.WCS` object.\n\nThe world coordinate system of the subimage must be separable in the\nsense that the world coordinates at any point in the subimage must\ndepend only on the pixel coordinates of the axes extracted.  In\npractice, this means that the ``PCi_ja`` matrix of the original image\n...\nThe last of these specifies all axis types other than spectral or\nStokes.  Extraction is done in the order specified by ``axes``, i.e. a\nlongitude axis (if present) would be extracted first (via ``axes[0]``)\nand not subsequently (via ``axes[3]``).  Likewise for the latitude and\ncubeface axes in this example.\n\nThe number of dimensions in the returned object may be less than or\ngreater than the length of ``axes``.  However, it will never exceed the\nnumber of axes in the input image.\n\"\"\"\n\ntab = \"\"\"\n``list of Tabprm`` Tabular coordinate objects.\n\nA list of tabular coordinate objects associated with this WCS.\n\"\"\"\n\nTabprm = \"\"\"\nA class to store the information related to tabular coordinates,\ni.e., coordinates that are defined via a lookup table.\n\nThis class can not be constructed directly from Python, but instead is\nreturned from `~astropy.wcs.Wcsprm.tab`.\n\"\"\"\n\ntheta0 = \"\"\"\n``double``  The native longitude of the fiducial point.\n\nThe point whose celestial coordinates are given in ``ref[1:2]``.  If\nundefined (NaN) the initialization routine, `~astropy.wcs.Wcsprm.set`,\nwill set this to a projection-specific default.\n\nSee also\n--------\nastropy.wcs.Wcsprm.phi0\n\"\"\"\n\nto_header = \"\"\"\nto_header(relax=False)\n\n`to_header` translates a WCS object into a FITS header.\n\nThe details of the header depends on context:\n\n    - If the `~astropy.wcs.Wcsprm.colnum` member is non-zero then a\n      binary table image array header will be produced.\n\n...\n      standard.\n\n    - `int`: a bit field selecting specific extensions to write.\n      See :ref:`relaxwrite` for details.\n\nReturns\n-------\nheader : str\n    Raw FITS header as a string.\n\"\"\"\n\nttype = \"\"\"\n``str`` (read-only)\n\n``TTYPEn`` identifying the column of the binary table that contains\nthe wcstab array.\n\"\"\"\n\nunitfix = \"\"\"\nunitfix(translate_units='')\n\nTranslates non-standard ``CUNITia`` keyvalues.\n\nFor example, ``DEG`` -> ``deg``, also stripping off unnecessary\nwhitespace.\n\nParameters\n----------\n...\n    - If the string contains ``\\\"d\\\"``, translate ``\\\"D\\\"`` to ``\\\"d\\\"``.\n\n    Thus ``''`` doesn't do any unsafe translations, whereas ``'shd'``\n    does all of them.\n\nReturns\n-------\nsuccess : int\n    Returns ``0`` for success; ``-1`` if no change required.\n\"\"\"\n\nvelangl = \"\"\"\n``double`` Velocity angle.\n\nThe angle in degrees that should be used to decompose an observed\nvelocity into radial and transverse components.\n\nAn undefined value is represented by NaN.\n\"\"\"\n\nvelosys = \"\"\"\n``double`` Relative radial velocity.\n\nThe relative radial velocity (m/s) between the observer and the\nselected standard of rest in the direction of the celestial reference\ncoordinate, ``VELOSYSa``.\n\nAn undefined value is represented by NaN.\n\nSee also\n--------\nastropy.wcs.Wcsprm.specsys, astropy.wcs.Wcsprm.ssysobs\n\"\"\"\n\nvelref = \"\"\"\n``int`` AIPS velocity code.\n\nFrom ``VELREF`` keyword.\n\"\"\"\n\nwcs = \"\"\"\nA `~astropy.wcs.Wcsprm` object to perform the basic `wcslib`_ WCS\ntransformation.\n\"\"\"\n\nWcs = \"\"\"\nWcs(*sip, cpdis, wcsprm, det2im*)\n\nWcs objects amalgamate basic WCS (as provided by `wcslib`_), with\n`SIP`_ and `distortion paper`_ operations.\n\nTo perform all distortion corrections and WCS transformation, use\n``all_pix2world``.\n\nParameters\n----------\nsip : `~astropy.wcs.Sip` object or `None`\n\ncpdis : A pair of `~astropy.wcs.DistortionLookupTable` objects, or\n  ``(None, None)``.\n\nwcsprm : `~astropy.wcs.Wcsprm` object\n\ndet2im : A pair of `~astropy.wcs.DistortionLookupTable` objects, or\n   ``(None, None)``.\n\"\"\"\n\nWcsprm = \"\"\"\nWcsprm(header=None, key=' ', relax=False, naxis=2, keysel=0, colsel=None)\n\n`~astropy.wcs.Wcsprm` performs the core WCS transformations.\n\n.. note::\n    The members of this object correspond roughly to the key/value\n    pairs in the FITS header.  However, they are adjusted and\n    normalized in a number of ways that make performing the WCS\n    transformation easier.  Therefore, they can not be relied upon to\n...\n------\nMemoryError\n     Memory allocation failed.\n\nValueError\n     Invalid key.\n\nKeyError\n     Key not found in FITS header.\n\"\"\"\n\nWtbarr = \"\"\"\nClasses to construct coordinate lookup tables from a binary table\nextension (BINTABLE).\n\nThis class can not be constructed directly from Python, but instead is\nreturned from `~astropy.wcs.Wcsprm.wtb`.\n\"\"\"\n\nzsource = \"\"\"\n``double`` The redshift, ``ZSOURCEa``, of the source.\n\nAn undefined value is represented by NaN.\n\"\"\"\n\nWcsError = \"\"\"\nBase class of all invalid WCS errors.\n\"\"\"\n\nSingularMatrix = \"\"\"\nSingularMatrixError()\n\nThe linear transformation matrix is singular.\n\"\"\"\n\nInconsistentAxisTypes = \"\"\"\nInconsistentAxisTypesError()\n\nThe WCS header inconsistent or unrecognized coordinate axis type(s).\n\"\"\"\n\nInvalidTransform = \"\"\"\nInvalidTransformError()\n\nThe WCS transformation is invalid, or the transformation parameters\nare invalid.\n\"\"\"\n\nInvalidCoordinate = \"\"\"\nInvalidCoordinateError()\n\nOne or more of the world coordinates is invalid.\n\"\"\"\n\nNoSolution = \"\"\"\nNoSolutionError()\n\nNo solution can be found in the given interval.\n\"\"\"\n\nInvalidSubimageSpecification = \"\"\"\nInvalidSubimageSpecificationError()\n\nThe subimage specification is invalid.\n\"\"\"\n\nNonseparableSubimageCoordinateSystem = \"\"\"\nNonseparableSubimageCoordinateSystemError()\n\nNon-separable subimage coordinate system.\n\"\"\"\n\nNoWcsKeywordsFound = \"\"\"\nNoWcsKeywordsFoundError()\n\nNo WCS keywords were found in the given header.\n\"\"\"\n\nInvalidTabularParameters = \"\"\"\nInvalidTabularParametersError()\n\nThe given tabular parameters are invalid.\n\"\"\"\n\n```\n\n\n###\n\nPlease provide the complete set of locations as either a class name, a function name, or a variable name.\nNote that if you include a class, you do not need to list its specific methods.\nYou can include either the entire class or don't include the class name and instead include specific methods in the class.\n### Examples:\n```\nfull_path1/file1.py\nfunction: my_function_1\nclass: MyClass1\nfunction: MyClass2.my_method\n\nfull_path2/file2.py\nvariable: my_var\nfunction: MyClass3.my_method\n\nfull_path3/file3.py\nfunction: my_function_2\nfunction: my_function_3\nfunction: MyClass4.my_method_1\nclass: MyClass5\n```\n\nReturn just the locations wrapped with ```.\n"}], "found_edit_locs": {"astropy/wcs/wcs.py": ["\nfunction: WCS.wcs_pix2world\nline: 1352\nfunction: WCS._array_converter\nline: 1245\nline: 1246\nline: 1258\nline: 1259"]}, "additional_artifact_loc_edit_location": [{"raw_output_loc": ["```\nastropy/wcs/wcs.py\nfunction: WCS.wcs_pix2world\nline: 1352\n\nastropy/wcs/wcs.py\nfunction: WCS._array_converter\nline: 1245\nline: 1246\nline: 1258\nline: 1259\n```", "```\nastropy/wcs/wcs.py\nline: 1245\nline: 1258\n\nastropy/wcs/wcs.py\nfunction: wcs_pix2world\nline: 1350\n```", "```\nastropy/wcs/wcs.py\nfunction: WCS.wcs_pix2world\nline: 1349\nline: 1353\n\nastropy/wcs/wcs.py\nfunction: WCS._array_converter\nline: 1245\nline: 1259\n```", "```\nastropy/wcs/wcs.py\nfunction: WCS._array_converter\nline: 1245\nline: 1258\nline: 1271\n\nastropy/wcs/wcs.py\nfunction: WCS.wcs_pix2world\nline: 1352\n```"]}], "edit_loc_traj": {"prompt": "\nPlease review the following GitHub problem description and relevant files, and provide a set of locations that need to be edited to fix the issue.\nThe locations can be specified as class names, function or method names, or exact line numbers that require modification.\n\n### GitHub Problem Description ###\nIssue when passing empty lists/arrays to WCS transformations\nThe following should not fail but instead should return empty lists/arrays:\r\n\r\n```\r\nIn [1]: from astropy.wcs import WCS\r\n\r\nIn [2]: wcs = WCS('2MASS_h.fits')\r\n\r\nIn [3]: wcs.wcs_pix2world([], [], 0)\r\n---------------------------------------------------------------------------\r\nInconsistentAxisTypesError                Traceback (most recent call last)\r\n<ipython-input-3-e2cc0e97941a> in <module>()\r\n----> 1 wcs.wcs_pix2world([], [], 0)\r\n\r\n~/Dropbox/Code/Astropy/astropy/astropy/wcs/wcs.py in wcs_pix2world(self, *args, **kwargs)\r\n   1352         return self._array_converter(\r\n   1353             lambda xy, o: self.wcs.p2s(xy, o)['world'],\r\n-> 1354             'output', *args, **kwargs)\r\n   1355     wcs_pix2world.__doc__ = \"\"\"\r\n   1356         Transforms pixel coordinates to world coordinates by doing\r\n\r\n~/Dropbox/Code/Astropy/astropy/astropy/wcs/wcs.py in _array_converter(self, func, sky, ra_dec_order, *args)\r\n   1267                     \"a 1-D array for each axis, followed by an origin.\")\r\n   1268 \r\n-> 1269             return _return_list_of_arrays(axes, origin)\r\n   1270 \r\n   1271         raise TypeError(\r\n\r\n~/Dropbox/Code/Astropy/astropy/astropy/wcs/wcs.py in _return_list_of_arrays(axes, origin)\r\n   1223             if ra_dec_order and sky == 'input':\r\n   1224                 xy = self._denormalize_sky(xy)\r\n-> 1225             output = func(xy, origin)\r\n   1226             if ra_dec_order and sky == 'output':\r\n   1227                 output = self._normalize_sky(output)\r\n\r\n~/Dropbox/Code/Astropy/astropy/astropy/wcs/wcs.py in <lambda>(xy, o)\r\n   1351             raise ValueError(\"No basic WCS settings were created.\")\r\n   1352         return self._array_converter(\r\n-> 1353             lambda xy, o: self.wcs.p2s(xy, o)['world'],\r\n   1354             'output', *args, **kwargs)\r\n   1355     wcs_pix2world.__doc__ = \"\"\"\r\n\r\nInconsistentAxisTypesError: ERROR 4 in wcsp2s() at line 2646 of file cextern/wcslib/C/wcs.c:\r\nncoord and/or nelem inconsistent with the wcsprm.\r\n```\n\n\n###\n### astropy/wcs/wcs.py\n...\n1198|        else:\n1199|            if self.wcs.lng < 0 or self.wcs.lat < 0:\n1200|                raise ValueError(\n1201|                    \"WCS does not have both longitude and latitude celestial \"\n1202|                    \"axes, therefore (ra, dec) data can not be returned\")\n1203|            out = np.empty((sky.shape[0], 2))\n1204|            out[:, 0] = sky[:, self.wcs.lng]\n1205|            out[:, 1] = sky[:, self.wcs.lat]\n1206|            return out\n1207|\n1208|    def _array_converter(self, func, sky, *args, ra_dec_order=False):\n1209|        \"\"\"\n1210|        A helper function to support reading either a pair of arrays\n1211|        or a single Nx2 array.\n1212|        \"\"\"\n1213|\n1214|        def _return_list_of_arrays(axes, origin):\n1215|            try:\n1216|                axes = np.broadcast_arrays(*axes)\n1217|            except ValueError:\n1218|                raise ValueError(\n1219|                    \"Coordinate arrays are not broadcastable to each other\")\n1220|\n1221|            xy = np.hstack([x.reshape((x.size, 1)) for x in axes])\n1222|\n1223|            if ra_dec_order and sky == 'input':\n1224|                xy = self._denormalize_sky(xy)\n1225|            output = func(xy, origin)\n1226|            if ra_dec_order and sky == 'output':\n1227|                output = self._normalize_sky(output)\n1228|                return (output[:, 0].reshape(axes[0].shape),\n1229|                        output[:, 1].reshape(axes[0].shape))\n1230|            return [output[:, i].reshape(axes[0].shape)\n1231|                    for i in range(output.shape[1])]\n1232|\n1233|        def _return_single_array(xy, origin):\n1234|            if xy.shape[-1] != self.naxis:\n1235|                raise ValueError(\n1236|                    \"When providing two arguments, the array must be \"\n1237|                    \"of shape (N, {0})\".format(self.naxis))\n1238|            if ra_dec_order and sky == 'input':\n1239|                xy = self._denormalize_sky(xy)\n1240|            result = func(xy, origin)\n1241|            if ra_dec_order and sky == 'output':\n1242|                result = self._normalize_sky(result)\n1243|            return result\n1244|\n1245|        if len(args) == 2:\n1246|            try:\n1247|                xy, origin = args\n1248|                xy = np.asarray(xy)\n1249|                origin = int(origin)\n1250|            except Exception:\n1251|                raise TypeError(\n1252|                    \"When providing two arguments, they must be \"\n1253|                    \"(coords[N][{0}], origin)\".format(self.naxis))\n1254|            if self.naxis == 1 and len(xy.shape) == 1:\n1255|                return _return_list_of_arrays([xy], origin)\n1256|            return _return_single_array(xy, origin)\n1257|\n1258|        elif len(args) == self.naxis + 1:\n1259|            axes = args[:-1]\n1260|            origin = args[-1]\n1261|            try:\n1262|                axes = [np.asarray(x) for x in axes]\n1263|                origin = int(origin)\n1264|            except Exception:\n1265|                raise TypeError(\n1266|                    \"When providing more than two arguments, they must be \" +\n1267|                    \"a 1-D array for each axis, followed by an origin.\")\n1268|\n1269|            return _return_list_of_arrays(axes, origin)\n1270|\n1271|        raise TypeError(\n1272|            \"WCS projection has {0} dimensions, so expected 2 (an Nx{0} array \"\n1273|            \"and the origin argument) or {1} arguments (the position in each \"\n1274|            \"dimension, and the origin argument). Instead, {2} arguments were \"\n1275|            \"given.\".format(\n1276|                self.naxis, self.naxis + 1, len(args)))\n1277|\n1278|    def all_pix2world(self, *args, **kwargs):\n1279|        return self._array_converter(\n1280|            self._all_pix2world, 'output', *args, **kwargs)\n1281|    all_pix2world.__doc__ = \"\"\"\n1282|        Transforms pixel coordinates to world coordinates.\n1283|\n1284|        Performs all of the following in series:\n1285|\n1286|            - Detector to image plane correction (if present in the\n...\n1339|\n1340|        InvalidTransformError\n1341|            Invalid coordinate transformation parameters.\n1342|\n1343|        InvalidTransformError\n1344|            Ill-conditioned coordinate transformation parameters.\n1345|        \"\"\".format(__.TWO_OR_MORE_ARGS('naxis', 8),\n1346|                   __.RA_DEC_ORDER(8),\n1347|                   __.RETURNS('sky coordinates, in degrees', 8))\n1348|\n1349|    def wcs_pix2world(self, *args, **kwargs):\n1350|        if self.wcs is None:\n1351|            raise ValueError(\"No basic WCS settings were created.\")\n1352|        return self._array_converter(\n1353|            lambda xy, o: self.wcs.p2s(xy, o)['world'],\n1354|            'output', *args, **kwargs)\n1355|    wcs_pix2world.__doc__ = \"\"\"\n1356|        Transforms pixel coordinates to world coordinates by doing\n1357|        only the basic `wcslib`_ transformation.\n1358|\n1359|        No `SIP`_ or `distortion paper`_ table lookup correction is\n1360|        applied.  To perform distortion correction, see\n1361|        `~astropy.wcs.WCS.all_pix2world`,\n1362|        `~astropy.wcs.WCS.sip_pix2foc`, `~astropy.wcs.WCS.p4_pix2foc`,\n1363|        or `~astropy.wcs.WCS.pix2foc`.\n1364|\n...\n\n\n\n\n###\n\nPlease provide the class name, function or method name, or the exact line numbers that need to be edited.\nThe possible location outputs should be either \"class\", \"function\" or \"line\".\n\n### Examples:\n```\nfull_path1/file1.py\nline: 10\nclass: MyClass1\nline: 51\n\nfull_path2/file2.py\nfunction: MyClass2.my_method\nline: 12\n\nfull_path3/file3.py\nfunction: my_function\nline: 24\nline: 156\n```\n\nReturn just the location(s) wrapped with ```.\n", "response": ["```\nastropy/wcs/wcs.py\nfunction: WCS.wcs_pix2world\nline: 1352\n\nastropy/wcs/wcs.py\nfunction: WCS._array_converter\nline: 1245\nline: 1246\nline: 1258\nline: 1259\n```", "```\nastropy/wcs/wcs.py\nline: 1245\nline: 1258\n\nastropy/wcs/wcs.py\nfunction: wcs_pix2world\nline: 1350\n```", "```\nastropy/wcs/wcs.py\nfunction: WCS.wcs_pix2world\nline: 1349\nline: 1353\n\nastropy/wcs/wcs.py\nfunction: WCS._array_converter\nline: 1245\nline: 1259\n```", "```\nastropy/wcs/wcs.py\nfunction: WCS._array_converter\nline: 1245\nline: 1258\nline: 1271\n\nastropy/wcs/wcs.py\nfunction: WCS.wcs_pix2world\nline: 1352\n```"], "usage": {"completion_tokens": 238, "prompt_tokens": 2210}}}
{"instance_id": "astropy__astropy-14995", "found_files": ["astropy/nddata/nddata.py", "astropy/nddata/mixins/ndarithmetic.py", "astropy/nddata/utils.py", "astropy/utils/masked/core.py"], "additional_artifact_loc_file": {}, "file_traj": {}, "found_related_locs": {"astropy/nddata/nddata.py": ["class: NDData"], "astropy/nddata/mixins/ndarithmetic.py": ["class: NDArithmeticMixin\nfunction: NDArithmeticMixin._arithmetic\nfunction: NDArithmeticMixin._arithmetic_mask\nfunction: NDArithmeticMixin.multiply"], "astropy/nddata/utils.py": [""]}, "additional_artifact_loc_related": [{"raw_output_loc": "```\nastropy/nddata/nddata.py\nclass: NDData\n\nastropy/nddata/mixins/ndarithmetic.py\nclass: NDArithmeticMixin\nfunction: NDArithmeticMixin._arithmetic\nfunction: NDArithmeticMixin._arithmetic_mask\nfunction: NDArithmeticMixin.multiply\n```"}], "related_loc_traj": [{"response": "```\nastropy/nddata/nddata.py\nclass: NDData\n\nastropy/nddata/mixins/ndarithmetic.py\nclass: NDArithmeticMixin\nfunction: NDArithmeticMixin._arithmetic\nfunction: NDArithmeticMixin._arithmetic_mask\nfunction: NDArithmeticMixin.multiply\n```", "usage": {"completion_tokens": 64, "prompt_tokens": 2444}, "prompt": "\nPlease look through the following GitHub Problem Description and the Skeleton of Relevant Files.\nIdentify all locations that need inspection or editing to fix the problem, including directly related areas as well as any potentially related global variables, functions, and classes.\nFor each location you provide, either give the name of the class, the name of a method in a class, the name of a function, or the name of a global variable.\n\n### GitHub Problem Description ###\nIn v5.3, NDDataRef mask propagation fails when one of the operand does not have a mask\n### Description\n\nThis applies to v5.3. \r\n\r\nIt looks like when one of the operand does not have a mask, the mask propagation when doing arithmetic, in particular with `handle_mask=np.bitwise_or` fails.  This is not a problem in v5.2.\r\n\r\nI don't know enough about how all that works, but it seems from the error that the operand without a mask is set as a mask of None's and then the bitwise_or tries to operate on an integer and a None and fails.\n\n### Expected behavior\n\nWhen one of the operand does not have mask, the mask that exists should just be copied over to the output.  Or whatever was done in that situation in v5.2 where there's no problem.\n\n### How to Reproduce\n\nThis is with v5.3.   With v5.2, there are no errors.\r\n\r\n```\r\n>>> import numpy as np\r\n>>> from astropy.nddata import NDDataRef\r\n\r\n>>> array = np.array([[0, 1, 0], [1, 0, 1], [0, 1, 0]])\r\n>>> mask = np.array([[0, 1, 64], [8, 0, 1], [2, 1, 0]])\r\n\r\n>>> nref_nomask = NDDataRef(array)\r\n>>> nref_mask = NDDataRef(array, mask=mask)\r\n\r\n# multiply no mask by constant (no mask * no mask)\r\n>>> nref_nomask.multiply(1., handle_mask=np.bitwise_or).mask   # returns nothing, no mask,  OK\r\n\r\n# multiply no mask by itself (no mask * no mask)\r\n>>> nref_nomask.multiply(nref_nomask, handle_mask=np.bitwise_or).mask # return nothing, no mask, OK\r\n\r\n# multiply mask by constant (mask * no mask)\r\n>>> nref_mask.multiply(1., handle_mask=np.bitwise_or).mask\r\n...\r\nTypeError: unsupported operand type(s) for |: 'int' and 'NoneType'\r\n\r\n# multiply mask by itself (mask * mask)\r\n>>> nref_mask.multiply(nref_mask, handle_mask=np.bitwise_or).mask\r\narray([[ 0,  1, 64],\r\n       [ 8,  0,  1],\r\n       [ 2,  1,  0]])\r\n\r\n# multiply mask by no mask (mask * no mask)\r\n>>> nref_mask.multiply(nref_nomask, handle_mask=np.bitwise_or).mask\r\n...\r\nTypeError: unsupported operand type(s) for |: 'int' and 'NoneType'\r\n```\r\n\n\n### Versions\n\n>>> import sys; print(\"Python\", sys.version)\r\nPython 3.10.11 | packaged by conda-forge | (main, May 10 2023, 19:07:22) [Clang 14.0.6 ]\r\n>>> import astropy; print(\"astropy\", astropy.__version__)\r\nastropy 5.3\r\n>>> import numpy; print(\"Numpy\", numpy.__version__)\r\nNumpy 1.24.3\r\n>>> import erfa; print(\"pyerfa\", erfa.__version__)\r\npyerfa 2.0.0.3\r\n>>> import scipy; print(\"Scipy\", scipy.__version__)\r\nScipy 1.10.1\r\n>>> import matplotlib; print(\"Matplotlib\", matplotlib.__version__)\r\nMatplotlib 3.7.1\r\n\n\n\n### Skeleton of Relevant Files ###\n\n### File: astropy/nddata/nddata.py ###\n```python\n# Licensed under a 3-clause BSD style license - see LICENSE.rst\n# This module implements the base NDData class.\n\n\n\n__all__ = [\"NDData\"]\n\n_meta_doc = \"\"\"`dict`-like : Additional meta information about the dataset.\"\"\"\n\n\nclass NDData(NDDataBase):\n\n    # Instead of a custom property use the MetaData descriptor also used for\n    # Tables. It will check if the meta is dict-like or raise an exception.\n    meta = MetaData(doc=_meta_doc, copy=False)\n\n    def __init__(\n        self,\n        data,\n        uncertainty=None,\n        mask=None,\n        wcs=None,\n        meta=None,\n        unit=None,\n        copy=False,\n        psf=None,\n    ):\n...\n\n    def __str__(self):\n...\n\n    def __repr__(self):\n...\n\n    @property\n    def data(self):\n...\n\n    @property\n    def mask(self):\n...\n\n    @mask.setter\n    def mask(self, value):\n...\n\n    @property\n    def unit(self):\n...\n\n    @property\n    def wcs(self):\n...\n\n    @wcs.setter\n    def wcs(self, wcs):\n...\n\n    @property\n    def psf(self):\n...\n\n    @psf.setter\n    def psf(self, value):\n...\n\n    @property\n    def uncertainty(self):\n...\n\n    @uncertainty.setter\n    def uncertainty(self, value):\n...\n\n```\n\n### File: astropy/nddata/mixins/ndarithmetic.py ###\n```python\n# Licensed under a 3-clause BSD style license - see LICENSE.rst\n# This module implements the Arithmetic mixin to the NDData class.\n\n\n__all__ = [\"NDArithmeticMixin\"]\n\n# Global so it doesn't pollute the class dict unnecessarily:\n\n# Docstring templates for add, subtract, multiply, divide methods.\n_arit_doc = \"\"\"\n    Performs {name} by evaluating ``self`` {op} ``operand``.\n\n    Parameters\n    ----------\n    operand, operand2 : `NDData`-like instance\n        If ``operand2`` is ``None`` or not given it will perform the operation\n        ``self`` {op} ``operand``.\n        If ``operand2`` is given it will perform ``operand`` {op} ``operand2``.\n        If the method was called on a class rather than on the instance\n...\n    -----\n    If a ``callable`` is used for ``mask``, ``wcs`` or ``meta`` the\n    callable must accept the corresponding attributes as first two\n    parameters. If the callable also needs additional parameters these can be\n    defined as ``kwargs`` and must start with ``\"wcs_\"`` (for wcs callable) or\n    ``\"meta_\"`` (for meta callable). This startstring is removed before the\n    callable is called.\n\n    ``\"first_found\"`` can also be abbreviated with ``\"ff\"``.\n    \"\"\"\n\n\nclass NDArithmeticMixin:\n\n    def _arithmetic(\n        self,\n        operation,\n        operand,\n        propagate_uncertainties=True,\n        handle_mask=np.logical_or,\n        handle_meta=None,\n        uncertainty_correlation=0,\n        compare_wcs=\"first_found\",\n        operation_ignores_mask=False,\n        axis=None,\n        **kwds,\n    ):\n...\n\n    def _arithmetic_data(self, operation, operand, **kwds):\n...\n\n    def _arithmetic_uncertainty(self, operation, operand, result, correlation, **kwds):\n...\n\n    def _arithmetic_mask(self, operation, operand, handle_mask, axis=None, **kwds):\n...\n\n    def _arithmetic_wcs(self, operation, operand, compare_wcs, **kwds):\n...\n\n    def _arithmetic_meta(self, operation, operand, handle_meta, **kwds):\n...\n\n    @sharedmethod\n    @format_doc(_arit_doc, name=\"addition\", op=\"+\")\n    def add(self, operand, operand2=None, **kwargs):\n...\n\n    @sharedmethod\n    @format_doc(_arit_doc, name=\"subtraction\", op=\"-\")\n    def subtract(self, operand, operand2=None, **kwargs):\n...\n\n    @sharedmethod\n    @format_doc(_arit_doc, name=\"multiplication\", op=\"*\")\n    def multiply(self, operand, operand2=None, **kwargs):\n...\n\n    @sharedmethod\n    @format_doc(_arit_doc, name=\"division\", op=\"/\")\n    def divide(self, operand, operand2=None, **kwargs):\n...\n\n    @sharedmethod\n    def sum(self, **kwargs):\n...\n\n    @sharedmethod\n    def mean(self, **kwargs):\n...\n\n    @sharedmethod\n    def min(self, **kwargs):\n...\n\n    @sharedmethod\n    def max(self, **kwargs):\n...\n\n    @sharedmethod\n    def _prepare_then_do_arithmetic(\n        self_or_cls, operation, operand=None, operand2=None, **kwargs\n    ):\n...\n\n```\n\n### File: astropy/nddata/utils.py ###\n```python\n# Licensed under a 3-clause BSD style license - see LICENSE.rst\n\n__all__ = [\n    \"extract_array\",\n    \"add_array\",\n    \"subpixel_indices\",\n    \"overlap_slices\",\n    \"NoOverlapError\",\n    \"PartialOverlapError\",\n    \"Cutout2D\",\n]\n\n\nclass NoOverlapError(ValueError):\n\n    pass\n\n\nclass PartialOverlapError(ValueError):\n\n    pass\n\n\ndef overlap_slices(large_array_shape, small_array_shape, position, mode=\"partial\"):\n...\n\n\ndef extract_array(\n    array_large,\n    shape,\n    position,\n    mode=\"partial\",\n    fill_value=np.nan,\n    return_position=False,\n):\n...\n\n\ndef add_array(array_large, array_small, position):\n...\n\n\ndef subpixel_indices(position, subsampling):\n...\n\n\nclass Cutout2D:\n\n    def __init__(\n        self, data, position, size, wcs=None, mode=\"trim\", fill_value=np.nan, copy=False\n    ):\n...\n\n    def to_original_position(self, cutout_position):\n...\n\n    def to_cutout_position(self, original_position):\n...\n\n    def plot_on_original(self, ax=None, fill=False, **kwargs):\n...\n\n    @staticmethod\n    def _calc_center(slices):\n...\n\n    @staticmethod\n    def _calc_bbox(slices):\n...\n\n    @lazyproperty\n    def origin_original(self):\n...\n\n    @lazyproperty\n    def origin_cutout(self):\n...\n\n    @staticmethod\n    def _round(a):\n...\n\n    @lazyproperty\n    def position_original(self):\n...\n\n    @lazyproperty\n    def position_cutout(self):\n...\n\n    @lazyproperty\n    def center_original(self):\n...\n\n    @lazyproperty\n    def center_cutout(self):\n...\n\n    @lazyproperty\n    def bbox_original(self):\n...\n\n    @lazyproperty\n    def bbox_cutout(self):\n...\n\n```\n\n\n###\n\nPlease provide the complete set of locations as either a class name, a function name, or a variable name.\nNote that if you include a class, you do not need to list its specific methods.\nYou can include either the entire class or don't include the class name and instead include specific methods in the class.\n### Examples:\n```\nfull_path1/file1.py\nfunction: my_function_1\nclass: MyClass1\nfunction: MyClass2.my_method\n\nfull_path2/file2.py\nvariable: my_var\nfunction: MyClass3.my_method\n\nfull_path3/file3.py\nfunction: my_function_2\nfunction: my_function_3\nfunction: MyClass4.my_method_1\nclass: MyClass5\n```\n\nReturn just the locations wrapped with ```.\n"}], "found_edit_locs": {"astropy/nddata/mixins/ndarithmetic.py": ["\nfunction: NDArithmeticMixin._arithmetic\nline: 323\nline: 335"], "astropy/nddata/nddata.py": ["\nfunction: NDData.__init__\nline: 329"]}, "additional_artifact_loc_edit_location": [{"raw_output_loc": ["```\nastropy/nddata/mixins/ndarithmetic.py\nfunction: NDArithmeticMixin._arithmetic\nline: 323\nline: 335\n\nastropy/nddata/nddata.py\nfunction: NDData.__init__\nline: 329\n```", "```\nastropy/nddata/mixins/ndarithmetic.py\nfunction: NDArithmeticMixin._arithmetic_mask\nline: 485\nline: 522\n```", "```\nastropy/nddata/mixins/ndarithmetic.py\nline: 322\nline: 485\n```", "```\nastropy/nddata/mixins/ndarithmetic.py\nline: 522\nfunction: NDArithmeticMixin._arithmetic_mask\nline: 526\n```"]}], "edit_loc_traj": {"prompt": "\nPlease review the following GitHub problem description and relevant files, and provide a set of locations that need to be edited to fix the issue.\nThe locations can be specified as class names, function or method names, or exact line numbers that require modification.\n\n### GitHub Problem Description ###\nIn v5.3, NDDataRef mask propagation fails when one of the operand does not have a mask\n### Description\n\nThis applies to v5.3. \r\n\r\nIt looks like when one of the operand does not have a mask, the mask propagation when doing arithmetic, in particular with `handle_mask=np.bitwise_or` fails.  This is not a problem in v5.2.\r\n\r\nI don't know enough about how all that works, but it seems from the error that the operand without a mask is set as a mask of None's and then the bitwise_or tries to operate on an integer and a None and fails.\n\n### Expected behavior\n\nWhen one of the operand does not have mask, the mask that exists should just be copied over to the output.  Or whatever was done in that situation in v5.2 where there's no problem.\n\n### How to Reproduce\n\nThis is with v5.3.   With v5.2, there are no errors.\r\n\r\n```\r\n>>> import numpy as np\r\n>>> from astropy.nddata import NDDataRef\r\n\r\n>>> array = np.array([[0, 1, 0], [1, 0, 1], [0, 1, 0]])\r\n>>> mask = np.array([[0, 1, 64], [8, 0, 1], [2, 1, 0]])\r\n\r\n>>> nref_nomask = NDDataRef(array)\r\n>>> nref_mask = NDDataRef(array, mask=mask)\r\n\r\n# multiply no mask by constant (no mask * no mask)\r\n>>> nref_nomask.multiply(1., handle_mask=np.bitwise_or).mask   # returns nothing, no mask,  OK\r\n\r\n# multiply no mask by itself (no mask * no mask)\r\n>>> nref_nomask.multiply(nref_nomask, handle_mask=np.bitwise_or).mask # return nothing, no mask, OK\r\n\r\n# multiply mask by constant (mask * no mask)\r\n>>> nref_mask.multiply(1., handle_mask=np.bitwise_or).mask\r\n...\r\nTypeError: unsupported operand type(s) for |: 'int' and 'NoneType'\r\n\r\n# multiply mask by itself (mask * mask)\r\n>>> nref_mask.multiply(nref_mask, handle_mask=np.bitwise_or).mask\r\narray([[ 0,  1, 64],\r\n       [ 8,  0,  1],\r\n       [ 2,  1,  0]])\r\n\r\n# multiply mask by no mask (mask * no mask)\r\n>>> nref_mask.multiply(nref_nomask, handle_mask=np.bitwise_or).mask\r\n...\r\nTypeError: unsupported operand type(s) for |: 'int' and 'NoneType'\r\n```\r\n\n\n### Versions\n\n>>> import sys; print(\"Python\", sys.version)\r\nPython 3.10.11 | packaged by conda-forge | (main, May 10 2023, 19:07:22) [Clang 14.0.6 ]\r\n>>> import astropy; print(\"astropy\", astropy.__version__)\r\nastropy 5.3\r\n>>> import numpy; print(\"Numpy\", numpy.__version__)\r\nNumpy 1.24.3\r\n>>> import erfa; print(\"pyerfa\", erfa.__version__)\r\npyerfa 2.0.0.3\r\n>>> import scipy; print(\"Scipy\", scipy.__version__)\r\nScipy 1.10.1\r\n>>> import matplotlib; print(\"Matplotlib\", matplotlib.__version__)\r\nMatplotlib 3.7.1\r\n\n\n\n###\n### astropy/nddata/nddata.py\n...\n14|from astropy.wcs.wcsapi import BaseHighLevelWCS, BaseLowLevelWCS, HighLevelWCSWrapper\n15|\n16|from .nddata_base import NDDataBase\n17|from .nduncertainty import NDUncertainty, UnknownUncertainty\n18|\n19|__all__ = [\"NDData\"]\n20|\n21|_meta_doc = \"\"\"`dict`-like : Additional meta information about the dataset.\"\"\"\n22|\n23|\n24|class NDData(NDDataBase):\n25|    \"\"\"\n26|    A container for `numpy.ndarray`-based datasets, using the\n27|    `~astropy.nddata.NDDataBase` interface.\n28|\n29|    The key distinction from raw `numpy.ndarray` is the presence of\n30|    additional metadata such as uncertainty, mask, unit, a coordinate system\n31|    and/or a dictionary containing further meta information. This class *only*\n32|    provides a container for *storing* such datasets. For further functionality\n33|    take a look at the ``See also`` section.\n34|\n35|    See also: https://docs.astropy.org/en/stable/nddata/\n36|\n37|    Parameters\n38|    ----------\n39|    data : `numpy.ndarray`-like or `NDData`-like\n40|        The dataset.\n41|\n42|    uncertainty : any type, optional\n43|        Uncertainty in the dataset.\n44|        Should have an attribute ``uncertainty_type`` that defines what kind of\n45|        uncertainty is stored, for example ``\"std\"`` for standard deviation or\n46|        ``\"var\"`` for variance. A metaclass defining such an interface is\n47|        `NDUncertainty` - but isn't mandatory. If the uncertainty has no such\n48|        attribute the uncertainty is stored as `UnknownUncertainty`.\n49|        Defaults to ``None``.\n50|\n51|    mask : any type, optional\n52|        Mask for the dataset. Masks should follow the ``numpy`` convention that\n53|        **valid** data points are marked by ``False`` and **invalid** ones with\n54|        ``True``.\n55|        Defaults to ``None``.\n56|\n57|    wcs : any type, optional\n58|        World coordinate system (WCS) for the dataset.\n59|        Default is ``None``.\n60|\n61|    meta : `dict`-like object, optional\n62|        Additional meta information about the dataset. If no meta is provided\n63|        an empty `collections.OrderedDict` is created.\n64|        Default is ``None``.\n65|\n66|    unit : unit-like, optional\n67|        Unit for the dataset. Strings that can be converted to a\n68|        `~astropy.units.Unit` are allowed.\n69|        Default is ``None``.\n70|\n71|    copy : `bool`, optional\n72|        Indicates whether to save the arguments as copy. ``True`` copies\n73|        every attribute before saving it while ``False`` tries to save every\n74|        parameter as reference.\n75|        Note however that it is not always possible to save the input as\n76|        reference.\n77|        Default is ``False``.\n78|\n79|        .. versionadded:: 1.2\n80|\n81|    psf : `numpy.ndarray` or None, optional\n82|        Image representation of the PSF. In order for convolution to be flux-\n83|        preserving, this should generally be normalized to sum to unity.\n84|\n85|    Raises\n86|    ------\n87|    TypeError\n88|        In case ``data`` or ``meta`` don't meet the restrictions.\n89|\n90|    Notes\n91|    -----\n92|    Each attribute can be accessed through the homonymous instance attribute:\n93|    ``data`` in a `NDData` object can be accessed through the `data`\n94|    attribute::\n95|\n96|        >>> from astropy.nddata import NDData\n97|        >>> nd = NDData([1,2,3])\n98|        >>> nd.data\n99|        array([1, 2, 3])\n100|\n101|    Given a conflicting implicit and an explicit parameter during\n102|    initialization, for example the ``data`` is a `~astropy.units.Quantity` and\n103|    the unit parameter is not ``None``, then the implicit parameter is replaced\n104|    (without conversion) by the explicit one and a warning is issued::\n105|\n106|        >>> import numpy as np\n107|        >>> import astropy.units as u\n108|        >>> q = np.array([1,2,3,4]) * u.m\n109|        >>> nd2 = NDData(q, unit=u.cm)\n110|        INFO: overwriting Quantity's current unit with specified unit. [astropy.nddata.nddata]\n111|        >>> nd2.data  # doctest: +FLOAT_CMP\n112|        array([100., 200., 300., 400.])\n113|        >>> nd2.unit\n114|        Unit(\"cm\")\n115|\n116|    See Also\n117|    --------\n118|    NDDataRef\n119|    NDDataArray\n120|    \"\"\"\n121|\n122|    # Instead of a custom property use the MetaData descriptor also used for\n123|    # Tables. It will check if the meta is dict-like or raise an exception.\n124|    meta = MetaData(doc=_meta_doc, copy=False)\n125|\n126|    def __init__(\n127|        self,\n128|        data,\n129|        uncertainty=None,\n130|        mask=None,\n131|        wcs=None,\n132|        meta=None,\n133|        unit=None,\n134|        copy=False,\n135|        psf=None,\n136|    ):\n137|        # Rather pointless since the NDDataBase does not implement any setting\n138|        # but before the NDDataBase did call the uncertainty\n139|        # setter. But if anyone wants to alter this behavior again the call\n140|        # to the superclass NDDataBase should be in here.\n141|        super().__init__()\n142|\n143|        # Check if data is any type from which to collect some implicitly\n144|        # passed parameters.\n145|        if isinstance(data, NDData):  # don't use self.__class__ (issue #4137)\n146|            # Of course we need to check the data because subclasses with other\n147|            # init-logic might be passed in here. We could skip these\n148|            # tests if we compared for self.__class__ but that has other\n149|            # drawbacks.\n150|\n151|            # Comparing if there is an explicit and an implicit unit parameter.\n152|            # If that is the case use the explicit one and issue a warning\n153|            # that there might be a conflict. In case there is no explicit\n154|            # unit just overwrite the unit parameter with the NDData.unit\n155|            # and proceed as if that one was given as parameter. Same for the\n156|            # other parameters.\n157|            if unit is None and data.unit is not None:\n158|                unit = data.unit\n159|            elif unit is not None and data.unit is not None:\n160|                log.info(\"overwriting NDData's current unit with specified unit.\")\n161|\n162|            if uncertainty is not None and data.uncertainty is not None:\n163|                log.info(\n164|                    \"overwriting NDData's current \"\n165|                    \"uncertainty with specified uncertainty.\"\n166|                )\n167|            elif data.uncertainty is not None:\n168|                uncertainty = data.uncertainty\n169|\n170|            if mask is not None and data.mask is not None:\n171|                log.info(\"overwriting NDData's current mask with specified mask.\")\n172|            elif data.mask is not None:\n173|                mask = data.mask\n174|\n175|            if wcs is not None and data.wcs is not None:\n176|                log.info(\"overwriting NDData's current wcs with specified wcs.\")\n177|            elif data.wcs is not None:\n178|                wcs = data.wcs\n179|\n180|            if psf is not None and data.psf is not None:\n181|                log.info(\"Overwriting NDData's current psf with specified psf.\")\n182|            elif data.psf is not None:\n183|                psf = data.psf\n184|\n185|            if meta is not None and data.meta is not None:\n186|                log.info(\"overwriting NDData's current meta with specified meta.\")\n187|            elif data.meta is not None:\n188|                meta = data.meta\n189|\n190|            # get the data attribute as it is, and continue to process it:\n191|            data = data.data\n192|\n193|        # if the data is wrapped by astropy.utils.masked.Masked:\n194|        if isinstance(data, Masked):\n195|            # first get the mask if one is available:\n196|            if hasattr(data, \"mask\"):\n197|                if mask is not None:\n198|                    log.info(\n199|                        \"overwriting Masked Quantity's current mask with specified mask.\"\n200|                    )\n201|                else:\n202|                    mask = data.mask\n203|\n204|            if isinstance(data, MaskedNDArray):\n205|                if unit is not None and hasattr(data, \"unit\") and data.unit != unit:\n206|                    log.info(\n207|                        \"overwriting MaskedNDArray's current unit with specified unit.\"\n208|                    )\n209|                    data = data.to(unit).value\n210|                elif unit is None and hasattr(data, \"unit\"):\n211|                    unit = data.unit\n212|                    data = data.value\n213|\n214|                # now get the unmasked ndarray:\n215|                data = np.asarray(data)\n216|\n217|            if isinstance(data, Quantity):\n218|                # this is a Quantity:\n219|                if unit is not None and data.unit != unit:\n220|                    log.info(\"overwriting Quantity's current unit with specified unit.\")\n221|                    data = data.to(unit)\n222|                elif unit is None and data.unit is not None:\n223|                    unit = data.unit\n224|                data = data.value\n225|\n226|        if isinstance(data, np.ma.masked_array):\n227|            if mask is not None:\n228|                log.info(\n229|                    \"overwriting masked ndarray's current mask with specified mask.\"\n230|                )\n231|            else:\n232|                mask = data.mask\n233|            data = data.data\n234|\n235|        if isinstance(data, Quantity):\n236|            # this is a Quantity:\n237|            if unit is not None and data.unit != unit:\n238|                log.info(\"overwriting Quantity's current unit with specified unit.\")\n239|                data = data.to(unit)\n240|            elif unit is None and data.unit is not None:\n241|                unit = data.unit\n242|            data = data.value\n243|\n244|        if isinstance(data, np.ndarray):\n245|            # check for mask from np.ma.masked_ndarray\n246|            if hasattr(data, \"mask\"):\n247|                if mask is not None:\n248|                    log.info(\n249|                        \"overwriting masked ndarray's current mask with specified mask.\"\n250|                    )\n251|                else:\n252|                    mask = data.mask\n253|\n254|        # Quick check on the parameters if they match the requirements.\n255|        if (\n256|            not hasattr(data, \"shape\")\n257|            or not hasattr(data, \"__getitem__\")\n258|            or not hasattr(data, \"__array__\")\n259|        ):\n260|            # Data doesn't look like a numpy array, try converting it to\n261|            # one.\n262|            data = np.array(data, subok=True, copy=False)\n263|        # Another quick check to see if what we got looks like an array\n264|        # rather than an object (since numpy will convert a\n265|        # non-numerical/non-string inputs to an array of objects).\n266|        if data.dtype == \"O\":\n267|            raise TypeError(\"could not convert data to numpy array.\")\n268|\n269|        if unit is not None:\n270|            unit = Unit(unit)\n271|\n272|        if copy:\n273|            # Data might have been copied before but no way of validating\n274|            # without another variable.\n275|            data = deepcopy(data)\n276|            mask = deepcopy(mask)\n277|            wcs = deepcopy(wcs)\n278|            psf = deepcopy(psf)\n279|            meta = deepcopy(meta)\n280|            uncertainty = deepcopy(uncertainty)\n281|            # Actually - copying the unit is unnecessary but better safe\n282|            # than sorry :-)\n283|            unit = deepcopy(unit)\n284|\n285|        # Store the attributes\n286|        self._data = data\n287|        self.mask = mask\n288|        self._wcs = None\n289|        if wcs is not None:\n290|            # Validate the wcs\n291|            self.wcs = wcs\n292|        self.meta = meta  # TODO: Make this call the setter sometime\n293|        self._unit = unit\n294|        # Call the setter for uncertainty to further check the uncertainty\n295|        self.uncertainty = uncertainty\n296|        self.psf = psf\n297|\n298|    def __str__(self):\n299|        data = str(self.data)\n300|        unit = f\" {self.unit}\" if self.unit is not None else \"\"\n301|\n302|        return data + unit\n303|\n304|    def __repr__(self):\n305|        prefix = self.__class__.__name__ + \"(\"\n306|        data = np.array2string(self.data, separator=\", \", prefix=prefix)\n307|        unit = f\", unit='{self.unit}'\" if self.unit is not None else \"\"\n308|\n309|        return f\"{prefix}{data}{unit})\"\n310|\n311|    @property\n312|    def data(self):\n313|        \"\"\"\n314|        `~numpy.ndarray`-like : The stored dataset.\n315|        \"\"\"\n316|        return self._data\n317|\n318|    @property\n319|    def mask(self):\n320|        \"\"\"\n321|        any type : Mask for the dataset, if any.\n322|\n323|        Masks should follow the ``numpy`` convention that valid data points are\n324|        marked by ``False`` and invalid ones with ``True``.\n325|        \"\"\"\n326|        return self._mask\n327|\n328|    @mask.setter\n329|    def mask(self, value):\n330|        self._mask = value\n331|\n332|    @property\n333|    def unit(self):\n334|        \"\"\"\n335|        `~astropy.units.Unit` : Unit for the dataset, if any.\n336|        \"\"\"\n337|        return self._unit\n338|\n339|    @property\n340|    def wcs(self):\n341|        \"\"\"\n342|        any type : A world coordinate system (WCS) for the dataset, if any.\n343|        \"\"\"\n344|        return self._wcs\n345|\n346|    @wcs.setter\n347|    def wcs(self, wcs):\n348|        if self._wcs is not None and wcs is not None:\n349|            raise ValueError(\n350|                \"You can only set the wcs attribute with a WCS if no WCS is present.\"\n351|            )\n352|\n353|        if wcs is None or isinstance(wcs, BaseHighLevelWCS):\n354|            self._wcs = wcs\n355|        elif isinstance(wcs, BaseLowLevelWCS):\n356|            self._wcs = HighLevelWCSWrapper(wcs)\n357|        else:\n358|            raise TypeError(\n359|                \"The wcs argument must implement either the high or low level WCS API.\"\n360|            )\n361|\n362|    @property\n363|    def psf(self):\n364|        return self._psf\n365|\n366|    @psf.setter\n367|    def psf(self, value):\n368|        self._psf = value\n369|\n370|    @property\n371|    def uncertainty(self):\n372|        \"\"\"\n373|        any type : Uncertainty in the dataset, if any.\n374|\n375|        Should have an attribute ``uncertainty_type`` that defines what kind of\n376|        uncertainty is stored, such as ``'std'`` for standard deviation or\n377|        ``'var'`` for variance. A metaclass defining such an interface is\n378|        `~astropy.nddata.NDUncertainty` but isn't mandatory.\n379|        \"\"\"\n380|        return self._uncertainty\n381|\n382|    @uncertainty.setter\n383|    def uncertainty(self, value):\n384|        if value is not None:\n385|            # There is one requirements on the uncertainty: That\n386|            # it has an attribute 'uncertainty_type'.\n387|            # If it does not match this requirement convert it to an unknown\n388|            # uncertainty.\n389|            if not hasattr(value, \"uncertainty_type\"):\n390|                log.info(\"uncertainty should have attribute uncertainty_type.\")\n391|                value = UnknownUncertainty(value, copy=False)\n392|\n393|            # If it is a subclass of NDUncertainty we must set the\n394|            # parent_nddata attribute. (#4152)\n395|            if isinstance(value, NDUncertainty):\n396|                # In case the uncertainty already has a parent create a new\n397|                # instance because we need to assume that we don't want to\n398|                # steal the uncertainty from another NDData object\n399|                if value._parent_nddata is not None:\n400|                    value = value.__class__(value, copy=False)\n401|                # Then link it to this NDData instance (internally this needs\n402|                # to be saved as weakref but that's done by NDUncertainty\n403|                # setter).\n404|                value.parent_nddata = self\n405|        self._uncertainty = value\n\n\n### astropy/nddata/mixins/ndarithmetic.py\n...\n93|    callable must accept the corresponding attributes as first two\n94|    parameters. If the callable also needs additional parameters these can be\n95|    defined as ``kwargs`` and must start with ``\"wcs_\"`` (for wcs callable) or\n96|    ``\"meta_\"`` (for meta callable). This startstring is removed before the\n97|    callable is called.\n98|\n99|    ``\"first_found\"`` can also be abbreviated with ``\"ff\"``.\n100|    \"\"\"\n101|\n102|\n103|class NDArithmeticMixin:\n104|    \"\"\"\n105|    Mixin class to add arithmetic to an NDData object.\n106|\n107|    When subclassing, be sure to list the superclasses in the correct order\n108|    so that the subclass sees NDData as the main superclass. See\n109|    `~astropy.nddata.NDDataArray` for an example.\n110|\n111|    Notes\n112|    -----\n113|    This class only aims at covering the most common cases so there are certain\n114|    restrictions on the saved attributes::\n115|\n116|        - ``uncertainty`` : has to be something that has a `NDUncertainty`-like\n117|          interface for uncertainty propagation\n118|        - ``mask`` : has to be something that can be used by a bitwise ``or``\n119|          operation.\n120|        - ``wcs`` : has to implement a way of comparing with ``=`` to allow\n121|          the operation.\n122|\n123|    But there is a workaround that allows to disable handling a specific\n124|    attribute and to simply set the results attribute to ``None`` or to\n125|    copy the existing attribute (and neglecting the other).\n126|    For example for uncertainties not representing an `NDUncertainty`-like\n127|    interface you can alter the ``propagate_uncertainties`` parameter in\n128|    :meth:`NDArithmeticMixin.add`. ``None`` means that the result will have no\n129|    uncertainty, ``False`` means it takes the uncertainty of the first operand\n130|    (if this does not exist from the second operand) as the result's\n131|    uncertainty. This behavior is also explained in the docstring for the\n132|    different arithmetic operations.\n133|\n134|    Decomposing the units is not attempted, mainly due to the internal mechanics\n135|    of `~astropy.units.Quantity`, so the resulting data might have units like\n136|    ``km/m`` if you divided for example 100km by 5m. So this Mixin has adopted\n137|    this behavior.\n138|\n139|    Examples\n140|    --------\n141|    Using this Mixin with `~astropy.nddata.NDData`:\n142|\n143|        >>> from astropy.nddata import NDData, NDArithmeticMixin\n144|        >>> class NDDataWithMath(NDArithmeticMixin, NDData):\n145|        ...     pass\n146|\n147|    Using it with one operand on an instance::\n148|\n149|        >>> ndd = NDDataWithMath(100)\n150|        >>> ndd.add(20)\n151|        NDDataWithMath(120)\n152|\n153|    Using it with two operand on an instance::\n154|\n155|        >>> ndd = NDDataWithMath(-4)\n156|        >>> ndd.divide(1, ndd)\n157|        NDDataWithMath(-0.25)\n158|\n159|    Using it as classmethod requires two operands::\n160|\n161|        >>> NDDataWithMath.subtract(5, 4)\n162|        NDDataWithMath(1)\n163|\n164|    \"\"\"\n165|\n166|    def _arithmetic(\n167|        self,\n168|        operation,\n169|        operand,\n170|        propagate_uncertainties=True,\n171|        handle_mask=np.logical_or,\n172|        handle_meta=None,\n173|        uncertainty_correlation=0,\n174|        compare_wcs=\"first_found\",\n175|        operation_ignores_mask=False,\n176|        axis=None,\n177|        **kwds,\n178|    ):\n179|        \"\"\"\n180|        Base method which calculates the result of the arithmetic operation.\n181|\n182|        This method determines the result of the arithmetic operation on the\n183|        ``data`` including their units and then forwards to other methods\n184|        to calculate the other properties for the result (like uncertainty).\n185|\n186|        Parameters\n187|        ----------\n188|        operation : callable\n189|            The operation that is performed on the `NDData`. Supported are\n190|            `numpy.add`, `numpy.subtract`, `numpy.multiply` and\n191|            `numpy.true_divide`.\n192|\n193|        operand : same type (class) as self\n194|            see :meth:`NDArithmeticMixin.add`\n195|\n196|        propagate_uncertainties : `bool` or ``None``, optional\n197|            see :meth:`NDArithmeticMixin.add`\n198|\n199|        handle_mask : callable, ``'first_found'`` or ``None``, optional\n200|            see :meth:`NDArithmeticMixin.add`\n201|\n202|        handle_meta : callable, ``'first_found'`` or ``None``, optional\n203|            see :meth:`NDArithmeticMixin.add`\n204|\n205|        compare_wcs : callable, ``'first_found'`` or ``None``, optional\n206|            see :meth:`NDArithmeticMixin.add`\n207|\n208|        uncertainty_correlation : ``Number`` or `~numpy.ndarray`, optional\n209|            see :meth:`NDArithmeticMixin.add`\n210|\n211|        operation_ignores_mask : bool, optional\n212|            When True, masked values will be excluded from operations;\n213|            otherwise the operation will be performed on all values,\n214|            including masked ones.\n215|\n216|        axis : int or tuple of ints, optional\n217|            axis or axes over which to perform collapse operations like min, max, sum or mean.\n218|\n219|        kwargs :\n220|            Any other parameter that should be passed to the\n221|            different :meth:`NDArithmeticMixin._arithmetic_mask` (or wcs, ...)\n222|            methods.\n223|\n224|        Returns\n225|        -------\n226|        result : ndarray or `~astropy.units.Quantity`\n227|            The resulting data as array (in case both operands were without\n228|            unit) or as quantity if at least one had a unit.\n229|\n230|        kwargs : `dict`\n231|            The kwargs should contain all the other attributes (besides data\n232|            and unit) needed to create a new instance for the result. Creating\n233|            the new instance is up to the calling method, for example\n234|            :meth:`NDArithmeticMixin.add`.\n235|\n236|        \"\"\"\n237|        # Find the appropriate keywords for the appropriate method (not sure\n238|        # if data and uncertainty are ever used ...)\n239|        kwds2 = {\"mask\": {}, \"meta\": {}, \"wcs\": {}, \"data\": {}, \"uncertainty\": {}}\n240|        for i in kwds:\n241|            splitted = i.split(\"_\", 1)\n242|            try:\n243|                kwds2[splitted[0]][splitted[1]] = kwds[i]\n244|            except KeyError:\n245|                raise KeyError(f\"Unknown prefix {splitted[0]} for parameter {i}\")\n246|\n247|        kwargs = {}\n248|\n249|        # First check that the WCS allows the arithmetic operation\n250|        if compare_wcs is None:\n251|            kwargs[\"wcs\"] = None\n252|        elif compare_wcs in [\"ff\", \"first_found\"]:\n253|            if self.wcs is None and hasattr(operand, \"wcs\"):\n254|                kwargs[\"wcs\"] = deepcopy(operand.wcs)\n255|            else:\n256|                kwargs[\"wcs\"] = deepcopy(self.wcs)\n257|        else:\n258|            kwargs[\"wcs\"] = self._arithmetic_wcs(\n259|                operation, operand, compare_wcs, **kwds2[\"wcs\"]\n260|            )\n261|\n262|        # collapse operations on masked quantities/arrays which are supported by\n263|        # the astropy.utils.masked or np.ma modules should use those modules to\n264|        # do the arithmetic on the data and propagate masks.\n265|        use_masked_arith = operand is None and self.mask is not None\n266|        if use_masked_arith:\n267|            # if we're *including* masked values in the operation,\n268|            # use the astropy Masked module:\n269|            if not operation_ignores_mask:\n270|                # call the numpy operation on a Masked NDDataArray\n271|                # representation of the nddata, with units when available:\n272|                if self.unit is not None and not hasattr(self.data, \"unit\"):\n273|                    masked_input = Masked(self.data << self.unit, mask=self.mask)\n274|                else:\n275|                    masked_input = Masked(self.data, mask=self.mask)\n276|            # if we're *excluding* masked values in the operation,\n277|            # we use the numpy.ma module:\n278|            else:\n279|                masked_input = np.ma.masked_array(self.data, self.mask)\n280|            result = operation(masked_input, axis=axis)\n281|            # since result may be e.g. a float if operation is a sum over all axes,\n282|            # let's ensure that result is a masked array, since we'll assume this later:\n283|            if not hasattr(result, \"mask\"):\n284|                result = np.ma.masked_array(\n285|                    result, mask=np.zeros_like(result, dtype=bool)\n286|                )\n287|        else:\n288|            # Then calculate the resulting data (which can but needs not be a\n289|            # quantity)\n290|            result = self._arithmetic_data(\n291|                operation, operand, axis=axis, **kwds2[\"data\"]\n292|            )\n293|\n294|        # preserve original units\n295|        if not hasattr(result, \"unit\") and hasattr(self, \"unit\"):\n296|            kwargs[\"unit\"] = self.unit\n297|\n298|        # Determine the other properties\n299|        if propagate_uncertainties is None:\n300|            kwargs[\"uncertainty\"] = None\n301|        elif not propagate_uncertainties:\n302|            if self.uncertainty is None:\n303|                kwargs[\"uncertainty\"] = deepcopy(operand.uncertainty)\n304|            else:\n305|                kwargs[\"uncertainty\"] = deepcopy(self.uncertainty)\n306|        else:\n307|            kwargs[\"uncertainty\"] = self._arithmetic_uncertainty(\n308|                operation,\n309|                operand,\n310|                result,\n311|                uncertainty_correlation,\n312|                axis=axis,\n313|                **kwds2[\"uncertainty\"],\n314|            )\n315|\n316|        # If both are None, there is nothing to do.\n317|        if self.psf is not None or (operand is not None and operand.psf is not None):\n318|            warnings.warn(\n319|                f\"Not setting psf attribute during {operation.__name__}.\",\n320|                AstropyUserWarning,\n321|            )\n322|\n323|        if handle_mask is None:\n324|            pass\n325|        elif hasattr(result, \"mask\"):\n326|            # if numpy.ma or astropy.utils.masked is being used, the constructor\n327|            # will pick up the mask from the masked object:\n328|            kwargs[\"mask\"] = None\n329|        elif handle_mask in [\"ff\", \"first_found\"]:\n330|            if self.mask is None:\n331|                kwargs[\"mask\"] = deepcopy(operand.mask)\n332|            else:\n333|                kwargs[\"mask\"] = deepcopy(self.mask)\n334|        else:\n335|            kwargs[\"mask\"] = self._arithmetic_mask(\n336|                operation, operand, handle_mask, axis=axis, **kwds2[\"mask\"]\n337|            )\n338|\n339|        if handle_meta is None:\n340|            kwargs[\"meta\"] = None\n341|        elif handle_meta in [\"ff\", \"first_found\"]:\n342|            if not self.meta:\n343|                kwargs[\"meta\"] = deepcopy(operand.meta)\n344|            else:\n345|                kwargs[\"meta\"] = deepcopy(self.meta)\n346|        else:\n347|            kwargs[\"meta\"] = self._arithmetic_meta(\n348|                operation, operand, handle_meta, **kwds2[\"meta\"]\n349|            )\n350|\n351|        # Wrap the individual results into a new instance of the same class.\n352|        return result, kwargs\n353|\n354|    def _arithmetic_data(self, operation, operand, **kwds):\n355|        \"\"\"\n356|        Calculate the resulting data.\n357|\n358|        Parameters\n359|        ----------\n360|        operation : callable\n361|            see `NDArithmeticMixin._arithmetic` parameter description.\n362|\n363|        operand : `NDData`-like instance\n364|            The second operand wrapped in an instance of the same class as\n365|            self.\n366|\n367|        kwds :\n368|            Additional parameters.\n369|\n370|        Returns\n371|        -------\n372|        result_data : ndarray or `~astropy.units.Quantity`\n373|            If both operands had no unit the resulting data is a simple numpy\n374|            array, but if any of the operands had a unit the return is a\n375|            Quantity.\n376|        \"\"\"\n377|        # Do the calculation with or without units\n378|        if self.unit is None:\n379|            if operand.unit is None:\n380|                result = operation(self.data, operand.data)\n381|            else:\n382|                result = operation(\n383|                    self.data << dimensionless_unscaled, operand.data << operand.unit\n384|                )\n385|        elif hasattr(operand, \"unit\"):\n386|            if operand.unit is not None:\n387|                result = operation(self.data << self.unit, operand.data << operand.unit)\n388|            else:\n389|                result = operation(\n390|                    self.data << self.unit, operand.data << dimensionless_unscaled\n391|                )\n392|        elif operand is not None:\n393|            result = operation(self.data << self.unit, operand.data << operand.unit)\n394|        else:\n395|            result = operation(self.data, axis=kwds[\"axis\"])\n396|\n397|        return result\n398|\n399|    def _arithmetic_uncertainty(self, operation, operand, result, correlation, **kwds):\n400|        \"\"\"\n401|        Calculate the resulting uncertainty.\n402|\n403|        Parameters\n404|        ----------\n405|        operation : callable\n406|            see :meth:`NDArithmeticMixin._arithmetic` parameter description.\n407|\n408|        operand : `NDData`-like instance\n409|            The second operand wrapped in an instance of the same class as\n410|            self.\n411|\n412|        result : `~astropy.units.Quantity` or `~numpy.ndarray`\n413|            The result of :meth:`NDArithmeticMixin._arithmetic_data`.\n414|\n415|        correlation : number or `~numpy.ndarray`\n416|            see :meth:`NDArithmeticMixin.add` parameter description.\n417|\n418|        kwds :\n419|            Additional parameters.\n420|\n421|        Returns\n422|        -------\n423|        result_uncertainty : `NDUncertainty` subclass instance or None\n424|            The resulting uncertainty already saved in the same `NDUncertainty`\n425|            subclass that ``self`` had (or ``operand`` if self had no\n426|            uncertainty). ``None`` only if both had no uncertainty.\n427|        \"\"\"\n428|        # Make sure these uncertainties are NDUncertainties so this kind of\n429|        # propagation is possible.\n430|        if self.uncertainty is not None and not isinstance(\n431|            self.uncertainty, NDUncertainty\n432|        ):\n433|            raise TypeError(\n434|                \"Uncertainty propagation is only defined for \"\n435|                \"subclasses of NDUncertainty.\"\n436|            )\n437|        if (\n438|            operand is not None\n439|            and operand.uncertainty is not None\n440|            and not isinstance(operand.uncertainty, NDUncertainty)\n441|        ):\n442|            raise TypeError(\n443|                \"Uncertainty propagation is only defined for \"\n444|                \"subclasses of NDUncertainty.\"\n445|            )\n446|\n447|        # Now do the uncertainty propagation\n448|        # TODO: There is no enforced requirement that actually forbids the\n449|        # uncertainty to have negative entries but with correlation the\n450|        # sign of the uncertainty DOES matter.\n451|        if self.uncertainty is None and (\n452|            not hasattr(operand, \"uncertainty\") or operand.uncertainty is None\n453|        ):\n454|            # Neither has uncertainties so the result should have none.\n455|            return None\n456|        elif self.uncertainty is None:\n457|            # Create a temporary uncertainty to allow uncertainty propagation\n458|            # to yield the correct results. (issue #4152)\n459|            self.uncertainty = operand.uncertainty.__class__(None)\n460|            result_uncert = self.uncertainty.propagate(\n461|                operation, operand, result, correlation\n462|            )\n463|            # Delete the temporary uncertainty again.\n464|            self.uncertainty = None\n465|            return result_uncert\n466|\n467|        elif operand is not None and operand.uncertainty is None:\n468|            # As with self.uncertainty is None but the other way around.\n469|            operand.uncertainty = self.uncertainty.__class__(None)\n470|            result_uncert = self.uncertainty.propagate(\n471|                operation, operand, result, correlation\n472|            )\n473|            operand.uncertainty = None\n474|            return result_uncert\n475|\n476|        else:\n477|            # Both have uncertainties so just propagate.\n478|\n479|            # only supply the axis kwarg if one has been specified for a collapsing operation\n480|            axis_kwarg = dict(axis=kwds[\"axis\"]) if \"axis\" in kwds else dict()\n481|            return self.uncertainty.propagate(\n482|                operation, operand, result, correlation, **axis_kwarg\n483|            )\n484|\n485|    def _arithmetic_mask(self, operation, operand, handle_mask, axis=None, **kwds):\n486|        \"\"\"\n487|        Calculate the resulting mask.\n488|\n489|        This is implemented as the piecewise ``or`` operation if both have a\n490|        mask.\n491|\n492|        Parameters\n493|        ----------\n494|        operation : callable\n495|            see :meth:`NDArithmeticMixin._arithmetic` parameter description.\n496|            By default, the ``operation`` will be ignored.\n497|\n498|        operand : `NDData`-like instance\n499|            The second operand wrapped in an instance of the same class as\n500|            self.\n501|\n502|        handle_mask : callable\n503|            see :meth:`NDArithmeticMixin.add`\n504|\n505|        kwds :\n506|            Additional parameters given to ``handle_mask``.\n507|\n508|        Returns\n509|        -------\n510|        result_mask : any type\n511|            If only one mask was present this mask is returned.\n512|            If neither had a mask ``None`` is returned. Otherwise\n513|            ``handle_mask`` must create (and copy) the returned mask.\n514|        \"\"\"\n515|        # If only one mask is present we need not bother about any type checks\n516|        if (\n517|            self.mask is None and operand is not None and operand.mask is None\n518|        ) or handle_mask is None:\n519|            return None\n520|        elif self.mask is None and operand is not None:\n521|            # Make a copy so there is no reference in the result.\n522|            return deepcopy(operand.mask)\n523|        elif operand is None:\n524|            return deepcopy(self.mask)\n525|        else:\n526|            # Now lets calculate the resulting mask (operation enforces copy)\n527|            return handle_mask(self.mask, operand.mask, **kwds)\n528|\n529|    def _arithmetic_wcs(self, operation, operand, compare_wcs, **kwds):\n530|        \"\"\"\n531|        Calculate the resulting wcs.\n532|\n533|        There is actually no calculation involved but it is a good place to\n534|        compare wcs information of both operands. This is currently not working\n535|        properly with `~astropy.wcs.WCS` (which is the suggested class for\n536|        storing as wcs property) but it will not break it neither.\n537|\n538|        Parameters\n539|        ----------\n540|        operation : callable\n541|            see :meth:`NDArithmeticMixin._arithmetic` parameter description.\n542|            By default, the ``operation`` will be ignored.\n543|\n544|        operand : `NDData` instance or subclass\n545|            The second operand wrapped in an instance of the same class as\n546|            self.\n547|\n548|        compare_wcs : callable\n549|            see :meth:`NDArithmeticMixin.add` parameter description.\n550|\n551|        kwds :\n552|            Additional parameters given to ``compare_wcs``.\n553|\n554|        Raises\n555|        ------\n556|        ValueError\n557|            If ``compare_wcs`` returns ``False``.\n558|\n559|        Returns\n560|        -------\n561|        result_wcs : any type\n562|            The ``wcs`` of the first operand is returned.\n563|        \"\"\"\n564|        # ok, not really arithmetic but we need to check which wcs makes sense\n565|        # for the result and this is an ideal place to compare the two WCS,\n566|        # too.\n567|\n568|        # I'll assume that the comparison returned None or False in case they\n569|        # are not equal.\n570|        if not compare_wcs(self.wcs, operand.wcs, **kwds):\n571|            raise ValueError(\"WCS are not equal.\")\n572|\n573|        return deepcopy(self.wcs)\n574|\n575|    def _arithmetic_meta(self, operation, operand, handle_meta, **kwds):\n576|        \"\"\"\n577|        Calculate the resulting meta.\n578|\n579|        Parameters\n580|        ----------\n581|        operation : callable\n582|            see :meth:`NDArithmeticMixin._arithmetic` parameter description.\n583|            By default, the ``operation`` will be ignored.\n584|\n585|        operand : `NDData`-like instance\n586|            The second operand wrapped in an instance of the same class as\n587|            self.\n588|\n589|        handle_meta : callable\n590|            see :meth:`NDArithmeticMixin.add`\n591|\n592|        kwds :\n593|            Additional parameters given to ``handle_meta``.\n594|\n595|        Returns\n596|        -------\n597|        result_meta : any type\n598|            The result of ``handle_meta``.\n599|        \"\"\"\n600|        # Just return what handle_meta does with both of the metas.\n601|        return handle_meta(self.meta, operand.meta, **kwds)\n602|\n603|    @sharedmethod\n604|    @format_doc(_arit_doc, name=\"addition\", op=\"+\")\n605|    def add(self, operand, operand2=None, **kwargs):\n606|        return self._prepare_then_do_arithmetic(np.add, operand, operand2, **kwargs)\n607|\n608|    @sharedmethod\n609|    @format_doc(_arit_doc, name=\"subtraction\", op=\"-\")\n610|    def subtract(self, operand, operand2=None, **kwargs):\n611|        return self._prepare_then_do_arithmetic(\n612|            np.subtract, operand, operand2, **kwargs\n613|        )\n614|\n615|    @sharedmethod\n616|    @format_doc(_arit_doc, name=\"multiplication\", op=\"*\")\n617|    def multiply(self, operand, operand2=None, **kwargs):\n618|        return self._prepare_then_do_arithmetic(\n619|            np.multiply, operand, operand2, **kwargs\n620|        )\n621|\n622|    @sharedmethod\n623|    @format_doc(_arit_doc, name=\"division\", op=\"/\")\n624|    def divide(self, operand, operand2=None, **kwargs):\n625|        return self._prepare_then_do_arithmetic(\n626|            np.true_divide, operand, operand2, **kwargs\n627|        )\n628|\n629|    @sharedmethod\n630|    def sum(self, **kwargs):\n631|        return self._prepare_then_do_arithmetic(np.sum, **kwargs)\n632|\n633|    @sharedmethod\n634|    def mean(self, **kwargs):\n635|        return self._prepare_then_do_arithmetic(np.mean, **kwargs)\n636|\n637|    @sharedmethod\n638|    def min(self, **kwargs):\n639|        # use the provided propagate_uncertainties if available, otherwise default is False:\n640|        propagate_uncertainties = kwargs.pop(\"propagate_uncertainties\", None)\n641|        return self._prepare_then_do_arithmetic(\n642|            np.min, propagate_uncertainties=propagate_uncertainties, **kwargs\n643|        )\n644|\n645|    @sharedmethod\n646|    def max(self, **kwargs):\n647|        # use the provided propagate_uncertainties if available, otherwise default is False:\n648|        propagate_uncertainties = kwargs.pop(\"propagate_uncertainties\", None)\n649|        return self._prepare_then_do_arithmetic(\n650|            np.max, propagate_uncertainties=propagate_uncertainties, **kwargs\n651|        )\n652|\n653|    @sharedmethod\n654|    def _prepare_then_do_arithmetic(\n655|        self_or_cls, operation, operand=None, operand2=None, **kwargs\n656|    ):\n657|        \"\"\"Intermediate method called by public arithmetic (i.e. ``add``)\n658|        before the processing method (``_arithmetic``) is invoked.\n659|\n660|        .. warning::\n661|            Do not override this method in subclasses.\n662|\n663|        This method checks if it was called as instance or as class method and\n664|        then wraps the operands and the result from ``_arithmetic`` in the\n665|        appropriate subclass.\n666|\n667|        Parameters\n668|        ----------\n669|        self_or_cls : instance or class\n670|            ``sharedmethod`` behaves like a normal method if called on the\n671|            instance (then this parameter is ``self``) but like a classmethod\n672|            when called on the class (then this parameter is ``cls``).\n673|\n674|        operations : callable\n675|            The operation (normally a numpy-ufunc) that represents the\n676|            appropriate action.\n677|\n678|        operand, operand2, kwargs :\n679|            See for example ``add``.\n680|\n681|        Result\n682|        ------\n683|        result : `~astropy.nddata.NDData`-like\n684|            Depending how this method was called either ``self_or_cls``\n685|            (called on class) or ``self_or_cls.__class__`` (called on instance)\n686|            is the NDData-subclass that is used as wrapper for the result.\n687|        \"\"\"\n688|        # DO NOT OVERRIDE THIS METHOD IN SUBCLASSES.\n689|\n690|        if isinstance(self_or_cls, NDArithmeticMixin):\n691|            # True means it was called on the instance, so self_or_cls is\n692|            # a reference to self\n693|            cls = self_or_cls.__class__\n694|            if operand2 is None:\n695|                # Only one operand was given. Set operand2 to operand and\n696|                # operand to self so that we call the appropriate method of the\n697|                # operand.\n698|                operand2 = operand\n699|                operand = self_or_cls\n700|            else:\n701|                # Convert the first operand to the class of this method.\n702|                # This is important so that always the correct _arithmetics is\n703|                # called later that method.\n704|                operand = cls(operand)\n705|\n706|        else:\n707|            # It was used as classmethod so self_or_cls represents the cls\n708|            cls = self_or_cls\n709|\n710|            # It was called on the class so we expect two operands!\n711|            if operand2 is None:\n712|                raise TypeError(\n713|                    \"operand2 must be given when the method isn't \"\n714|                    \"called on an instance.\"\n715|                )\n716|\n717|            # Convert to this class. See above comment why.\n718|            operand = cls(operand)\n719|\n720|        # At this point operand, operand2, kwargs and cls are determined.\n721|        if operand2 is not None and not issubclass(\n722|            operand2.__class__, NDArithmeticMixin\n723|        ):\n724|            # Let's try to convert operand2 to the class of operand to allow for\n725|            # arithmetic operations with numbers, lists, numpy arrays, numpy masked\n726|            # arrays, astropy quantities, masked quantities and of other subclasses\n727|            # of NDData.\n728|            operand2 = cls(operand2)\n729|\n730|            # Now call the _arithmetics method to do the arithmetic.\n731|            result, init_kwds = operand._arithmetic(operation, operand2, **kwargs)\n732|        elif issubclass(operand2.__class__, NDArithmeticMixin):\n733|            # calling as class method:\n734|            result, init_kwds = cls._arithmetic(\n735|                operand,\n736|                operation,\n737|                operand2,\n738|                **kwargs,\n739|            )\n740|        else:\n741|            # otherwise call the _arithmetic method on self for a collapse operation:\n742|            # for collapse operations, use astropy.utils.masked rather than handle_mask\n743|            result, init_kwds = self_or_cls._arithmetic(\n744|                operation,\n745|                operand2,\n746|                **kwargs,\n747|            )\n748|\n749|        # Return a new class based on the result\n750|        return cls(result, **init_kwds)\n\n\n\n\n###\n\nPlease provide the class name, function or method name, or the exact line numbers that need to be edited.\nThe possible location outputs should be either \"class\", \"function\" or \"line\".\n\n### Examples:\n```\nfull_path1/file1.py\nline: 10\nclass: MyClass1\nline: 51\n\nfull_path2/file2.py\nfunction: MyClass2.my_method\nline: 12\n\nfull_path3/file3.py\nfunction: my_function\nline: 24\nline: 156\n```\n\nReturn just the location(s) wrapped with ```.\n", "response": ["```\nastropy/nddata/mixins/ndarithmetic.py\nfunction: NDArithmeticMixin._arithmetic\nline: 323\nline: 335\n\nastropy/nddata/nddata.py\nfunction: NDData.__init__\nline: 329\n```", "```\nastropy/nddata/mixins/ndarithmetic.py\nfunction: NDArithmeticMixin._arithmetic_mask\nline: 485\nline: 522\n```", "```\nastropy/nddata/mixins/ndarithmetic.py\nline: 322\nline: 485\n```", "```\nastropy/nddata/mixins/ndarithmetic.py\nline: 522\nfunction: NDArithmeticMixin._arithmetic_mask\nline: 526\n```"], "usage": {"completion_tokens": 159, "prompt_tokens": 11583}}}
{"instance_id": "astropy__astropy-14182", "found_files": ["astropy/io/ascii/rst.py", "astropy/io/ascii/core.py", "astropy/io/ascii/ui.py"], "additional_artifact_loc_file": {}, "file_traj": {}, "found_related_locs": {"astropy/io/ascii/rst.py": ["class: RST\nclass: SimpleRSTHeader\nclass: SimpleRSTData"], "astropy/io/ascii/core.py": ["function: _get_writer\nclass: BaseHeader\nclass: BaseData"], "astropy/io/ascii/ui.py": ["function: write\nfunction: get_writer"]}, "additional_artifact_loc_related": [{"raw_output_loc": "```\nastropy/io/ascii/rst.py\nclass: RST\nclass: SimpleRSTHeader\nclass: SimpleRSTData\n\nastropy/io/ascii/core.py\nfunction: _get_writer\nclass: BaseHeader\nclass: BaseData\n\nastropy/io/ascii/ui.py\nfunction: write\nfunction: get_writer\n```"}], "related_loc_traj": [{"response": "```\nastropy/io/ascii/rst.py\nclass: RST\nclass: SimpleRSTHeader\nclass: SimpleRSTData\n\nastropy/io/ascii/core.py\nfunction: _get_writer\nclass: BaseHeader\nclass: BaseData\n\nastropy/io/ascii/ui.py\nfunction: write\nfunction: get_writer\n```", "usage": {"completion_tokens": 71, "prompt_tokens": 2889}, "prompt": "\nPlease look through the following GitHub Problem Description and the Skeleton of Relevant Files.\nIdentify all locations that need inspection or editing to fix the problem, including directly related areas as well as any potentially related global variables, functions, and classes.\nFor each location you provide, either give the name of the class, the name of a method in a class, the name of a function, or the name of a global variable.\n\n### GitHub Problem Description ###\nPlease support header rows in RestructuredText output\n### Description\r\n\r\nIt would be great if the following would work:\r\n\r\n```Python\r\n>>> from astropy.table import QTable\r\n>>> import astropy.units as u\r\n>>> import sys\r\n>>> tbl = QTable({'wave': [350,950]*u.nm, 'response': [0.7, 1.2]*u.count})\r\n>>> tbl.write(sys.stdout,  format=\"ascii.rst\")\r\n===== ========\r\n wave response\r\n===== ========\r\n350.0      0.7\r\n950.0      1.2\r\n===== ========\r\n>>> tbl.write(sys.stdout,  format=\"ascii.fixed_width\", header_rows=[\"name\", \"unit\"])\r\n|  wave | response |\r\n|    nm |       ct |\r\n| 350.0 |      0.7 |\r\n| 950.0 |      1.2 |\r\n>>> tbl.write(sys.stdout,  format=\"ascii.rst\", header_rows=[\"name\", \"unit\"])\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/usr/lib/python3/dist-packages/astropy/table/connect.py\", line 129, in __call__\r\n    self.registry.write(instance, *args, **kwargs)\r\n  File \"/usr/lib/python3/dist-packages/astropy/io/registry/core.py\", line 369, in write\r\n    return writer(data, *args, **kwargs)\r\n  File \"/usr/lib/python3/dist-packages/astropy/io/ascii/connect.py\", line 26, in io_write\r\n    return write(table, filename, **kwargs)\r\n  File \"/usr/lib/python3/dist-packages/astropy/io/ascii/ui.py\", line 856, in write\r\n    writer = get_writer(Writer=Writer, fast_writer=fast_writer, **kwargs)\r\n  File \"/usr/lib/python3/dist-packages/astropy/io/ascii/ui.py\", line 800, in get_writer\r\n    writer = core._get_writer(Writer, fast_writer, **kwargs)\r\n  File \"/usr/lib/python3/dist-packages/astropy/io/ascii/core.py\", line 1719, in _get_writer\r\n    writer = Writer(**writer_kwargs)\r\nTypeError: RST.__init__() got an unexpected keyword argument 'header_rows'\r\n```\r\n\r\n\r\n### Additional context\r\n\r\nRestructuredText output is a great way to fill autogenerated documentation with content, so having this flexible makes the life easier `:-)`\r\n\r\n\n\n\n### Skeleton of Relevant Files ###\n\n### File: astropy/io/ascii/rst.py ###\n```python\n# Licensed under a 3-clause BSD style license\n\n\nclass SimpleRSTHeader(FixedWidthHeader):\n    position_line = 0\n    start_line = 1\n    splitter_class = DefaultSplitter\n    position_char = \"=\"\n\n    def get_fixedwidth_params(self, line):\n...\n\n\nclass SimpleRSTData(FixedWidthData):\n    start_line = 3\n    end_line = -1\n    splitter_class = FixedWidthTwoLineDataSplitter\n\n\nclass RST(FixedWidth):\n\n    _format_name = \"rst\"\n    _description = \"reStructuredText simple table\"\n    data_class = SimpleRSTData\n    header_class = SimpleRSTHeader\n\n    def __init__(self):\n...\n\n    def write(self, lines):\n...\n\n```\n\n### File: astropy/io/ascii/core.py ###\n```python\n# Licensed under a 3-clause BSD style license - see LICENSE.rst\n\n# Global dictionary mapping format arg to the corresponding Reader class\nFORMAT_CLASSES = {}\n\n# Similar dictionary for fast readers\nFAST_CLASSES = {}\n\n\ndef _check_multidim_table(table, max_ndim):\n...\n\n\nclass CsvWriter:\n\n    # Random 16-character string that gets injected instead of any\n    # empty fields and is then replaced post-write with doubled-quotechar.\n    # Created with:\n    # ''.join(random.choice(string.printable[:90]) for _ in range(16))\n    replace_sentinel = \"2b=48Av%0-V3p>bX\"\n\n    def __init__(self, csvfile=None, **kwargs):\n...\n\n    def writerow(self, values):\n...\n\n    def writerows(self, values_list):\n...\n\n    def _writerow(self, writerow_func, values, has_empty):\n...\n\n\nclass MaskedConstant(numpy.ma.core.MaskedConstant):\n\n    def __hash__(self):\n...\n\n    def __copy__(self):\n...\n\n    def __deepcopy__(self, memo):\n...\n\n\nmasked = MaskedConstant()\n\n\nclass InconsistentTableError(ValueError):\n    pass\n\n\nclass OptionalTableImportError(ImportError):\n    pass\n\n\nclass ParameterError(NotImplementedError):\n    pass\n\n\nclass FastOptionsError(NotImplementedError):\n    pass\n\n\nclass NoType:\n    pass\n\n\nclass StrType(NoType):\n    pass\n\n\nclass NumType(NoType):\n    pass\n\n\nclass FloatType(NumType):\n    pass\n\n\nclass BoolType(NoType):\n    pass\n\n\nclass IntType(NumType):\n    pass\n\n\nclass AllType(StrType, FloatType, IntType):\n    pass\n\n\nclass Column:\n\n    def __init__(self, name):\n...\n\n\nclass BaseInputter:\n\n    encoding = None\n\n    def get_lines(self, table, newline=None):\n...\n\n    def process_lines(self, lines):\n...\n\n\nclass BaseSplitter:\n\n    delimiter = None\n\n    def process_line(self, line):\n...\n\n    def process_val(self, val):\n...\n\n    def __call__(self, lines):\n...\n\n    def join(self, vals):\n...\n\n\nclass DefaultSplitter(BaseSplitter):\n\n    delimiter = \" \"\n    quotechar = '\"'\n    doublequote = True\n    escapechar = None\n    quoting = csv.QUOTE_MINIMAL\n    skipinitialspace = True\n    csv_writer = None\n    csv_writer_out = StringIO()\n\n    def process_line(self, line):\n...\n\n    def process_val(self, val):\n...\n\n    def __call__(self, lines):\n...\n\n    def join(self, vals):\n...\n\n\ndef _replace_tab_with_space(line, escapechar, quotechar):\n...\n\n\ndef _get_line_index(line_or_func, lines):\n...\n\n\nclass BaseHeader:\n\n    auto_format = \"col{}\"\n    start_line = None\n    comment = None\n    splitter_class = DefaultSplitter\n    names = None\n    write_comment = False\n    write_spacer_lines = [\"ASCII_TABLE_WRITE_SPACER_LINE\"]\n\n    def __init__(self):\n...\n\n    def _set_cols_from_names(self):\n...\n\n    def update_meta(self, lines, meta):\n...\n\n    def get_cols(self, lines):\n...\n\n    def process_lines(self, lines):\n...\n\n    def write_comments(self, lines, meta):\n...\n\n    def write(self, lines):\n...\n\n    @property\n    def colnames(self):\n...\n\n    def remove_columns(self, names):\n...\n\n    def rename_column(self, name, new_name):\n...\n\n    def get_type_map_key(self, col):\n...\n\n    def get_col_type(self, col):\n...\n\n    def check_column_names(self, names, strict_names, guessing):\n...\n\n\nclass BaseData:\n\n    start_line = None\n    end_line = None\n    comment = None\n    splitter_class = DefaultSplitter\n    write_spacer_lines = [\"ASCII_TABLE_WRITE_SPACER_LINE\"]\n    fill_include_names = None\n    fill_exclude_names = None\n    fill_values = [(masked, \"\")]\n    formats = {}\n\n    def __init__(self):\n...\n\n    def process_lines(self, lines):\n...\n\n    def get_data_lines(self, lines):\n...\n\n    def get_str_vals(self):\n...\n\n    def masks(self, cols):\n...\n\n    def _set_fill_values(self, cols):\n...\n\n    def _set_masks(self, cols):\n...\n\n    def _replace_vals(self, cols):\n...\n\n    def str_vals(self):\n...\n\n    def write(self, lines):\n...\n\n    def _set_col_formats(self):\n...\n\n\ndef convert_numpy(numpy_type):\n...\n\n\nclass BaseOutputter:\n\n    # User-defined converters which gets set in ascii.ui if a `converter` kwarg\n    # is supplied.\n    converters = {}\n\n    # Derived classes must define default_converters and __call__\n\n    @staticmethod\n    def _validate_and_copy(col, converters):\n...\n\n    def _convert_vals(self, cols):\n...\n\n\ndef _deduplicate_names(names):\n...\n\n\nclass TableOutputter(BaseOutputter):\n\n    default_converters = [convert_numpy(int), convert_numpy(float), convert_numpy(str)]\n\n    def __call__(self, cols, meta):\n...\n\n\nclass MetaBaseReader(type):\n    def __init__(cls, name, bases, dct):\n...\n\n\ndef _is_number(x):\n...\n\n\ndef _apply_include_exclude_names(table, names, include_names, exclude_names):\n...\n\n\nclass BaseReader(metaclass=MetaBaseReader):\n\n    names = None\n    include_names = None\n    exclude_names = None\n    strict_names = False\n    guessing = False\n    encoding = None\n\n    header_class = BaseHeader\n    data_class = BaseData\n    inputter_class = BaseInputter\n    outputter_class = TableOutputter\n\n    # Max column dimension that writer supports for this format. Exceptions\n    # include ECSV (no limit) and HTML (max_ndim=2).\n    max_ndim = 1\n\n    def __init__(self):\n...\n\n    def _check_multidim_table(self, table):\n...\n\n    def read(self, table):\n...\n\n    def inconsistent_handler(self, str_vals, ncols):\n...\n\n    @property\n    def comment_lines(self):\n...\n\n    def update_table_data(self, table):\n...\n\n    def write_header(self, lines, meta):\n...\n\n    def write(self, table):\n...\n\n\nclass ContinuationLinesInputter(BaseInputter):\n\n    continuation_char = \"\\\\\"\n    replace_char = \" \"\n    # If no_continue is not None then lines matching this regex are not subject\n    # to line continuation.  The initial use case here is Daophot.  In this\n    # case the continuation character is just replaced with replace_char.\n    no_continue = None\n\n    def process_lines(self, lines):\n...\n\n\nclass WhitespaceSplitter(DefaultSplitter):\n    def process_line(self, line):\n...\n\n\nextra_reader_pars = (\n    \"Reader\",\n    \"Inputter\",\n    \"Outputter\",\n    \"delimiter\",\n    \"comment\",\n    \"quotechar\",\n    \"header_start\",\n    \"data_start\",\n    \"data_end\",\n    \"converters\",\n    \"encoding\",\n    \"data_Splitter\",\n    \"header_Splitter\",\n    \"names\",\n    \"include_names\",\n    \"exclude_names\",\n    \"strict_names\",\n    \"fill_values\",\n    \"fill_include_names\",\n    \"fill_exclude_names\",\n)\n\n\ndef _get_reader(Reader, Inputter=None, Outputter=None, **kwargs):\n...\n\n\nextra_writer_pars = (\n    \"delimiter\",\n    \"comment\",\n    \"quotechar\",\n    \"formats\",\n    \"strip_whitespace\",\n    \"names\",\n    \"include_names\",\n    \"exclude_names\",\n    \"fill_values\",\n    \"fill_include_names\",\n    \"fill_exclude_names\",\n)\n\n\ndef _get_writer(Writer, fast_writer, **kwargs):\n...\n\n```\n\n### File: astropy/io/ascii/ui.py ###\n```python\n# Licensed under a 3-clause BSD style license - see LICENSE.rst\n\n_read_trace = []\n\n# Default setting for guess parameter in read()\n_GUESS = True\n\n\ndef _probably_html(table, maxchars=100000):\n...\n\n\ndef set_guess(guess):\n...\n\n\ndef get_reader(Reader=None, Inputter=None, Outputter=None, **kwargs):\n...\n\n\ndef _get_format_class(format, ReaderWriter, label):\n...\n\n\ndef _get_fast_reader_dict(kwargs):\n...\n\n\ndef _validate_read_write_kwargs(read_write, **kwargs):\n...\n\n\ndef _expand_user_if_path(argument):\n...\n\n\ndef read(table, guess=None, **kwargs):\n...\n\n\nread.__doc__ = core.READ_DOCSTRING\n\n\ndef _guess(table, read_kwargs, format, fast_reader):\n...\n\n\ndef _get_guess_kwargs_list(read_kwargs):\n...\n\n\ndef _read_in_chunks(table, **kwargs):\n...\n\n\ndef _read_in_chunks_generator(table, chunk_size, **kwargs):\n...\n\n\nextra_writer_pars = (\n    \"delimiter\",\n    \"comment\",\n    \"quotechar\",\n    \"formats\",\n    \"names\",\n    \"include_names\",\n    \"exclude_names\",\n    \"strip_whitespace\",\n)\n\n\ndef get_writer(Writer=None, fast_writer=True, **kwargs):\n...\n\n\ndef write(\n    table,\n    output=None,\n    format=None,\n    Writer=None,\n    fast_writer=True,\n    *,\n    overwrite=False,\n    **kwargs,\n):\n...\n\n\nwrite.__doc__ = core.WRITE_DOCSTRING\n\n\ndef get_read_trace():\n...\n\n```\n\n\n###\n\nPlease provide the complete set of locations as either a class name, a function name, or a variable name.\nNote that if you include a class, you do not need to list its specific methods.\nYou can include either the entire class or don't include the class name and instead include specific methods in the class.\n### Examples:\n```\nfull_path1/file1.py\nfunction: my_function_1\nclass: MyClass1\nfunction: MyClass2.my_method\n\nfull_path2/file2.py\nvariable: my_var\nfunction: MyClass3.my_method\n\nfull_path3/file3.py\nfunction: my_function_2\nfunction: my_function_3\nfunction: MyClass4.my_method_1\nclass: MyClass5\n```\n\nReturn just the locations wrapped with ```.\n"}], "found_edit_locs": {"astropy/io/ascii/rst.py": ["\nclass: RST\nline: 60\nfunction: RST.__init__\nline: 61"], "astropy/io/ascii/ui.py": ["\nfunction: get_writer\nline: 861"], "astropy/io/ascii/core.py": ["\nfunction: _get_writer\nline: 1787"]}, "additional_artifact_loc_edit_location": [{"raw_output_loc": ["```\nastropy/io/ascii/rst.py\nclass: RST\nline: 60\nfunction: RST.__init__\nline: 61\n\nastropy/io/ascii/ui.py\nfunction: get_writer\nline: 861\n\nastropy/io/ascii/core.py\nfunction: _get_writer\nline: 1787\n```", "```\nastropy/io/ascii/rst.py\nclass: RST\nline: 60\n\nastropy/io/ascii/core.py\nline: 1793\n\nastropy/io/ascii/ui.py\nline: 863\nfunction: get_writer\nline: 901\n```", "```\nastropy/io/ascii/connect.py\nline: 26\n\nastropy/io/ascii/rst.py\nclass: RST\nline: 60\n\nastropy/io/ascii/ui.py\nline: 861\nline: 901\n\nastropy/io/ascii/core.py\nfunction: _get_writer\nline: 1790\n```", "```\nastropy/io/ascii/rst.py\nclass: SimpleRSTHeader\nline: 16\nline: 22\n\nastropy/io/ascii/rst.py\nclass: SimpleRSTData\nline: 29\n\nastropy/io/ascii/rst.py\nclass: RST\nline: 35\nline: 60\nline: 63\n\nastropy/io/ascii/core.py\nfunction: _get_writer\nline: 1815\n\nastropy/io/ascii/ui.py\nfunction: get_writer\nline: 864\n\nastropy/io/ascii/ui.py\nfunction: write\nline: 921\n```"]}], "edit_loc_traj": {"prompt": "\nPlease review the following GitHub problem description and relevant files, and provide a set of locations that need to be edited to fix the issue.\nThe locations can be specified as class names, function or method names, or exact line numbers that require modification.\n\n### GitHub Problem Description ###\nPlease support header rows in RestructuredText output\n### Description\r\n\r\nIt would be great if the following would work:\r\n\r\n```Python\r\n>>> from astropy.table import QTable\r\n>>> import astropy.units as u\r\n>>> import sys\r\n>>> tbl = QTable({'wave': [350,950]*u.nm, 'response': [0.7, 1.2]*u.count})\r\n>>> tbl.write(sys.stdout,  format=\"ascii.rst\")\r\n===== ========\r\n wave response\r\n===== ========\r\n350.0      0.7\r\n950.0      1.2\r\n===== ========\r\n>>> tbl.write(sys.stdout,  format=\"ascii.fixed_width\", header_rows=[\"name\", \"unit\"])\r\n|  wave | response |\r\n|    nm |       ct |\r\n| 350.0 |      0.7 |\r\n| 950.0 |      1.2 |\r\n>>> tbl.write(sys.stdout,  format=\"ascii.rst\", header_rows=[\"name\", \"unit\"])\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/usr/lib/python3/dist-packages/astropy/table/connect.py\", line 129, in __call__\r\n    self.registry.write(instance, *args, **kwargs)\r\n  File \"/usr/lib/python3/dist-packages/astropy/io/registry/core.py\", line 369, in write\r\n    return writer(data, *args, **kwargs)\r\n  File \"/usr/lib/python3/dist-packages/astropy/io/ascii/connect.py\", line 26, in io_write\r\n    return write(table, filename, **kwargs)\r\n  File \"/usr/lib/python3/dist-packages/astropy/io/ascii/ui.py\", line 856, in write\r\n    writer = get_writer(Writer=Writer, fast_writer=fast_writer, **kwargs)\r\n  File \"/usr/lib/python3/dist-packages/astropy/io/ascii/ui.py\", line 800, in get_writer\r\n    writer = core._get_writer(Writer, fast_writer, **kwargs)\r\n  File \"/usr/lib/python3/dist-packages/astropy/io/ascii/core.py\", line 1719, in _get_writer\r\n    writer = Writer(**writer_kwargs)\r\nTypeError: RST.__init__() got an unexpected keyword argument 'header_rows'\r\n```\r\n\r\n\r\n### Additional context\r\n\r\nRestructuredText output is a great way to fill autogenerated documentation with content, so having this flexible makes the life easier `:-)`\r\n\r\n\n\n\n###\n### astropy/io/ascii/rst.py\n...\n6|\n7|from .core import DefaultSplitter\n8|from .fixedwidth import (\n9|    FixedWidth,\n10|    FixedWidthData,\n11|    FixedWidthHeader,\n12|    FixedWidthTwoLineDataSplitter,\n13|)\n14|\n15|\n16|class SimpleRSTHeader(FixedWidthHeader):\n17|    position_line = 0\n18|    start_line = 1\n19|    splitter_class = DefaultSplitter\n20|    position_char = \"=\"\n21|\n22|    def get_fixedwidth_params(self, line):\n23|        vals, starts, ends = super().get_fixedwidth_params(line)\n24|        # The right hand column can be unbounded\n25|        ends[-1] = None\n26|        return vals, starts, ends\n27|\n28|\n29|class SimpleRSTData(FixedWidthData):\n30|    start_line = 3\n31|    end_line = -1\n32|    splitter_class = FixedWidthTwoLineDataSplitter\n33|\n34|\n35|class RST(FixedWidth):\n36|    \"\"\"reStructuredText simple format table.\n37|\n38|    See: https://docutils.sourceforge.io/docs/ref/rst/restructuredtext.html#simple-tables\n39|\n40|    Example::\n41|\n42|        ==== ===== ======\n43|        Col1  Col2  Col3\n44|        ==== ===== ======\n45|          1    2.3  Hello\n46|          2    4.5  Worlds\n47|        ==== ===== ======\n48|\n49|    Currently there is no support for reading tables which utilize continuation lines,\n50|    or for ones which define column spans through the use of an additional\n51|    line of dashes in the header.\n52|\n53|    \"\"\"\n54|\n55|    _format_name = \"rst\"\n56|    _description = \"reStructuredText simple table\"\n57|    data_class = SimpleRSTData\n58|    header_class = SimpleRSTHeader\n59|\n60|    def __init__(self):\n61|        super().__init__(delimiter_pad=None, bookend=False)\n62|\n63|    def write(self, lines):\n64|        lines = super().write(lines)\n65|        lines = [lines[1]] + lines + [lines[1]]\n66|        return lines\n\n\n### astropy/io/ascii/core.py\n...\n558|    elif line_or_func:\n559|        if line_or_func >= 0:\n560|            return line_or_func\n561|        else:\n562|            n_lines = sum(1 for line in lines)\n563|            return n_lines + line_or_func\n564|    else:\n565|        return line_or_func\n566|\n567|\n568|class BaseHeader:\n569|    \"\"\"\n570|    Base table header reader\n571|    \"\"\"\n572|\n573|    auto_format = \"col{}\"\n574|    \"\"\" format string for auto-generating column names \"\"\"\n575|    start_line = None\n576|    \"\"\" None, int, or a function of ``lines`` that returns None or int \"\"\"\n577|    comment = None\n578|    \"\"\" regular expression for comment lines \"\"\"\n579|    splitter_class = DefaultSplitter\n580|    \"\"\" Splitter class for splitting data lines into columns \"\"\"\n581|    names = None\n582|    \"\"\" list of names corresponding to each data column \"\"\"\n583|    write_comment = False\n584|    write_spacer_lines = [\"ASCII_TABLE_WRITE_SPACER_LINE\"]\n585|\n586|    def __init__(self):\n587|        self.splitter = self.splitter_class()\n588|\n589|    def _set_cols_from_names(self):\n590|        self.cols = [Column(name=x) for x in self.names]\n591|\n592|    def update_meta(self, lines, meta):\n593|        \"\"\"\n594|        Extract any table-level metadata, e.g. keywords, comments, column metadata, from\n595|        the table ``lines`` and update the OrderedDict ``meta`` in place.  This base\n596|        method extracts comment lines and stores them in ``meta`` for output.\n597|        \"\"\"\n598|        if self.comment:\n599|            re_comment = re.compile(self.comment)\n600|            comment_lines = [x for x in lines if re_comment.match(x)]\n601|        else:\n602|            comment_lines = []\n603|        comment_lines = [\n604|            re.sub(\"^\" + self.comment, \"\", x).strip() for x in comment_lines\n605|        ]\n606|        if comment_lines:\n607|            meta.setdefault(\"table\", {})[\"comments\"] = comment_lines\n608|\n609|    def get_cols(self, lines):\n610|        \"\"\"Initialize the header Column objects from the table ``lines``.\n611|\n612|        Based on the previously set Header attributes find or create the column names.\n613|        Sets ``self.cols`` with the list of Columns.\n614|\n615|        Parameters\n616|        ----------\n617|        lines : list\n618|            List of table lines\n619|\n620|        \"\"\"\n621|\n622|        start_line = _get_line_index(self.start_line, self.process_lines(lines))\n623|        if start_line is None:\n624|            # No header line so auto-generate names from n_data_cols\n625|            # Get the data values from the first line of table data to determine n_data_cols\n626|            try:\n627|                first_data_vals = next(self.data.get_str_vals())\n628|            except StopIteration:\n629|                raise InconsistentTableError(\n630|                    \"No data lines found so cannot autogenerate column names\"\n631|                )\n632|            n_data_cols = len(first_data_vals)\n633|            self.names = [self.auto_format.format(i) for i in range(1, n_data_cols + 1)]\n634|\n635|        else:\n636|            for i, line in enumerate(self.process_lines(lines)):\n637|                if i == start_line:\n638|                    break\n639|            else:  # No header line matching\n640|                raise ValueError(\"No header line found in table\")\n641|\n642|            self.names = next(self.splitter([line]))\n643|\n644|        self._set_cols_from_names()\n645|\n646|    def process_lines(self, lines):\n647|        \"\"\"Generator to yield non-blank and non-comment lines\"\"\"\n648|        re_comment = re.compile(self.comment) if self.comment else None\n649|        # Yield non-comment lines\n650|        for line in lines:\n651|            if line.strip() and (not self.comment or not re_comment.match(line)):\n652|                yield line\n653|\n654|    def write_comments(self, lines, meta):\n655|        if self.write_comment not in (False, None):\n656|            for comment in meta.get(\"comments\", []):\n657|                lines.append(self.write_comment + comment)\n658|\n659|    def write(self, lines):\n660|        if self.start_line is not None:\n661|            for i, spacer_line in zip(\n662|                range(self.start_line), itertools.cycle(self.write_spacer_lines)\n663|            ):\n664|                lines.append(spacer_line)\n665|            lines.append(self.splitter.join([x.info.name for x in self.cols]))\n666|\n667|    @property\n668|    def colnames(self):\n669|        \"\"\"Return the column names of the table\"\"\"\n670|        return tuple(\n671|            col.name if isinstance(col, Column) else col.info.name for col in self.cols\n672|        )\n673|\n674|    def remove_columns(self, names):\n675|        \"\"\"\n676|        Remove several columns from the table.\n677|\n678|        Parameters\n679|        ----------\n680|        names : list\n681|            A list containing the names of the columns to remove\n682|        \"\"\"\n683|        colnames = self.colnames\n684|        for name in names:\n685|            if name not in colnames:\n686|                raise KeyError(f\"Column {name} does not exist\")\n687|\n688|        self.cols = [col for col in self.cols if col.name not in names]\n689|\n690|    def rename_column(self, name, new_name):\n691|        \"\"\"\n692|        Rename a column.\n693|\n694|        Parameters\n695|        ----------\n696|        name : str\n697|            The current name of the column.\n698|        new_name : str\n699|            The new name for the column\n700|        \"\"\"\n701|        try:\n702|            idx = self.colnames.index(name)\n703|        except ValueError:\n704|            raise KeyError(f\"Column {name} does not exist\")\n705|\n706|        col = self.cols[idx]\n707|\n708|        # For writing self.cols can contain cols that are not Column.  Raise\n709|        # exception in that case.\n710|        if isinstance(col, Column):\n711|            col.name = new_name\n712|        else:\n713|            raise TypeError(f\"got column type {type(col)} instead of required {Column}\")\n714|\n715|    def get_type_map_key(self, col):\n716|        return col.raw_type\n717|\n718|    def get_col_type(self, col):\n719|        try:\n720|            type_map_key = self.get_type_map_key(col)\n721|            return self.col_type_map[type_map_key.lower()]\n722|        except KeyError:\n723|            raise ValueError(\n724|                'Unknown data type \"\"{}\"\" for column \"{}\"'.format(\n725|                    col.raw_type, col.name\n726|                )\n727|            )\n728|\n729|    def check_column_names(self, names, strict_names, guessing):\n730|        \"\"\"\n731|        Check column names.\n732|\n733|        This must be done before applying the names transformation\n734|        so that guessing will fail appropriately if ``names`` is supplied.\n735|        For instance if the basic reader is given a table with no column header\n736|        row.\n737|\n738|        Parameters\n739|        ----------\n740|        names : list\n741|            User-supplied list of column names\n742|        strict_names : bool\n743|            Whether to impose extra requirements on names\n744|        guessing : bool\n745|            True if this method is being called while guessing the table format\n746|        \"\"\"\n747|        if strict_names:\n748|            # Impose strict requirements on column names (normally used in guessing)\n749|            bads = [\" \", \",\", \"|\", \"\\t\", \"'\", '\"']\n750|            for name in self.colnames:\n751|                if (\n752|                    _is_number(name)\n753|                    or len(name) == 0\n754|                    or name[0] in bads\n755|                    or name[-1] in bads\n756|                ):\n757|                    raise InconsistentTableError(\n758|                        f\"Column name {name!r} does not meet strict name requirements\"\n759|                    )\n760|        # When guessing require at least two columns, except for ECSV which can\n761|        # reliably be guessed from the header requirements.\n762|        if (\n763|            guessing\n764|            and len(self.colnames) <= 1\n765|            and self.__class__.__name__ != \"EcsvHeader\"\n766|        ):\n767|            raise ValueError(\n768|                \"Table format guessing requires at least two columns, got {}\".format(\n769|                    list(self.colnames)\n770|                )\n771|            )\n772|\n773|        if names is not None and len(names) != len(self.colnames):\n774|            raise InconsistentTableError(\n775|                \"Length of names argument ({}) does not match number\"\n776|                \" of table columns ({})\".format(len(names), len(self.colnames))\n777|            )\n778|\n779|\n780|class BaseData:\n781|    \"\"\"\n782|    Base table data reader.\n783|    \"\"\"\n784|\n785|    start_line = None\n786|    \"\"\" None, int, or a function of ``lines`` that returns None or int \"\"\"\n787|    end_line = None\n788|    \"\"\" None, int, or a function of ``lines`` that returns None or int \"\"\"\n789|    comment = None\n790|    \"\"\" Regular expression for comment lines \"\"\"\n791|    splitter_class = DefaultSplitter\n792|    \"\"\" Splitter class for splitting data lines into columns \"\"\"\n793|    write_spacer_lines = [\"ASCII_TABLE_WRITE_SPACER_LINE\"]\n794|    fill_include_names = None\n795|    fill_exclude_names = None\n796|    fill_values = [(masked, \"\")]\n797|    formats = {}\n798|\n799|    def __init__(self):\n800|        # Need to make sure fill_values list is instance attribute, not class attribute.\n801|        # On read, this will be overwritten by the default in the ui.read (thus, in\n802|        # the current implementation there can be no different default for different\n803|        # Readers). On write, ui.py does not specify a default, so this line here matters.\n804|        self.fill_values = copy.copy(self.fill_values)\n805|        self.formats = copy.copy(self.formats)\n806|        self.splitter = self.splitter_class()\n807|\n808|    def process_lines(self, lines):\n809|        \"\"\"\n810|        READ: Strip out comment lines and blank lines from list of ``lines``\n811|\n812|        Parameters\n813|        ----------\n814|        lines : list\n815|            All lines in table\n816|\n817|        Returns\n818|        -------\n819|        lines : list\n820|            List of lines\n821|\n822|        \"\"\"\n823|        nonblank_lines = (x for x in lines if x.strip())\n824|        if self.comment:\n825|            re_comment = re.compile(self.comment)\n826|            return [x for x in nonblank_lines if not re_comment.match(x)]\n827|        else:\n828|            return [x for x in nonblank_lines]\n829|\n830|    def get_data_lines(self, lines):\n831|        \"\"\"\n832|        READ: Set ``data_lines`` attribute to lines slice comprising table data values.\n833|        \"\"\"\n834|        data_lines = self.process_lines(lines)\n835|        start_line = _get_line_index(self.start_line, data_lines)\n836|        end_line = _get_line_index(self.end_line, data_lines)\n837|\n838|        if start_line is not None or end_line is not None:\n839|            self.data_lines = data_lines[slice(start_line, end_line)]\n840|        else:  # Don't copy entire data lines unless necessary\n841|            self.data_lines = data_lines\n842|\n843|    def get_str_vals(self):\n844|        \"\"\"Return a generator that returns a list of column values (as strings)\n845|        for each data line.\"\"\"\n846|        return self.splitter(self.data_lines)\n847|\n848|    def masks(self, cols):\n849|        \"\"\"READ: Set fill value for each column and then apply that fill value\n850|\n851|        In the first step it is evaluated with value from ``fill_values`` applies to\n852|        which column using ``fill_include_names`` and ``fill_exclude_names``.\n853|        In the second step all replacements are done for the appropriate columns.\n854|        \"\"\"\n855|        if self.fill_values:\n856|            self._set_fill_values(cols)\n857|            self._set_masks(cols)\n858|\n859|    def _set_fill_values(self, cols):\n860|        \"\"\"READ, WRITE: Set fill values of individual cols based on fill_values of BaseData\n861|\n862|        fill values has the following form:\n863|        <fill_spec> = (<bad_value>, <fill_value>, <optional col_name>...)\n864|        fill_values = <fill_spec> or list of <fill_spec>'s\n865|\n866|        \"\"\"\n867|        if self.fill_values:\n868|            # when we write tables the columns may be astropy.table.Columns\n869|            # which don't carry a fill_values by default\n870|            for col in cols:\n871|                if not hasattr(col, \"fill_values\"):\n872|                    col.fill_values = {}\n873|\n874|            # if input is only one <fill_spec>, then make it a list\n875|            with suppress(TypeError):\n876|                self.fill_values[0] + \"\"\n877|                self.fill_values = [self.fill_values]\n878|\n879|            # Step 1: Set the default list of columns which are affected by\n880|            # fill_values\n881|            colnames = set(self.header.colnames)\n882|            if self.fill_include_names is not None:\n883|                colnames.intersection_update(self.fill_include_names)\n884|            if self.fill_exclude_names is not None:\n885|                colnames.difference_update(self.fill_exclude_names)\n886|\n887|            # Step 2a: Find out which columns are affected by this tuple\n888|            # iterate over reversed order, so last condition is set first and\n889|            # overwritten by earlier conditions\n890|            for replacement in reversed(self.fill_values):\n891|                if len(replacement) < 2:\n892|                    raise ValueError(\n893|                        \"Format of fill_values must be \"\n894|                        \"(<bad>, <fill>, <optional col1>, ...)\"\n895|                    )\n896|                elif len(replacement) == 2:\n897|                    affect_cols = colnames\n898|                else:\n899|                    affect_cols = replacement[2:]\n900|\n901|                for i, key in (\n902|                    (i, x)\n903|                    for i, x in enumerate(self.header.colnames)\n904|                    if x in affect_cols\n905|                ):\n906|                    cols[i].fill_values[replacement[0]] = str(replacement[1])\n907|\n908|    def _set_masks(self, cols):\n909|        \"\"\"READ: Replace string values in col.str_vals and set masks\"\"\"\n910|        if self.fill_values:\n911|            for col in (col for col in cols if col.fill_values):\n912|                col.mask = numpy.zeros(len(col.str_vals), dtype=bool)\n913|                for i, str_val in (\n914|                    (i, x) for i, x in enumerate(col.str_vals) if x in col.fill_values\n915|                ):\n916|                    col.str_vals[i] = col.fill_values[str_val]\n917|                    col.mask[i] = True\n918|\n919|    def _replace_vals(self, cols):\n920|        \"\"\"WRITE: replace string values in col.str_vals\"\"\"\n921|        if self.fill_values:\n922|            for col in (col for col in cols if col.fill_values):\n923|                for i, str_val in (\n924|                    (i, x) for i, x in enumerate(col.str_vals) if x in col.fill_values\n925|                ):\n926|                    col.str_vals[i] = col.fill_values[str_val]\n927|                if masked in col.fill_values and hasattr(col, \"mask\"):\n928|                    mask_val = col.fill_values[masked]\n929|                    for i in col.mask.nonzero()[0]:\n930|                        col.str_vals[i] = mask_val\n931|\n932|    def str_vals(self):\n933|        \"\"\"WRITE: convert all values in table to a list of lists of strings\n934|\n935|        This sets the fill values and possibly column formats from the input\n936|        formats={} keyword, then ends up calling table.pprint._pformat_col_iter()\n937|        by a circuitous path. That function does the real work of formatting.\n938|        Finally replace anything matching the fill_values.\n939|\n940|        Returns\n941|        -------\n942|        values : list of list of str\n943|        \"\"\"\n944|        self._set_fill_values(self.cols)\n945|        self._set_col_formats()\n946|        for col in self.cols:\n947|            col.str_vals = list(col.info.iter_str_vals())\n948|        self._replace_vals(self.cols)\n949|        return [col.str_vals for col in self.cols]\n950|\n951|    def write(self, lines):\n952|        \"\"\"Write ``self.cols`` in place to ``lines``.\n953|\n954|        Parameters\n955|        ----------\n956|        lines : list\n957|            List for collecting output of writing self.cols.\n958|        \"\"\"\n959|        if hasattr(self.start_line, \"__call__\"):\n960|            raise TypeError(\"Start_line attribute cannot be callable for write()\")\n961|        else:\n962|            data_start_line = self.start_line or 0\n963|\n964|        while len(lines) < data_start_line:\n965|            lines.append(itertools.cycle(self.write_spacer_lines))\n966|\n967|        col_str_iters = self.str_vals()\n968|        for vals in zip(*col_str_iters):\n969|            lines.append(self.splitter.join(vals))\n970|\n971|    def _set_col_formats(self):\n972|        \"\"\"WRITE: set column formats.\"\"\"\n973|        for col in self.cols:\n974|            if col.info.name in self.formats:\n975|                col.info.format = self.formats[col.info.name]\n976|\n977|\n978|def convert_numpy(numpy_type):\n979|    \"\"\"Return a tuple containing a function which converts a list into a numpy\n980|    array and the type produced by the converter function.\n981|\n982|    Parameters\n983|    ----------\n984|    numpy_type : numpy data-type\n985|        The numpy type required of an array returned by ``converter``. Must be a\n...\n1783|    \"strip_whitespace\",\n1784|    \"names\",\n1785|    \"include_names\",\n1786|    \"exclude_names\",\n1787|    \"fill_values\",\n1788|    \"fill_include_names\",\n1789|    \"fill_exclude_names\",\n1790|)\n1791|\n1792|\n1793|def _get_writer(Writer, fast_writer, **kwargs):\n1794|    \"\"\"Initialize a table writer allowing for common customizations. This\n1795|    routine is for internal (package) use only and is useful because it depends\n1796|    only on the \"core\" module.\"\"\"\n1797|\n1798|    from .fastbasic import FastBasic\n1799|\n1800|    # A value of None for fill_values imply getting the default string\n1801|    # representation of masked values (depending on the writer class), but the\n1802|    # machinery expects a list.  The easiest here is to just pop the value off,\n1803|    # i.e. fill_values=None is the same as not providing it at all.\n1804|    if \"fill_values\" in kwargs and kwargs[\"fill_values\"] is None:\n1805|        del kwargs[\"fill_values\"]\n1806|\n1807|    if issubclass(Writer, FastBasic):  # Fast writers handle args separately\n1808|        return Writer(**kwargs)\n1809|    elif fast_writer and f\"fast_{Writer._format_name}\" in FAST_CLASSES:\n1810|        # Switch to fast writer\n1811|        kwargs[\"fast_writer\"] = fast_writer\n1812|        return FAST_CLASSES[f\"fast_{Writer._format_name}\"](**kwargs)\n1813|\n1814|    writer_kwargs = {k: v for k, v in kwargs.items() if k not in extra_writer_pars}\n1815|    writer = Writer(**writer_kwargs)\n1816|\n1817|    if \"delimiter\" in kwargs:\n1818|        writer.header.splitter.delimiter = kwargs[\"delimiter\"]\n1819|        writer.data.splitter.delimiter = kwargs[\"delimiter\"]\n1820|    if \"comment\" in kwargs:\n1821|        writer.header.write_comment = kwargs[\"comment\"]\n1822|        writer.data.write_comment = kwargs[\"comment\"]\n1823|    if \"quotechar\" in kwargs:\n1824|        writer.header.splitter.quotechar = kwargs[\"quotechar\"]\n1825|        writer.data.splitter.quotechar = kwargs[\"quotechar\"]\n1826|    if \"formats\" in kwargs:\n1827|        writer.data.formats = kwargs[\"formats\"]\n1828|    if \"strip_whitespace\" in kwargs:\n1829|        if kwargs[\"strip_whitespace\"]:\n1830|            # Restore the default SplitterClass process_val method which strips\n1831|            # whitespace.  This may have been changed in the Writer\n1832|            # initialization (e.g. Rdb and Tab)\n1833|            writer.data.splitter.process_val = operator.methodcaller(\"strip\", \" \\t\")\n1834|        else:\n1835|            writer.data.splitter.process_val = None\n1836|    if \"names\" in kwargs:\n1837|        writer.header.names = kwargs[\"names\"]\n1838|    if \"include_names\" in kwargs:\n1839|        writer.include_names = kwargs[\"include_names\"]\n1840|    if \"exclude_names\" in kwargs:\n1841|        writer.exclude_names = kwargs[\"exclude_names\"]\n1842|    if \"fill_values\" in kwargs:\n1843|        # Prepend user-specified values to the class default.\n1844|        with suppress(TypeError, IndexError):\n1845|            # Test if it looks like (match, replace_string, optional_colname),\n1846|            # in which case make it a list\n1847|            kwargs[\"fill_values\"][1] + \"\"\n1848|            kwargs[\"fill_values\"] = [kwargs[\"fill_values\"]]\n1849|        writer.data.fill_values = kwargs[\"fill_values\"] + writer.data.fill_values\n1850|    if \"fill_include_names\" in kwargs:\n1851|        writer.data.fill_include_names = kwargs[\"fill_include_names\"]\n1852|    if \"fill_exclude_names\" in kwargs:\n1853|        writer.data.fill_exclude_names = kwargs[\"fill_exclude_names\"]\n1854|    return writer\n\n\n### astropy/io/ascii/ui.py\n...\n854|    \"comment\",\n855|    \"quotechar\",\n856|    \"formats\",\n857|    \"names\",\n858|    \"include_names\",\n859|    \"exclude_names\",\n860|    \"strip_whitespace\",\n861|)\n862|\n863|\n864|def get_writer(Writer=None, fast_writer=True, **kwargs):\n865|    \"\"\"\n866|    Initialize a table writer allowing for common customizations.  Most of the\n867|    default behavior for various parameters is determined by the Writer class.\n868|\n869|    Parameters\n870|    ----------\n871|    Writer : ``Writer``\n872|        Writer class (DEPRECATED). Defaults to :class:`Basic`.\n873|    delimiter : str\n874|        Column delimiter string\n875|    comment : str\n876|        String defining a comment line in table\n877|    quotechar : str\n878|        One-character string to quote fields containing special characters\n879|    formats : dict\n880|        Dictionary of format specifiers or formatting functions\n881|    strip_whitespace : bool\n882|        Strip surrounding whitespace from column values.\n883|    names : list\n884|        List of names corresponding to each data column\n885|    include_names : list\n886|        List of names to include in output.\n887|    exclude_names : list\n888|        List of names to exclude from output (applied after ``include_names``)\n889|    fast_writer : bool\n890|        Whether to use the fast Cython writer.\n891|\n892|    Returns\n893|    -------\n894|    writer : `~astropy.io.ascii.BaseReader` subclass\n895|        ASCII format writer instance\n896|    \"\"\"\n897|    if Writer is None:\n898|        Writer = basic.Basic\n899|    if \"strip_whitespace\" not in kwargs:\n900|        kwargs[\"strip_whitespace\"] = True\n901|    writer = core._get_writer(Writer, fast_writer, **kwargs)\n902|\n903|    # Handle the corner case of wanting to disable writing table comments for the\n904|    # commented_header format.  This format *requires* a string for `write_comment`\n905|    # because that is used for the header column row, so it is not possible to\n906|    # set the input `comment` to None.  Without adding a new keyword or assuming\n907|    # a default comment character, there is no other option but to tell user to\n908|    # simply remove the meta['comments'].\n909|    if isinstance(\n910|        writer, (basic.CommentedHeader, fastbasic.FastCommentedHeader)\n911|    ) and not isinstance(kwargs.get(\"comment\", \"\"), str):\n912|        raise ValueError(\n913|            \"for the commented_header writer you must supply a string\\n\"\n914|            \"value for the `comment` keyword.  In order to disable writing\\n\"\n915|            \"table comments use `del t.meta['comments']` prior to writing.\"\n916|        )\n917|\n918|    return writer\n919|\n920|\n921|def write(\n922|    table,\n923|    output=None,\n924|    format=None,\n925|    Writer=None,\n926|    fast_writer=True,\n927|    *,\n928|    overwrite=False,\n929|    **kwargs,\n930|):\n931|    # Docstring inserted below\n932|\n933|    _validate_read_write_kwargs(\n934|        \"write\", format=format, fast_writer=fast_writer, overwrite=overwrite, **kwargs\n935|    )\n936|\n937|    if isinstance(output, (str, bytes, os.PathLike)):\n938|        output = os.path.expanduser(output)\n939|        if not overwrite and os.path.lexists(output):\n940|            raise OSError(NOT_OVERWRITING_MSG.format(output))\n941|\n942|    if output is None:\n943|        output = sys.stdout\n944|\n945|    # Ensure that `table` is a Table subclass.\n946|    names = kwargs.get(\"names\")\n947|    if isinstance(table, Table):\n948|        # While we are only going to read data from columns, we may need to\n949|        # to adjust info attributes such as format, so we make a shallow copy.\n950|        table = table.__class__(table, names=names, copy=False)\n951|    else:\n952|        # Otherwise, create a table from the input.\n953|        table = Table(table, names=names, copy=False)\n954|\n955|    table0 = table[:0].copy()\n956|    core._apply_include_exclude_names(\n957|        table0,\n958|        kwargs.get(\"names\"),\n959|        kwargs.get(\"include_names\"),\n960|        kwargs.get(\"exclude_names\"),\n961|    )\n962|    diff_format_with_names = set(kwargs.get(\"formats\", [])) - set(table0.colnames)\n963|\n964|    if diff_format_with_names:\n965|        warnings.warn(\n966|            \"The key(s) {} specified in the formats argument do not match a column\"\n967|            \" name.\".format(diff_format_with_names),\n968|            AstropyWarning,\n969|        )\n970|\n971|    if table.has_mixin_columns:\n972|        fast_writer = False\n973|\n974|    Writer = _get_format_class(format, Writer, \"Writer\")\n975|    writer = get_writer(Writer=Writer, fast_writer=fast_writer, **kwargs)\n976|    if writer._format_name in core.FAST_CLASSES:\n977|        writer.write(table, output)\n978|        return\n979|\n980|    lines = writer.write(table)\n981|\n982|    # Write the lines to output\n983|    outstr = os.linesep.join(lines)\n984|    if not hasattr(output, \"write\"):\n985|        # NOTE: we need to specify newline='', otherwise the default\n986|        # behavior is for Python to translate \\r\\n (which we write because\n987|        # of os.linesep) into \\r\\r\\n. Specifying newline='' disables any\n988|        # auto-translation.\n989|        output = open(output, \"w\", newline=\"\")\n990|        output.write(outstr)\n991|        output.write(os.linesep)\n992|        output.close()\n993|    else:\n994|        output.write(outstr)\n995|        output.write(os.linesep)\n996|\n997|\n998|write.__doc__ = core.WRITE_DOCSTRING\n999|\n1000|\n1001|def get_read_trace():\n1002|    \"\"\"\n1003|    Return a traceback of the attempted read formats for the last call to\n1004|    `~astropy.io.ascii.read` where guessing was enabled.  This is primarily for\n1005|    debugging.\n...\n\n\n\n\n###\n\nPlease provide the class name, function or method name, or the exact line numbers that need to be edited.\nThe possible location outputs should be either \"class\", \"function\" or \"line\".\n\n### Examples:\n```\nfull_path1/file1.py\nline: 10\nclass: MyClass1\nline: 51\n\nfull_path2/file2.py\nfunction: MyClass2.my_method\nline: 12\n\nfull_path3/file3.py\nfunction: my_function\nline: 24\nline: 156\n```\n\nReturn just the location(s) wrapped with ```.\n", "response": ["```\nastropy/io/ascii/rst.py\nclass: RST\nline: 60\nfunction: RST.__init__\nline: 61\n\nastropy/io/ascii/ui.py\nfunction: get_writer\nline: 861\n\nastropy/io/ascii/core.py\nfunction: _get_writer\nline: 1787\n```", "```\nastropy/io/ascii/rst.py\nclass: RST\nline: 60\n\nastropy/io/ascii/core.py\nline: 1793\n\nastropy/io/ascii/ui.py\nline: 863\nfunction: get_writer\nline: 901\n```", "```\nastropy/io/ascii/connect.py\nline: 26\n\nastropy/io/ascii/rst.py\nclass: RST\nline: 60\n\nastropy/io/ascii/ui.py\nline: 861\nline: 901\n\nastropy/io/ascii/core.py\nfunction: _get_writer\nline: 1790\n```", "```\nastropy/io/ascii/rst.py\nclass: SimpleRSTHeader\nline: 16\nline: 22\n\nastropy/io/ascii/rst.py\nclass: SimpleRSTData\nline: 29\n\nastropy/io/ascii/rst.py\nclass: RST\nline: 35\nline: 60\nline: 63\n\nastropy/io/ascii/core.py\nfunction: _get_writer\nline: 1815\n\nastropy/io/ascii/ui.py\nfunction: get_writer\nline: 864\n\nastropy/io/ascii/ui.py\nfunction: write\nline: 921\n```"], "usage": {"completion_tokens": 340, "prompt_tokens": 7724}}}
{"instance_id": "astropy__astropy-14365", "found_files": ["astropy/io/ascii/qdp.py", "astropy/io/ascii/core.py", "astropy/io/ascii/docs.py", "astropy/io/ascii/ui.py"], "additional_artifact_loc_file": {}, "file_traj": {}, "found_related_locs": {"astropy/io/ascii/qdp.py": ["function: _line_type\nfunction: _get_type_from_list_of_lines\nfunction: _get_lines_from_file\nfunction: _interpret_err_lines\nfunction: _get_tables_from_qdp_file\nfunction: _understand_err_col\nfunction: _read_table_qdp\nfunction: _write_table_qdp\nclass: QDPSplitter\nclass: QDPHeader\nclass: QDPData\nclass: QDP"], "astropy/io/ascii/core.py": ["variable: FORMAT_CLASSES\nvariable: FAST_CLASSES\nfunction: _check_multidim_table\nclass: CsvWriter\nclass: MaskedConstant\nvariable: masked\nclass: InconsistentTableError\nclass: OptionalTableImportError\nclass: ParameterError\nclass: FastOptionsError\nclass: NoType\nclass: StrType\nclass: NumType\nclass: FloatType\nclass: BoolType\nclass: IntType\nclass: AllType\nclass: Column\nclass: BaseInputter\nclass: BaseSplitter\nclass: DefaultSplitter\nfunction: _replace_tab_with_space\nfunction: _get_line_index\nclass: BaseHeader\nclass: BaseData\nfunction: convert_numpy\nclass: BaseOutputter\nfunction: _deduplicate_names\nclass: TableOutputter\nclass: MetaBaseReader\nfunction: _is_number\nfunction: _apply_include_exclude_names\nclass: BaseReader\nclass:"], "astropy/io/ascii/docs.py": [""]}, "additional_artifact_loc_related": [{"raw_output_loc": "```\nastropy/io/ascii/qdp.py\nfunction: _line_type\nfunction: _get_type_from_list_of_lines\nfunction: _get_lines_from_file\nfunction: _interpret_err_lines\nfunction: _get_tables_from_qdp_file\nfunction: _understand_err_col\nfunction: _read_table_qdp\nfunction: _write_table_qdp\nclass: QDPSplitter\nclass: QDPHeader\nclass: QDPData\nclass: QDP\n\nastropy/io/ascii/core.py\nvariable: FORMAT_CLASSES\nvariable: FAST_CLASSES\nfunction: _check_multidim_table\nclass: CsvWriter\nclass: MaskedConstant\nvariable: masked\nclass: InconsistentTableError\nclass: OptionalTableImportError\nclass: ParameterError\nclass: FastOptionsError\nclass: NoType\nclass: StrType\nclass: NumType\nclass: FloatType\nclass: BoolType\nclass: IntType\nclass: AllType\nclass: Column\nclass: BaseInputter\nclass: BaseSplitter\nclass: DefaultSplitter\nfunction: _replace_tab_with_space\nfunction: _get_line_index\nclass: BaseHeader\nclass: BaseData\nfunction: convert_numpy\nclass: BaseOutputter\nfunction: _deduplicate_names\nclass: TableOutputter\nclass: MetaBaseReader\nfunction: _is_number\nfunction: _apply_include_exclude_names\nclass: BaseReader\nclass:"}], "related_loc_traj": [{"response": "```\nastropy/io/ascii/qdp.py\nfunction: _line_type\nfunction: _get_type_from_list_of_lines\nfunction: _get_lines_from_file\nfunction: _interpret_err_lines\nfunction: _get_tables_from_qdp_file\nfunction: _understand_err_col\nfunction: _read_table_qdp\nfunction: _write_table_qdp\nclass: QDPSplitter\nclass: QDPHeader\nclass: QDPData\nclass: QDP\n\nastropy/io/ascii/core.py\nvariable: FORMAT_CLASSES\nvariable: FAST_CLASSES\nfunction: _check_multidim_table\nclass: CsvWriter\nclass: MaskedConstant\nvariable: masked\nclass: InconsistentTableError\nclass: OptionalTableImportError\nclass: ParameterError\nclass: FastOptionsError\nclass: NoType\nclass: StrType\nclass: NumType\nclass: FloatType\nclass: BoolType\nclass: IntType\nclass: AllType\nclass: Column\nclass: BaseInputter\nclass: BaseSplitter\nclass: DefaultSplitter\nfunction: _replace_tab_with_space\nfunction: _get_line_index\nclass: BaseHeader\nclass: BaseData\nfunction: convert_numpy\nclass: BaseOutputter\nfunction: _deduplicate_names\nclass: TableOutputter\nclass: MetaBaseReader\nfunction: _is_number\nfunction: _apply_include_exclude_names\nclass: BaseReader\nclass:", "usage": {"completion_tokens": 300, "prompt_tokens": 3488}, "prompt": "\nPlease look through the following GitHub Problem Description and the Skeleton of Relevant Files.\nIdentify all locations that need inspection or editing to fix the problem, including directly related areas as well as any potentially related global variables, functions, and classes.\nFor each location you provide, either give the name of the class, the name of a method in a class, the name of a function, or the name of a global variable.\n\n### GitHub Problem Description ###\nascii.qdp Table format assumes QDP commands are upper case\n### Description\n\nascii.qdp assumes that commands in a QDP file are upper case, for example, for errors they must be \"READ SERR 1 2\" whereas QDP itself is not case sensitive and case use \"read serr 1 2\". \r\n\r\nAs many QDP files are created by hand, the expectation that all commands be all-caps should be removed.\n\n### Expected behavior\n\nThe following qdp file should read into a `Table` with errors, rather than crashing.\r\n```\r\nread serr 1 2 \r\n1 0.5 1 0.5\r\n```\n\n### How to Reproduce\n\nCreate a QDP file:\r\n```\r\n> cat > test.qdp\r\nread serr 1 2 \r\n1 0.5 1 0.5\r\n<EOF>\r\n\r\n > python\r\nPython 3.10.9 (main, Dec  7 2022, 02:03:23) [Clang 13.0.0 (clang-1300.0.29.30)] on darwin\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> from astropy.table import Table\r\n>>> Table.read('test.qdp',format='ascii.qdp')\r\nWARNING: table_id not specified. Reading the first available table [astropy.io.ascii.qdp]\r\nTraceback (most recent call last):\r\n...\r\n    raise ValueError(f'Unrecognized QDP line: {line}')\r\nValueError: Unrecognized QDP line: read serr 1 2\r\n```\r\n\r\nRunning \"qdp test.qdp\" works just fine.\r\n\n\n### Versions\n\nPython 3.10.9 (main, Dec  7 2022, 02:03:23) [Clang 13.0.0 (clang-1300.0.29.30)]\r\nastropy 5.1\r\nNumpy 1.24.1\r\npyerfa 2.0.0.1\r\nScipy 1.10.0\r\nMatplotlib 3.6.3\r\n\n\n\n### Skeleton of Relevant Files ###\n\n### File: astropy/io/ascii/qdp.py ###\n```python\n# Licensed under a 3-clause BSD style license - see LICENSE.rst\n\n\ndef _line_type(line, delimiter=None):\n...\n\n\ndef _get_type_from_list_of_lines(lines, delimiter=None):\n...\n\n\ndef _get_lines_from_file(qdp_file):\n...\n\n\ndef _interpret_err_lines(err_specs, ncols, names=None):\n...\n\n\ndef _get_tables_from_qdp_file(qdp_file, input_colnames=None, delimiter=None):\n...\n\n\ndef _understand_err_col(colnames):\n...\n\n\ndef _read_table_qdp(qdp_file, names=None, table_id=None, delimiter=None):\n...\n\n\ndef _write_table_qdp(table, filename=None, err_specs=None):\n...\n\n\nclass QDPSplitter(core.DefaultSplitter):\n\n    delimiter = \" \"\n\n\nclass QDPHeader(basic.CommentedHeaderHeader):\n\n    splitter_class = QDPSplitter\n    comment = \"!\"\n    write_comment = \"!\"\n\n\nclass QDPData(basic.BasicData):\n\n    splitter_class = QDPSplitter\n    fill_values = [(core.masked, \"NO\")]\n    comment = \"!\"\n    write_comment = None\n\n\nclass QDP(basic.Basic):\n\n    _format_name = \"qdp\"\n    _io_registry_can_write = True\n    _io_registry_suffix = \".qdp\"\n    _description = \"Quick and Dandy Plotter\"\n\n    header_class = QDPHeader\n    data_class = QDPData\n\n    def __init__(self, table_id=None, names=None, err_specs=None, sep=None):\n...\n\n    def read(self, table):\n...\n\n    def write(self, table):\n...\n\n```\n\n### File: astropy/io/ascii/core.py ###\n```python\n# Licensed under a 3-clause BSD style license - see LICENSE.rst\n\n# Global dictionary mapping format arg to the corresponding Reader class\nFORMAT_CLASSES = {}\n\n# Similar dictionary for fast readers\nFAST_CLASSES = {}\n\n\ndef _check_multidim_table(table, max_ndim):\n...\n\n\nclass CsvWriter:\n\n    # Random 16-character string that gets injected instead of any\n    # empty fields and is then replaced post-write with doubled-quotechar.\n    # Created with:\n    # ''.join(random.choice(string.printable[:90]) for _ in range(16))\n    replace_sentinel = \"2b=48Av%0-V3p>bX\"\n\n    def __init__(self, csvfile=None, **kwargs):\n...\n\n    def writerow(self, values):\n...\n\n    def writerows(self, values_list):\n...\n\n    def _writerow(self, writerow_func, values, has_empty):\n...\n\n\nclass MaskedConstant(numpy.ma.core.MaskedConstant):\n\n    def __hash__(self):\n...\n\n    def __copy__(self):\n...\n\n    def __deepcopy__(self, memo):\n...\n\n\nmasked = MaskedConstant()\n\n\nclass InconsistentTableError(ValueError):\n    pass\n\n\nclass OptionalTableImportError(ImportError):\n    pass\n\n\nclass ParameterError(NotImplementedError):\n    pass\n\n\nclass FastOptionsError(NotImplementedError):\n    pass\n\n\nclass NoType:\n    pass\n\n\nclass StrType(NoType):\n    pass\n\n\nclass NumType(NoType):\n    pass\n\n\nclass FloatType(NumType):\n    pass\n\n\nclass BoolType(NoType):\n    pass\n\n\nclass IntType(NumType):\n    pass\n\n\nclass AllType(StrType, FloatType, IntType):\n    pass\n\n\nclass Column:\n\n    def __init__(self, name):\n...\n\n\nclass BaseInputter:\n\n    encoding = None\n\n    def get_lines(self, table, newline=None):\n...\n\n    def process_lines(self, lines):\n...\n\n\nclass BaseSplitter:\n\n    delimiter = None\n\n    def process_line(self, line):\n...\n\n    def process_val(self, val):\n...\n\n    def __call__(self, lines):\n...\n\n    def join(self, vals):\n...\n\n\nclass DefaultSplitter(BaseSplitter):\n\n    delimiter = \" \"\n    quotechar = '\"'\n    doublequote = True\n    escapechar = None\n    quoting = csv.QUOTE_MINIMAL\n    skipinitialspace = True\n    csv_writer = None\n    csv_writer_out = StringIO()\n\n    def process_line(self, line):\n...\n\n    def process_val(self, val):\n...\n\n    def __call__(self, lines):\n...\n\n    def join(self, vals):\n...\n\n\ndef _replace_tab_with_space(line, escapechar, quotechar):\n...\n\n\ndef _get_line_index(line_or_func, lines):\n...\n\n\nclass BaseHeader:\n\n    auto_format = \"col{}\"\n    start_line = None\n    comment = None\n    splitter_class = DefaultSplitter\n    names = None\n    write_comment = False\n    write_spacer_lines = [\"ASCII_TABLE_WRITE_SPACER_LINE\"]\n\n    def __init__(self):\n...\n\n    def _set_cols_from_names(self):\n...\n\n    def update_meta(self, lines, meta):\n...\n\n    def get_cols(self, lines):\n...\n\n    def process_lines(self, lines):\n...\n\n    def write_comments(self, lines, meta):\n...\n\n    def write(self, lines):\n...\n\n    @property\n    def colnames(self):\n...\n\n    def remove_columns(self, names):\n...\n\n    def rename_column(self, name, new_name):\n...\n\n    def get_type_map_key(self, col):\n...\n\n    def get_col_type(self, col):\n...\n\n    def check_column_names(self, names, strict_names, guessing):\n...\n\n\nclass BaseData:\n\n    start_line = None\n    end_line = None\n    comment = None\n    splitter_class = DefaultSplitter\n    write_spacer_lines = [\"ASCII_TABLE_WRITE_SPACER_LINE\"]\n    fill_include_names = None\n    fill_exclude_names = None\n    fill_values = [(masked, \"\")]\n    formats = {}\n\n    def __init__(self):\n...\n\n    def process_lines(self, lines):\n...\n\n    def get_data_lines(self, lines):\n...\n\n    def get_str_vals(self):\n...\n\n    def masks(self, cols):\n...\n\n    def _set_fill_values(self, cols):\n...\n\n    def _set_masks(self, cols):\n...\n\n    def _replace_vals(self, cols):\n...\n\n    def str_vals(self):\n...\n\n    def write(self, lines):\n...\n\n    def _set_col_formats(self):\n...\n\n\ndef convert_numpy(numpy_type):\n...\n\n\nclass BaseOutputter:\n\n    # User-defined converters which gets set in ascii.ui if a `converter` kwarg\n    # is supplied.\n    converters = {}\n\n    # Derived classes must define default_converters and __call__\n\n    @staticmethod\n    def _validate_and_copy(col, converters):\n...\n\n    def _convert_vals(self, cols):\n...\n\n\ndef _deduplicate_names(names):\n...\n\n\nclass TableOutputter(BaseOutputter):\n\n    default_converters = [convert_numpy(int), convert_numpy(float), convert_numpy(str)]\n\n    def __call__(self, cols, meta):\n...\n\n\nclass MetaBaseReader(type):\n    def __init__(cls, name, bases, dct):\n...\n\n\ndef _is_number(x):\n...\n\n\ndef _apply_include_exclude_names(table, names, include_names, exclude_names):\n...\n\n\nclass BaseReader(metaclass=MetaBaseReader):\n\n    names = None\n    include_names = None\n    exclude_names = None\n    strict_names = False\n    guessing = False\n    encoding = None\n\n    header_class = BaseHeader\n    data_class = BaseData\n    inputter_class = BaseInputter\n    outputter_class = TableOutputter\n\n    # Max column dimension that writer supports for this format. Exceptions\n    # include ECSV (no limit) and HTML (max_ndim=2).\n    max_ndim = 1\n\n    def __init__(self):\n...\n\n    def _check_multidim_table(self, table):\n...\n\n    def read(self, table):\n...\n\n    def inconsistent_handler(self, str_vals, ncols):\n...\n\n    @property\n    def comment_lines(self):\n...\n\n    def update_table_data(self, table):\n...\n\n    def write_header(self, lines, meta):\n...\n\n    def write(self, table):\n...\n\n\nclass ContinuationLinesInputter(BaseInputter):\n\n    continuation_char = \"\\\\\"\n    replace_char = \" \"\n    # If no_continue is not None then lines matching this regex are not subject\n    # to line continuation.  The initial use case here is Daophot.  In this\n    # case the continuation character is just replaced with replace_char.\n    no_continue = None\n\n    def process_lines(self, lines):\n...\n\n\nclass WhitespaceSplitter(DefaultSplitter):\n    def process_line(self, line):\n...\n\n\nextra_reader_pars = (\n    \"Reader\",\n    \"Inputter\",\n    \"Outputter\",\n    \"delimiter\",\n    \"comment\",\n    \"quotechar\",\n    \"header_start\",\n    \"data_start\",\n    \"data_end\",\n    \"converters\",\n    \"encoding\",\n    \"data_Splitter\",\n    \"header_Splitter\",\n    \"names\",\n    \"include_names\",\n    \"exclude_names\",\n    \"strict_names\",\n    \"fill_values\",\n    \"fill_include_names\",\n    \"fill_exclude_names\",\n)\n\n\ndef _get_reader(Reader, Inputter=None, Outputter=None, **kwargs):\n...\n\n\nextra_writer_pars = (\n    \"delimiter\",\n    \"comment\",\n    \"quotechar\",\n    \"formats\",\n    \"strip_whitespace\",\n    \"names\",\n    \"include_names\",\n    \"exclude_names\",\n    \"fill_values\",\n    \"fill_include_names\",\n    \"fill_exclude_names\",\n)\n\n\ndef _get_writer(Writer, fast_writer, **kwargs):\n...\n\n```\n\n### File: astropy/io/ascii/docs.py ###\n```python\nREAD_DOCSTRING = \"\"\"\n    Read the input ``table`` and return the table.  Most of\n    the default behavior for various parameters is determined by the Reader\n    class.\n\n    See also:\n\n    - https://docs.astropy.org/en/stable/io/ascii/\n    - https://docs.astropy.org/en/stable/io/ascii/read.html\n\n...\n\n    encoding : str\n        Allow to specify encoding to read the file (default= ``None``).\n\n    Returns\n    -------\n    dat : `~astropy.table.Table` or <generator>\n        Output table\n\n    \"\"\"\n\n# Specify allowed types for core write() keyword arguments.  Each entry\n# corresponds to the name of an argument and either a type (e.g. int) or a\n# list of types.  These get used in io.ascii.ui._validate_read_write_kwargs().\n# -  The commented-out kwargs are too flexible for a useful check\n# -  'list-list' is a special case for an iterable that is not a string.\nREAD_KWARG_TYPES = {\n    # 'table'\n    \"guess\": bool,\n    # 'format'\n    # 'Reader'\n    # 'Inputter'\n    # 'Outputter'\n    \"delimiter\": str,\n    \"comment\": str,\n    \"quotechar\": str,\n    \"header_start\": int,\n    \"data_start\": (int, str),  # CDS allows 'guess'\n    \"data_end\": int,\n    \"converters\": dict,\n    # 'data_Splitter'\n    # 'header_Splitter'\n    \"names\": \"list-like\",\n    \"include_names\": \"list-like\",\n    \"exclude_names\": \"list-like\",\n    \"fill_values\": \"list-like\",\n    \"fill_include_names\": \"list-like\",\n    \"fill_exclude_names\": \"list-like\",\n    \"fast_reader\": (bool, str, dict),\n    \"encoding\": str,\n}\n\n\nWRITE_DOCSTRING = \"\"\"\n    Write the input ``table`` to ``filename``.  Most of the default behavior\n    for various parameters is determined by the Writer class.\n\n    See also:\n\n    - https://docs.astropy.org/en/stable/io/ascii/\n    - https://docs.astropy.org/en/stable/io/ascii/write.html\n\n    Parameters\n...\n    fast_writer : bool, str\n        Whether to use the fast Cython writer.  Can be `True` (use fast writer\n        if available), `False` (do not use fast writer), or ``'force'`` (use\n        fast writer and fail if not available, mostly for testing).\n    overwrite : bool\n        If ``overwrite=False`` (default) and the file exists, then an OSError\n        is raised. This parameter is ignored when the ``output`` arg is not a\n        string (e.g., a file object).\n\n    \"\"\"\n# Specify allowed types for core write() keyword arguments.  Each entry\n# corresponds to the name of an argument and either a type (e.g. int) or a\n# list of types.  These get used in io.ascii.ui._validate_read_write_kwargs().\n# -  The commented-out kwargs are too flexible for a useful check\n# -  'list-list' is a special case for an iterable that is not a string.\nWRITE_KWARG_TYPES = {\n    # 'table'\n    # 'output'\n    \"format\": str,\n    \"delimiter\": str,\n    \"comment\": (str, bool),\n    \"quotechar\": str,\n    \"header_start\": int,\n    \"formats\": dict,\n    \"strip_whitespace\": (bool),\n    \"names\": \"list-like\",\n    \"include_names\": \"list-like\",\n    \"exclude_names\": \"list-like\",\n    \"fast_writer\": (bool, str),\n    \"overwrite\": (bool),\n}\n\n```\n\n\n###\n\nPlease provide the complete set of locations as either a class name, a function name, or a variable name.\nNote that if you include a class, you do not need to list its specific methods.\nYou can include either the entire class or don't include the class name and instead include specific methods in the class.\n### Examples:\n```\nfull_path1/file1.py\nfunction: my_function_1\nclass: MyClass1\nfunction: MyClass2.my_method\n\nfull_path2/file2.py\nvariable: my_var\nfunction: MyClass3.my_method\n\nfull_path3/file3.py\nfunction: my_function_2\nfunction: my_function_3\nfunction: MyClass4.my_method_1\nclass: MyClass5\n```\n\nReturn just the locations wrapped with ```.\n"}], "found_edit_locs": {"astropy/io/ascii/qdp.py": ["\nline: 62\nline: 63\nfunction: _line_type\nline: 300"]}, "additional_artifact_loc_edit_location": [{"raw_output_loc": ["```\nastropy/io/ascii/qdp.py\nline: 62\nline: 63\nfunction: _line_type\nline: 300\n```", "```\nastropy/io/ascii/qdp.py\nline: 63\nline: 70\nline: 300\n```", "```\nastropy/io/ascii/qdp.py\nline: 63\nline: 300\n```", "```\nastropy/io/ascii/qdp.py\nline: 63\nline: 70\nline: 300\n```"]}], "edit_loc_traj": {"prompt": "\nPlease review the following GitHub problem description and relevant files, and provide a set of locations that need to be edited to fix the issue.\nThe locations can be specified as class names, function or method names, or exact line numbers that require modification.\n\n### GitHub Problem Description ###\nascii.qdp Table format assumes QDP commands are upper case\n### Description\n\nascii.qdp assumes that commands in a QDP file are upper case, for example, for errors they must be \"READ SERR 1 2\" whereas QDP itself is not case sensitive and case use \"read serr 1 2\". \r\n\r\nAs many QDP files are created by hand, the expectation that all commands be all-caps should be removed.\n\n### Expected behavior\n\nThe following qdp file should read into a `Table` with errors, rather than crashing.\r\n```\r\nread serr 1 2 \r\n1 0.5 1 0.5\r\n```\n\n### How to Reproduce\n\nCreate a QDP file:\r\n```\r\n> cat > test.qdp\r\nread serr 1 2 \r\n1 0.5 1 0.5\r\n<EOF>\r\n\r\n > python\r\nPython 3.10.9 (main, Dec  7 2022, 02:03:23) [Clang 13.0.0 (clang-1300.0.29.30)] on darwin\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> from astropy.table import Table\r\n>>> Table.read('test.qdp',format='ascii.qdp')\r\nWARNING: table_id not specified. Reading the first available table [astropy.io.ascii.qdp]\r\nTraceback (most recent call last):\r\n...\r\n    raise ValueError(f'Unrecognized QDP line: {line}')\r\nValueError: Unrecognized QDP line: read serr 1 2\r\n```\r\n\r\nRunning \"qdp test.qdp\" works just fine.\r\n\n\n### Versions\n\nPython 3.10.9 (main, Dec  7 2022, 02:03:23) [Clang 13.0.0 (clang-1300.0.29.30)]\r\nastropy 5.1\r\nNumpy 1.24.1\r\npyerfa 2.0.0.1\r\nScipy 1.10.0\r\nMatplotlib 3.6.3\r\n\n\n\n###\n### astropy/io/ascii/qdp.py\n...\n10|from collections.abc import Iterable\n11|\n12|import numpy as np\n13|\n14|from astropy.table import Table\n15|from astropy.utils.exceptions import AstropyUserWarning\n16|\n17|from . import basic, core\n18|\n19|\n20|def _line_type(line, delimiter=None):\n21|    \"\"\"Interpret a QDP file line.\n22|\n23|    Parameters\n24|    ----------\n25|    line : str\n26|        a single line of the file\n27|\n28|    Returns\n29|    -------\n30|    type : str\n31|        Line type: \"comment\", \"command\", or \"data\"\n32|\n33|    Examples\n34|    --------\n35|    >>> _line_type(\"READ SERR 3\")\n36|    'command'\n37|    >>> _line_type(\" \\\\n    !some gibberish\")\n38|    'comment'\n39|    >>> _line_type(\"   \")\n40|    'comment'\n41|    >>> _line_type(\" 21345.45\")\n42|    'data,1'\n43|    >>> _line_type(\" 21345.45 1.53e-3 1e-3 .04 NO nan\")\n44|    'data,6'\n45|    >>> _line_type(\" 21345.45,1.53e-3,1e-3,.04,NO,nan\", delimiter=',')\n46|    'data,6'\n47|    >>> _line_type(\" 21345.45 ! a comment to disturb\")\n48|    'data,1'\n49|    >>> _line_type(\"NO NO NO NO NO\")\n50|    'new'\n51|    >>> _line_type(\"NO,NO,NO,NO,NO\", delimiter=',')\n52|    'new'\n53|    >>> _line_type(\"N O N NOON OON O\")\n54|    Traceback (most recent call last):\n55|        ...\n56|    ValueError: Unrecognized QDP line...\n57|    >>> _line_type(\" some non-comment gibberish\")\n58|    Traceback (most recent call last):\n59|        ...\n60|    ValueError: Unrecognized QDP line...\n61|    \"\"\"\n62|    _decimal_re = r\"[+-]?(\\d+(\\.\\d*)?|\\.\\d+)([eE][+-]?\\d+)?\"\n63|    _command_re = r\"READ [TS]ERR(\\s+[0-9]+)+\"\n64|\n65|    sep = delimiter\n66|    if delimiter is None:\n67|        sep = r\"\\s+\"\n68|    _new_re = rf\"NO({sep}NO)+\"\n69|    _data_re = rf\"({_decimal_re}|NO|[-+]?nan)({sep}({_decimal_re}|NO|[-+]?nan))*)\"\n70|    _type_re = rf\"^\\s*((?P<command>{_command_re})|(?P<new>{_new_re})|(?P<data>{_data_re})?\\s*(\\!(?P<comment>.*))?\\s*$\"\n71|    _line_type_re = re.compile(_type_re)\n72|    line = line.strip()\n73|    if not line:\n74|        return \"comment\"\n75|    match = _line_type_re.match(line)\n76|\n77|    if match is None:\n78|        raise ValueError(f\"Unrecognized QDP line: {line}\")\n79|    for type_, val in match.groupdict().items():\n80|        if val is None:\n81|            continue\n82|        if type_ == \"data\":\n83|            return f\"data,{len(val.split(sep=delimiter))}\"\n84|        else:\n85|            return type_\n86|\n87|\n88|def _get_type_from_list_of_lines(lines, delimiter=None):\n89|    \"\"\"Read through the list of QDP file lines and label each line by type.\n90|\n91|    Parameters\n92|    ----------\n93|    lines : list\n94|        List containing one file line in each entry\n95|\n96|    Returns\n97|    -------\n98|    contents : list\n99|        List containing the type for each line (see `line_type_and_data`)\n100|    ncol : int\n101|        The number of columns in the data lines. Must be the same throughout\n102|        the file\n103|\n104|    Examples\n105|    --------\n106|    >>> line0 = \"! A comment\"\n107|    >>> line1 = \"543 12 456.0\"\n108|    >>> lines = [line0, line1]\n109|    >>> types, ncol = _get_type_from_list_of_lines(lines)\n110|    >>> types[0]\n111|    'comment'\n112|    >>> types[1]\n113|    'data,3'\n114|    >>> ncol\n115|    3\n116|    >>> lines.append(\"23\")\n117|    >>> _get_type_from_list_of_lines(lines)\n118|    Traceback (most recent call last):\n119|        ...\n120|    ValueError: Inconsistent number of columns\n121|    \"\"\"\n122|    types = [_line_type(line, delimiter=delimiter) for line in lines]\n123|    current_ncol = None\n124|    for type_ in types:\n125|        if type_.startswith(\"data,\"):\n126|            ncol = int(type_[5:])\n127|            if current_ncol is None:\n128|                current_ncol = ncol\n129|            elif ncol != current_ncol:\n130|                raise ValueError(\"Inconsistent number of columns\")\n131|\n132|    return types, current_ncol\n133|\n134|\n135|def _get_lines_from_file(qdp_file):\n136|    if \"\\n\" in qdp_file:\n137|        lines = qdp_file.split(\"\\n\")\n138|    elif isinstance(qdp_file, str):\n139|        with open(qdp_file) as fobj:\n140|            lines = [line.strip() for line in fobj.readlines()]\n141|    elif isinstance(qdp_file, Iterable):\n142|        lines = qdp_file\n143|    else:\n144|        raise ValueError(\"invalid value of qdb_file\")\n145|\n146|    return lines\n147|\n148|\n149|def _interpret_err_lines(err_specs, ncols, names=None):\n150|    \"\"\"Give list of column names from the READ SERR and TERR commands.\n151|\n152|    Parameters\n153|    ----------\n154|    err_specs : dict\n155|        ``{'serr': [n0, n1, ...], 'terr': [n2, n3, ...]}``\n156|        Error specifications for symmetric and two-sided errors\n157|    ncols : int\n158|        Number of data columns\n159|\n160|    Other Parameters\n161|    ----------------\n162|    names : list of str\n163|        Name of data columns (defaults to ['col1', 'col2', ...]), _not_\n164|        including error columns.\n165|\n166|    Returns\n167|    -------\n168|    colnames : list\n169|        List containing the column names. Error columns will have the name\n170|        of the main column plus ``_err`` for symmetric errors, and ``_perr``\n171|        and ``_nerr`` for positive and negative errors respectively\n172|\n173|    Examples\n174|    --------\n175|    >>> col_in = ['MJD', 'Rate']\n176|    >>> cols = _interpret_err_lines(None, 2, names=col_in)\n177|    >>> cols[0]\n178|    'MJD'\n179|    >>> err_specs = {'terr': [1], 'serr': [2]}\n180|    >>> ncols = 5\n181|    >>> cols = _interpret_err_lines(err_specs, ncols, names=col_in)\n182|    >>> cols[0]\n183|    'MJD'\n184|    >>> cols[2]\n185|    'MJD_nerr'\n186|    >>> cols[4]\n187|    'Rate_err'\n188|    >>> _interpret_err_lines(err_specs, 6, names=col_in)\n189|    Traceback (most recent call last):\n190|        ...\n191|    ValueError: Inconsistent number of input colnames\n192|    \"\"\"\n193|    colnames = [\"\" for i in range(ncols)]\n194|    if err_specs is None:\n195|        serr_cols = terr_cols = []\n196|\n197|    else:\n198|        # I don't want to empty the original one when using `pop` below\n199|        err_specs = copy.deepcopy(err_specs)\n200|\n201|        serr_cols = err_specs.pop(\"serr\", [])\n202|        terr_cols = err_specs.pop(\"terr\", [])\n203|\n204|    if names is not None:\n205|        all_error_cols = len(serr_cols) + len(terr_cols) * 2\n206|        if all_error_cols + len(names) != ncols:\n207|            raise ValueError(\"Inconsistent number of input colnames\")\n208|\n209|    shift = 0\n210|    for i in range(ncols):\n211|        col_num = i + 1 - shift\n212|        if colnames[i] != \"\":\n213|            continue\n214|\n215|        colname_root = f\"col{col_num}\"\n216|\n217|        if names is not None:\n218|            colname_root = names[col_num - 1]\n219|\n220|        colnames[i] = f\"{colname_root}\"\n221|        if col_num in serr_cols:\n222|            colnames[i + 1] = f\"{colname_root}_err\"\n223|            shift += 1\n224|            continue\n225|\n226|        if col_num in terr_cols:\n227|            colnames[i + 1] = f\"{colname_root}_perr\"\n228|            colnames[i + 2] = f\"{colname_root}_nerr\"\n229|            shift += 2\n230|            continue\n231|\n232|    assert not np.any([c == \"\" for c in colnames])\n233|\n234|    return colnames\n235|\n236|\n237|def _get_tables_from_qdp_file(qdp_file, input_colnames=None, delimiter=None):\n238|    \"\"\"Get all tables from a QDP file.\n239|\n240|    Parameters\n241|    ----------\n242|    qdp_file : str\n243|        Input QDP file name\n244|\n245|    Other Parameters\n246|    ----------------\n247|    input_colnames : list of str\n248|        Name of data columns (defaults to ['col1', 'col2', ...]), _not_\n249|        including error columns.\n250|    delimiter : str\n251|        Delimiter for the values in the table.\n252|\n253|    Returns\n254|    -------\n255|    list of `~astropy.table.Table`\n256|        List containing all the tables present inside the QDP file\n257|    \"\"\"\n258|    lines = _get_lines_from_file(qdp_file)\n259|    contents, ncol = _get_type_from_list_of_lines(lines, delimiter=delimiter)\n260|\n261|    table_list = []\n262|    err_specs = {}\n263|    colnames = None\n264|\n265|    comment_text = \"\"\n266|    initial_comments = \"\"\n267|    command_lines = \"\"\n268|    current_rows = None\n269|\n270|    for line, datatype in zip(lines, contents):\n271|        line = line.strip().lstrip(\"!\")\n272|        # Is this a comment?\n273|        if datatype == \"comment\":\n274|            comment_text += line + \"\\n\"\n275|            continue\n276|\n277|        if datatype == \"command\":\n278|            # The first time I find commands, I save whatever comments into\n279|            # The initial comments.\n280|            if command_lines == \"\":\n281|                initial_comments = comment_text\n282|                comment_text = \"\"\n283|\n284|            if err_specs != {}:\n285|                warnings.warn(\n286|                    \"This file contains multiple command blocks. Please verify\",\n287|                    AstropyUserWarning,\n288|                )\n289|            command_lines += line + \"\\n\"\n290|            continue\n291|\n292|        if datatype.startswith(\"data\"):\n293|            # The first time I find data, I define err_specs\n294|            if err_specs == {} and command_lines != \"\":\n295|                for cline in command_lines.strip().split(\"\\n\"):\n296|                    command = cline.strip().split()\n297|                    # This should never happen, but just in case.\n298|                    if len(command) < 3:\n299|                        continue\n300|                    err_specs[command[1].lower()] = [int(c) for c in command[2:]]\n301|            if colnames is None:\n302|                colnames = _interpret_err_lines(err_specs, ncol, names=input_colnames)\n303|\n304|            if current_rows is None:\n305|                current_rows = []\n306|\n307|            values = []\n308|            for v in line.split(delimiter):\n309|                if v == \"NO\":\n310|                    values.append(np.ma.masked)\n311|                else:\n312|                    # Understand if number is int or float\n313|                    try:\n314|                        values.append(int(v))\n315|                    except ValueError:\n316|                        values.append(float(v))\n317|            current_rows.append(values)\n318|            continue\n319|\n320|        if datatype == \"new\":\n321|            # Save table to table_list and reset\n322|            if current_rows is not None:\n323|                new_table = Table(names=colnames, rows=current_rows)\n324|                new_table.meta[\"initial_comments\"] = initial_comments.strip().split(\n325|                    \"\\n\"\n326|                )\n327|                new_table.meta[\"comments\"] = comment_text.strip().split(\"\\n\")\n328|                # Reset comments\n329|                comment_text = \"\"\n330|                table_list.append(new_table)\n331|                current_rows = None\n332|            continue\n333|\n334|    # At the very end, if there is still a table being written, let's save\n335|    # it to the table_list\n336|    if current_rows is not None:\n337|        new_table = Table(names=colnames, rows=current_rows)\n338|        new_table.meta[\"initial_comments\"] = initial_comments.strip().split(\"\\n\")\n339|        new_table.meta[\"comments\"] = comment_text.strip().split(\"\\n\")\n340|        table_list.append(new_table)\n341|\n342|    return table_list\n343|\n344|\n345|def _understand_err_col(colnames):\n346|    \"\"\"Get which column names are error columns.\n347|\n348|    Examples\n349|    --------\n350|    >>> colnames = ['a', 'a_err', 'b', 'b_perr', 'b_nerr']\n351|    >>> serr, terr = _understand_err_col(colnames)\n352|    >>> np.allclose(serr, [1])\n353|    True\n354|    >>> np.allclose(terr, [2])\n355|    True\n356|    >>> serr, terr = _understand_err_col(['a', 'a_nerr'])\n357|    Traceback (most recent call last):\n358|    ...\n359|    ValueError: Missing positive error...\n360|    >>> serr, terr = _understand_err_col(['a', 'a_perr'])\n361|    Traceback (most recent call last):\n362|    ...\n363|    ValueError: Missing negative error...\n364|    \"\"\"\n365|    shift = 0\n366|    serr = []\n367|    terr = []\n368|\n369|    for i, col in enumerate(colnames):\n370|        if col.endswith(\"_err\"):\n371|            # The previous column, but they're numbered from 1!\n372|            # Plus, take shift into account\n373|            serr.append(i - shift)\n374|            shift += 1\n375|        elif col.endswith(\"_perr\"):\n376|            terr.append(i - shift)\n377|            if len(colnames) == i + 1 or not colnames[i + 1].endswith(\"_nerr\"):\n378|                raise ValueError(\"Missing negative error\")\n379|            shift += 2\n380|        elif col.endswith(\"_nerr\") and not colnames[i - 1].endswith(\"_perr\"):\n381|            raise ValueError(\"Missing positive error\")\n382|    return serr, terr\n383|\n384|\n385|def _read_table_qdp(qdp_file, names=None, table_id=None, delimiter=None):\n386|    \"\"\"Read a table from a QDP file.\n387|\n388|    Parameters\n389|    ----------\n390|    qdp_file : str\n391|        Input QDP file name\n392|\n393|    Other Parameters\n394|    ----------------\n395|    names : list of str\n396|        Name of data columns (defaults to ['col1', 'col2', ...]), _not_\n397|        including error columns.\n398|\n399|    table_id : int, default 0\n400|        Number of the table to be read from the QDP file. This is useful\n401|        when multiple tables present in the file. By default, the first is read.\n402|\n403|    delimiter : str\n404|        Any delimiter accepted by the `sep` argument of str.split()\n405|\n406|    Returns\n407|    -------\n408|    tables : list of `~astropy.table.Table`\n409|        List containing all the tables present inside the QDP file\n410|    \"\"\"\n411|    if table_id is None:\n412|        warnings.warn(\n413|            \"table_id not specified. Reading the first available table\",\n414|            AstropyUserWarning,\n415|        )\n416|        table_id = 0\n417|\n418|    tables = _get_tables_from_qdp_file(\n419|        qdp_file, input_colnames=names, delimiter=delimiter\n420|    )\n421|\n422|    return tables[table_id]\n423|\n424|\n425|def _write_table_qdp(table, filename=None, err_specs=None):\n426|    \"\"\"Write a table to a QDP file.\n427|\n428|    Parameters\n429|    ----------\n430|    table : :class:`~astropy.table.Table`\n431|        Input table to be written\n432|    filename : str\n433|        Output QDP file name\n434|\n435|    Other Parameters\n436|    ----------------\n437|    err_specs : dict\n438|        Dictionary of the format {'serr': [1], 'terr': [2, 3]}, specifying\n439|        which columns have symmetric and two-sided errors (see QDP format\n440|        specification)\n441|    \"\"\"\n442|    import io\n443|\n444|    fobj = io.StringIO()\n445|\n446|    if \"initial_comments\" in table.meta and table.meta[\"initial_comments\"] != []:\n447|        for line in table.meta[\"initial_comments\"]:\n448|            line = line.strip()\n449|            if not line.startswith(\"!\"):\n450|                line = \"!\" + line\n451|            print(line, file=fobj)\n452|\n453|    if err_specs is None:\n454|        serr_cols, terr_cols = _understand_err_col(table.colnames)\n455|    else:\n456|        serr_cols = err_specs.pop(\"serr\", [])\n457|        terr_cols = err_specs.pop(\"terr\", [])\n458|    if serr_cols != []:\n459|        col_string = \" \".join([str(val) for val in serr_cols])\n460|        print(f\"READ SERR {col_string}\", file=fobj)\n461|    if terr_cols != []:\n462|        col_string = \" \".join([str(val) for val in terr_cols])\n463|        print(f\"READ TERR {col_string}\", file=fobj)\n464|\n465|    if \"comments\" in table.meta and table.meta[\"comments\"] != []:\n466|        for line in table.meta[\"comments\"]:\n467|            line = line.strip()\n468|            if not line.startswith(\"!\"):\n469|                line = \"!\" + line\n470|            print(line, file=fobj)\n471|\n472|    colnames = table.colnames\n473|    print(\"!\" + \" \".join(colnames), file=fobj)\n474|    for row in table:\n475|        values = []\n476|        for val in row:\n477|            if not np.ma.is_masked(val):\n478|                rep = str(val)\n479|            else:\n480|                rep = \"NO\"\n481|            values.append(rep)\n482|        print(\" \".join(values), file=fobj)\n483|\n484|    full_string = fobj.getvalue()\n485|    fobj.close()\n486|\n487|    if filename is not None:\n488|        with open(filename, \"w\") as fobj:\n489|            print(full_string, file=fobj)\n490|\n491|    return full_string.split(\"\\n\")\n492|\n493|\n494|class QDPSplitter(core.DefaultSplitter):\n495|    \"\"\"\n496|    Split on space for QDP tables.\n497|    \"\"\"\n498|\n499|    delimiter = \" \"\n500|\n501|\n502|class QDPHeader(basic.CommentedHeaderHeader):\n503|    \"\"\"\n504|    Header that uses the :class:`astropy.io.ascii.basic.QDPSplitter`.\n505|    \"\"\"\n506|\n507|    splitter_class = QDPSplitter\n508|    comment = \"!\"\n509|    write_comment = \"!\"\n510|\n511|\n512|class QDPData(basic.BasicData):\n513|    \"\"\"\n514|    Data that uses the :class:`astropy.io.ascii.basic.CsvSplitter`.\n515|    \"\"\"\n516|\n517|    splitter_class = QDPSplitter\n518|    fill_values = [(core.masked, \"NO\")]\n519|    comment = \"!\"\n520|    write_comment = None\n521|\n522|\n523|class QDP(basic.Basic):\n524|    \"\"\"Quick and Dandy Plot table.\n525|\n526|    Example::\n527|\n528|        ! Initial comment line 1\n529|        ! Initial comment line 2\n530|        READ TERR 1\n531|        READ SERR 3\n532|        ! Table 0 comment\n533|        !a a(pos) a(neg) b be c d\n534|        53000.5   0.25  -0.5   1  1.5  3.5 2\n535|        54000.5   1.25  -1.5   2  2.5  4.5 3\n536|        NO NO NO NO NO\n537|        ! Table 1 comment\n538|        !a a(pos) a(neg) b be c d\n539|        54000.5   2.25  -2.5   NO  3.5  5.5 5\n540|        55000.5   3.25  -3.5   4  4.5  6.5 nan\n541|\n542|    The input table above contains some initial comments, the error commands,\n543|    then two tables.\n544|    This file format can contain multiple tables, separated by a line full\n545|    of ``NO``s. Comments are exclamation marks, and missing values are single\n546|    ``NO`` entries. The delimiter is usually whitespace, more rarely a comma.\n547|    The QDP format differentiates between data and error columns. The table\n548|    above has commands::\n549|\n550|        READ TERR 1\n551|        READ SERR 3\n552|\n553|    which mean that after data column 1 there will be two error columns\n554|    containing its positive and engative error bars, then data column 2 without\n555|    error bars, then column 3, then a column with the symmetric error of column\n556|    3, then the remaining data columns.\n557|\n558|    As explained below, table headers are highly inconsistent. Possible\n559|    comments containing column names will be ignored and columns will be called\n560|    ``col1``, ``col2``, etc. unless the user specifies their names with the\n561|    ``names=`` keyword argument,\n562|    When passing column names, pass **only the names of the data columns, not\n563|    the error columns.**\n564|    Error information will be encoded in the names of the table columns.\n565|    (e.g. ``a_perr`` and ``a_nerr`` for the positive and negative error of\n566|    column ``a``, ``b_err`` the symmetric error of column ``b``.)\n567|\n568|    When writing tables to this format, users can pass an ``err_specs`` keyword\n569|    passing a dictionary ``{'serr': [3], 'terr': [1, 2]}``, meaning that data\n570|    columns 1 and two will have two additional columns each with their positive\n571|    and negative errors, and data column 3 will have an additional column with\n572|    a symmetric error (just like the ``READ SERR`` and ``READ TERR`` commands\n573|    above)\n574|\n575|    Headers are just comments, and tables distributed by various missions\n576|    can differ greatly in their use of conventions. For example, light curves\n577|    distributed by the Swift-Gehrels mission have an extra space in one header\n578|    entry that makes the number of labels inconsistent with the number of cols.\n579|    For this reason, we ignore the comments that might encode the column names\n580|    and leave the name specification to the user.\n581|\n582|    Example::\n583|\n584|        >               Extra space\n585|        >                   |\n586|        >                   v\n587|        >!     MJD       Err (pos)       Err(neg)        Rate            Error\n588|        >53000.123456   2.378e-05     -2.378472e-05     NO             0.212439\n589|\n590|    These readers and writer classes will strive to understand which of the\n591|    comments belong to all the tables, and which ones to each single table.\n592|    General comments will be stored in the ``initial_comments`` meta of each\n593|    table. The comments of each table will be stored in the ``comments`` meta.\n594|\n595|    Example::\n596|\n597|        t = Table.read(example_qdp, format='ascii.qdp', table_id=1, names=['a', 'b', 'c', 'd'])\n598|\n599|    reads the second table (``table_id=1``) in file ``example.qdp`` containing\n600|    the table above. There are four column names but seven data columns, why?\n601|    Because the ``READ SERR`` and ``READ TERR`` commands say that there are\n602|    three error columns.\n603|    ``t.meta['initial_comments']`` will contain the initial two comment lines\n604|    in the file, while ``t.meta['comments']`` will contain ``Table 1 comment``\n605|\n606|    The table can be written to another file, preserving the same information,\n607|    as::\n608|\n609|        t.write(test_file, err_specs={'terr': [1], 'serr': [3]})\n610|\n611|    Note how the ``terr`` and ``serr`` commands are passed to the writer.\n612|\n613|    \"\"\"\n614|\n615|    _format_name = \"qdp\"\n616|    _io_registry_can_write = True\n617|    _io_registry_suffix = \".qdp\"\n618|    _description = \"Quick and Dandy Plotter\"\n619|\n620|    header_class = QDPHeader\n621|    data_class = QDPData\n622|\n623|    def __init__(self, table_id=None, names=None, err_specs=None, sep=None):\n624|        super().__init__()\n625|        self.table_id = table_id\n626|        self.names = names\n627|        self.err_specs = err_specs\n628|        self.delimiter = sep\n629|\n630|    def read(self, table):\n631|        self.lines = self.inputter.get_lines(table, newline=\"\\n\")\n632|        return _read_table_qdp(\n633|            self.lines,\n634|            table_id=self.table_id,\n635|            names=self.names,\n636|            delimiter=self.delimiter,\n637|        )\n638|\n639|    def write(self, table):\n640|        self._check_multidim_table(table)\n641|        lines = _write_table_qdp(table, err_specs=self.err_specs)\n642|        return lines\n\n\n### astropy/io/ascii/core.py\n...\n26|import numpy\n27|\n28|from astropy.table import Table\n29|from astropy.utils.data import get_readable_fileobj\n30|from astropy.utils.exceptions import AstropyWarning\n31|\n32|from . import connect\n33|from .docs import READ_DOCSTRING, WRITE_DOCSTRING\n34|\n35|# Global dictionary mapping format arg to the corresponding Reader class\n36|FORMAT_CLASSES = {}\n37|\n38|# Similar dictionary for fast readers\n39|FAST_CLASSES = {}\n40|\n41|\n42|def _check_multidim_table(table, max_ndim):\n43|    \"\"\"Check that ``table`` has only columns with ndim <= ``max_ndim``.\n44|\n45|    Currently ECSV is the only built-in format that supports output of arbitrary\n46|    N-d columns, but HTML supports 2-d.\n47|    \"\"\"\n48|    # No limit?\n49|    if max_ndim is None:\n50|        return\n51|\n52|    # Check for N-d columns\n53|    nd_names = [col.info.name for col in table.itercols() if len(col.shape) > max_ndim]\n54|    if nd_names:\n55|        raise ValueError(\n56|            f\"column(s) with dimension > {max_ndim} \"\n57|            \"cannot be be written with this format, try using 'ecsv' \"\n58|            \"(Enhanced CSV) format\"\n59|        )\n60|\n61|\n62|class CsvWriter:\n63|    \"\"\"\n64|    Internal class to replace the csv writer ``writerow`` and ``writerows``\n65|    functions so that in the case of ``delimiter=' '`` and\n66|    ``quoting=csv.QUOTE_MINIMAL``, the output field value is quoted for empty\n67|    fields (when value == '').\n68|\n69|    This changes the API slightly in that the writerow() and writerows()\n70|    methods return the output written string instead of the length of\n71|    that string.\n72|\n73|    Examples\n74|    --------\n75|    >>> from astropy.io.ascii.core import CsvWriter\n76|    >>> writer = CsvWriter(delimiter=' ')\n77|    >>> print(writer.writerow(['hello', '', 'world']))\n78|    hello \"\" world\n79|    \"\"\"\n80|\n81|    # Random 16-character string that gets injected instead of any\n82|    # empty fields and is then replaced post-write with doubled-quotechar.\n83|    # Created with:\n84|    # ''.join(random.choice(string.printable[:90]) for _ in range(16))\n85|    replace_sentinel = \"2b=48Av%0-V3p>bX\"\n86|\n87|    def __init__(self, csvfile=None, **kwargs):\n88|        self.csvfile = csvfile\n89|\n90|        # Temporary StringIO for catching the real csv.writer() object output\n91|        self.temp_out = StringIO()\n92|        self.writer = csv.writer(self.temp_out, **kwargs)\n93|\n94|        dialect = self.writer.dialect\n95|        self.quotechar2 = dialect.quotechar * 2\n96|        self.quote_empty = (dialect.quoting == csv.QUOTE_MINIMAL) and (\n97|            dialect.delimiter == \" \"\n98|        )\n99|\n100|    def writerow(self, values):\n101|        \"\"\"\n102|        Similar to csv.writer.writerow but with the custom quoting behavior.\n103|        Returns the written string instead of the length of that string.\n104|        \"\"\"\n105|        has_empty = False\n106|\n107|        # If QUOTE_MINIMAL and space-delimited then replace empty fields with\n108|        # the sentinel value.\n109|        if self.quote_empty:\n110|            for i, value in enumerate(values):\n111|                if value == \"\":\n112|                    has_empty = True\n113|                    values[i] = self.replace_sentinel\n114|\n115|        return self._writerow(self.writer.writerow, values, has_empty)\n116|\n117|    def writerows(self, values_list):\n118|        \"\"\"\n119|        Similar to csv.writer.writerows but with the custom quoting behavior.\n120|        Returns the written string instead of the length of that string.\n121|        \"\"\"\n122|        has_empty = False\n123|\n124|        # If QUOTE_MINIMAL and space-delimited then replace empty fields with\n125|        # the sentinel value.\n126|        if self.quote_empty:\n127|            for values in values_list:\n128|                for i, value in enumerate(values):\n129|                    if value == \"\":\n130|                        has_empty = True\n131|                        values[i] = self.replace_sentinel\n132|\n133|        return self._writerow(self.writer.writerows, values_list, has_empty)\n134|\n135|    def _writerow(self, writerow_func, values, has_empty):\n136|        \"\"\"\n137|        Call ``writerow_func`` (either writerow or writerows) with ``values``.\n138|        If it has empty fields that have been replaced then change those\n139|        sentinel strings back to quoted empty strings, e.g. ``\"\"``.\n140|        \"\"\"\n141|        # Clear the temporary StringIO buffer that self.writer writes into and\n142|        # then call the real csv.writer().writerow or writerows with values.\n143|        self.temp_out.seek(0)\n144|        self.temp_out.truncate()\n145|        writerow_func(values)\n146|\n147|        row_string = self.temp_out.getvalue()\n148|\n149|        if self.quote_empty and has_empty:\n150|            row_string = re.sub(self.replace_sentinel, self.quotechar2, row_string)\n151|\n152|        # self.csvfile is defined then write the output.  In practice the pure\n153|        # Python writer calls with csvfile=None, while the fast writer calls with\n154|        # a file-like object.\n155|        if self.csvfile:\n156|            self.csvfile.write(row_string)\n157|\n158|        return row_string\n159|\n160|\n161|class MaskedConstant(numpy.ma.core.MaskedConstant):\n162|    \"\"\"A trivial extension of numpy.ma.masked.\n163|\n164|    We want to be able to put the generic term ``masked`` into a dictionary.\n165|    The constant ``numpy.ma.masked`` is not hashable (see\n166|    https://github.com/numpy/numpy/issues/4660), so we need to extend it\n167|    here with a hash value.\n168|\n169|    See https://github.com/numpy/numpy/issues/11021 for rationale for\n170|    __copy__ and __deepcopy__ methods.\n171|    \"\"\"\n172|\n173|    def __hash__(self):\n174|        \"\"\"All instances of this class shall have the same hash.\"\"\"\n175|        # Any large number will do.\n176|        return 1234567890\n177|\n178|    def __copy__(self):\n179|        \"\"\"This is a singleton so just return self.\"\"\"\n180|        return self\n181|\n182|    def __deepcopy__(self, memo):\n183|        return self\n184|\n185|\n186|masked = MaskedConstant()\n187|\n188|\n189|class InconsistentTableError(ValueError):\n190|    \"\"\"\n191|    Indicates that an input table is inconsistent in some way.\n192|\n193|    The default behavior of ``BaseReader`` is to throw an instance of\n194|    this class if a data row doesn't match the header.\n195|    \"\"\"\n196|\n197|\n198|class OptionalTableImportError(ImportError):\n199|    \"\"\"\n200|    Indicates that a dependency for table reading is not present.\n201|\n202|    An instance of this class is raised whenever an optional reader\n203|    with certain required dependencies cannot operate because of\n204|    an ImportError.\n205|    \"\"\"\n206|\n207|\n208|class ParameterError(NotImplementedError):\n209|    \"\"\"\n210|    Indicates that a reader cannot handle a passed parameter.\n211|\n212|    The C-based fast readers in ``io.ascii`` raise an instance of\n213|    this error class upon encountering a parameter that the\n214|    C engine cannot handle.\n215|    \"\"\"\n216|\n217|\n218|class FastOptionsError(NotImplementedError):\n219|    \"\"\"\n220|    Indicates that one of the specified options for fast\n221|    reading is invalid.\n222|    \"\"\"\n223|\n224|\n225|class NoType:\n226|    \"\"\"\n227|    Superclass for ``StrType`` and ``NumType`` classes.\n228|\n229|    This class is the default type of ``Column`` and provides a base\n230|    class for other data types.\n231|    \"\"\"\n232|\n233|\n234|class StrType(NoType):\n235|    \"\"\"\n236|    Indicates that a column consists of text data.\n237|    \"\"\"\n238|\n239|\n240|class NumType(NoType):\n241|    \"\"\"\n242|    Indicates that a column consists of numerical data.\n243|    \"\"\"\n244|\n245|\n246|class FloatType(NumType):\n247|    \"\"\"\n248|    Describes floating-point data.\n249|    \"\"\"\n250|\n251|\n252|class BoolType(NoType):\n253|    \"\"\"\n254|    Describes boolean data.\n255|    \"\"\"\n256|\n257|\n258|class IntType(NumType):\n259|    \"\"\"\n260|    Describes integer data.\n261|    \"\"\"\n262|\n263|\n264|class AllType(StrType, FloatType, IntType):\n265|    \"\"\"\n266|    Subclass of all other data types.\n267|\n268|    This type is returned by ``convert_numpy`` if the given numpy\n269|    type does not match ``StrType``, ``FloatType``, or ``IntType``.\n270|    \"\"\"\n271|\n272|\n273|class Column:\n274|    \"\"\"Table column.\n275|\n276|    The key attributes of a Column object are:\n277|\n278|    * **name** : column name\n279|    * **type** : column type (NoType, StrType, NumType, FloatType, IntType)\n280|    * **dtype** : numpy dtype (optional, overrides **type** if set)\n281|    * **str_vals** : list of column values as strings\n282|    * **fill_values** : dict of fill values\n283|    * **shape** : list of element shape (default [] => scalar)\n284|    * **data** : list of converted column values\n285|    * **subtype** : actual datatype for columns serialized with JSON\n286|    \"\"\"\n287|\n288|    def __init__(self, name):\n289|        self.name = name\n290|        self.type = NoType  # Generic type (Int, Float, Str etc)\n291|        self.dtype = None  # Numpy dtype if available\n292|        self.str_vals = []\n293|        self.fill_values = {}\n294|        self.shape = []\n295|        self.subtype = None\n296|\n297|\n298|class BaseInputter:\n299|    \"\"\"\n300|    Get the lines from the table input and return a list of lines.\n301|\n302|    \"\"\"\n303|\n304|    encoding = None\n305|    \"\"\"Encoding used to read the file\"\"\"\n306|\n307|    def get_lines(self, table, newline=None):\n308|        \"\"\"Get the lines from the ``table`` input.\n309|\n310|        The input table can be one of:\n311|\n312|        * File name\n313|        * String (newline separated) with all header and data lines (must have at least 2 lines)\n314|        * File-like object with read() method\n315|        * List of strings\n316|\n317|        Parameters\n318|        ----------\n319|        table : str, file-like, list\n320|            Can be either a file name, string (newline separated) with all header and data\n321|            lines (must have at least 2 lines), a file-like object with a\n322|            ``read()`` method, or a list of strings.\n323|        newline :\n324|            Line separator. If `None` use OS default from ``splitlines()``.\n325|\n326|        Returns\n327|        -------\n328|        lines : list\n329|            List of lines\n330|        \"\"\"\n331|        try:\n332|            if hasattr(table, \"read\") or (\n333|                \"\\n\" not in table + \"\" and \"\\r\" not in table + \"\"\n334|            ):\n335|                with get_readable_fileobj(table, encoding=self.encoding) as fileobj:\n336|                    table = fileobj.read()\n337|            if newline is None:\n338|                lines = table.splitlines()\n339|            else:\n340|                lines = table.split(newline)\n341|        except TypeError:\n342|            try:\n343|                # See if table supports indexing, slicing, and iteration\n344|                table[0]\n345|                table[0:1]\n346|                iter(table)\n347|                if len(table) > 1:\n348|                    lines = table\n349|                else:\n350|                    # treat single entry as if string had been passed directly\n351|                    if newline is None:\n352|                        lines = table[0].splitlines()\n353|                    else:\n354|                        lines = table[0].split(newline)\n355|\n356|            except TypeError:\n357|                raise TypeError(\n358|                    'Input \"table\" must be a string (filename or data) or an iterable'\n359|                )\n360|\n361|        return self.process_lines(lines)\n362|\n363|    def process_lines(self, lines):\n364|        \"\"\"Process lines for subsequent use.  In the default case do nothing.\n365|        This routine is not generally intended for removing comment lines or\n366|        stripping whitespace.  These are done (if needed) in the header and\n367|        data line processing.\n368|\n369|        Override this method if something more has to be done to convert raw\n370|        input lines to the table rows.  For example the\n371|        ContinuationLinesInputter derived class accounts for continuation\n372|        characters if a row is split into lines.\n373|        \"\"\"\n374|        return lines\n375|\n376|\n377|class BaseSplitter:\n378|    \"\"\"\n379|    Base splitter that uses python's split method to do the work.\n380|\n381|    This does not handle quoted values.  A key feature is the formulation of\n382|    __call__ as a generator that returns a list of the split line values at\n383|    each iteration.\n384|\n385|    There are two methods that are intended to be overridden, first\n386|    ``process_line()`` to do pre-processing on each input line before splitting\n387|    and ``process_val()`` to do post-processing on each split string value.  By\n388|    default these apply the string ``strip()`` function.  These can be set to\n389|    another function via the instance attribute or be disabled entirely, for\n390|    example::\n391|\n392|      reader.header.splitter.process_val = lambda x: x.lstrip()\n393|      reader.data.splitter.process_val = None\n394|\n395|    \"\"\"\n396|\n397|    delimiter = None\n398|    \"\"\" one-character string used to separate fields \"\"\"\n399|\n400|    def process_line(self, line):\n401|        \"\"\"Remove whitespace at the beginning or end of line.  This is especially useful for\n402|        whitespace-delimited files to prevent spurious columns at the beginning or end.\n403|        \"\"\"\n404|        return line.strip()\n405|\n406|    def process_val(self, val):\n407|        \"\"\"Remove whitespace at the beginning or end of value.\"\"\"\n408|        return val.strip()\n409|\n410|    def __call__(self, lines):\n411|        if self.process_line:\n412|            lines = (self.process_line(x) for x in lines)\n413|        for line in lines:\n414|            vals = line.split(self.delimiter)\n415|            if self.process_val:\n416|                yield [self.process_val(x) for x in vals]\n417|            else:\n418|                yield vals\n419|\n420|    def join(self, vals):\n421|        if self.delimiter is None:\n422|            delimiter = \" \"\n423|        else:\n424|            delimiter = self.delimiter\n425|        return delimiter.join(str(x) for x in vals)\n426|\n427|\n428|class DefaultSplitter(BaseSplitter):\n429|    \"\"\"Default class to split strings into columns using python csv.  The class\n430|    attributes are taken from the csv Dialect class.\n431|\n432|    Typical usage::\n433|\n434|      # lines = ..\n435|      splitter = ascii.DefaultSplitter()\n436|      for col_vals in splitter(lines):\n437|          for col_val in col_vals:\n438|               ...\n439|\n440|    \"\"\"\n441|\n442|    delimiter = \" \"\n443|    \"\"\" one-character string used to separate fields. \"\"\"\n444|    quotechar = '\"'\n445|    \"\"\" control how instances of *quotechar* in a field are quoted \"\"\"\n446|    doublequote = True\n447|    \"\"\" character to remove special meaning from following character \"\"\"\n448|    escapechar = None\n449|    \"\"\" one-character stringto quote fields containing special characters \"\"\"\n450|    quoting = csv.QUOTE_MINIMAL\n451|    \"\"\" control when quotes are recognized by the reader \"\"\"\n452|    skipinitialspace = True\n453|    \"\"\" ignore whitespace immediately following the delimiter \"\"\"\n454|    csv_writer = None\n455|    csv_writer_out = StringIO()\n456|\n457|    def process_line(self, line):\n458|        \"\"\"Remove whitespace at the beginning or end of line.  This is especially useful for\n459|        whitespace-delimited files to prevent spurious columns at the beginning or end.\n460|        If splitting on whitespace then replace unquoted tabs with space first.\n461|        \"\"\"\n462|        if self.delimiter == r\"\\s\":\n463|            line = _replace_tab_with_space(line, self.escapechar, self.quotechar)\n464|        return line.strip() + \"\\n\"\n465|\n466|    def process_val(self, val):\n467|        \"\"\"Remove whitespace at the beginning or end of value.\"\"\"\n468|        return val.strip(\" \\t\")\n469|\n470|    def __call__(self, lines):\n471|        \"\"\"Return an iterator over the table ``lines``, where each iterator output\n472|        is a list of the split line values.\n473|\n474|        Parameters\n475|        ----------\n476|        lines : list\n477|            List of table lines\n478|\n479|        Yields\n480|        ------\n481|        line : list of str\n482|            Each line's split values.\n483|\n484|        \"\"\"\n485|        if self.process_line:\n486|            lines = [self.process_line(x) for x in lines]\n487|\n488|        delimiter = \" \" if self.delimiter == r\"\\s\" else self.delimiter\n489|\n490|        csv_reader = csv.reader(\n491|            lines,\n492|            delimiter=delimiter,\n493|            doublequote=self.doublequote,\n494|            escapechar=self.escapechar,\n495|            quotechar=self.quotechar,\n496|            quoting=self.quoting,\n497|            skipinitialspace=self.skipinitialspace,\n498|        )\n499|        for vals in csv_reader:\n500|            if self.process_val:\n501|                yield [self.process_val(x) for x in vals]\n502|            else:\n503|                yield vals\n504|\n505|    def join(self, vals):\n506|        delimiter = \" \" if self.delimiter is None else str(self.delimiter)\n507|\n508|        if self.csv_writer is None:\n509|            self.csv_writer = CsvWriter(\n510|                delimiter=delimiter,\n511|                doublequote=self.doublequote,\n512|                escapechar=self.escapechar,\n513|                quotechar=self.quotechar,\n514|                quoting=self.quoting,\n515|            )\n516|        if self.process_val:\n517|            vals = [self.process_val(x) for x in vals]\n518|        out = self.csv_writer.writerow(vals).rstrip(\"\\r\\n\")\n519|\n520|        return out\n521|\n522|\n523|def _replace_tab_with_space(line, escapechar, quotechar):\n524|    \"\"\"Replace tabs with spaces in given string, preserving quoted substrings.\n525|\n526|    Parameters\n527|    ----------\n528|    line : str\n529|        String containing tabs to be replaced with spaces.\n530|    escapechar : str\n531|        Character in ``line`` used to escape special characters.\n532|    quotechar : str\n533|        Character in ``line`` indicating the start/end of a substring.\n534|\n535|    Returns\n536|    -------\n537|    line : str\n538|        A copy of ``line`` with tabs replaced by spaces, preserving quoted substrings.\n539|    \"\"\"\n540|    newline = []\n541|    in_quote = False\n542|    lastchar = \"NONE\"\n543|    for char in line:\n544|        if char == quotechar and lastchar != escapechar:\n545|            in_quote = not in_quote\n546|        if char == \"\\t\" and not in_quote:\n547|            char = \" \"\n548|        lastchar = char\n549|        newline.append(char)\n550|    return \"\".join(newline)\n551|\n552|\n553|def _get_line_index(line_or_func, lines):\n554|    \"\"\"Return the appropriate line index, depending on ``line_or_func`` which\n555|    can be either a function, a positive or negative int, or None.\n556|    \"\"\"\n557|    if hasattr(line_or_func, \"__call__\"):\n558|        return line_or_func(lines)\n559|    elif line_or_func:\n560|        if line_or_func >= 0:\n561|            return line_or_func\n562|        else:\n563|            n_lines = sum(1 for line in lines)\n564|            return n_lines + line_or_func\n565|    else:\n566|        return line_or_func\n567|\n568|\n569|class BaseHeader:\n570|    \"\"\"\n571|    Base table header reader.\n572|    \"\"\"\n573|\n574|    auto_format = \"col{}\"\n575|    \"\"\" format string for auto-generating column names \"\"\"\n576|    start_line = None\n577|    \"\"\" None, int, or a function of ``lines`` that returns None or int \"\"\"\n578|    comment = None\n579|    \"\"\" regular expression for comment lines \"\"\"\n580|    splitter_class = DefaultSplitter\n581|    \"\"\" Splitter class for splitting data lines into columns \"\"\"\n582|    names = None\n583|    \"\"\" list of names corresponding to each data column \"\"\"\n584|    write_comment = False\n585|    write_spacer_lines = [\"ASCII_TABLE_WRITE_SPACER_LINE\"]\n586|\n587|    def __init__(self):\n588|        self.splitter = self.splitter_class()\n589|\n590|    def _set_cols_from_names(self):\n591|        self.cols = [Column(name=x) for x in self.names]\n592|\n593|    def update_meta(self, lines, meta):\n594|        \"\"\"\n595|        Extract any table-level metadata, e.g. keywords, comments, column metadata, from\n596|        the table ``lines`` and update the OrderedDict ``meta`` in place.  This base\n597|        method extracts comment lines and stores them in ``meta`` for output.\n598|        \"\"\"\n599|        if self.comment:\n600|            re_comment = re.compile(self.comment)\n601|            comment_lines = [x for x in lines if re_comment.match(x)]\n602|        else:\n603|            comment_lines = []\n604|        comment_lines = [\n605|            re.sub(\"^\" + self.comment, \"\", x).strip() for x in comment_lines\n606|        ]\n607|        if comment_lines:\n608|            meta.setdefault(\"table\", {})[\"comments\"] = comment_lines\n609|\n610|    def get_cols(self, lines):\n611|        \"\"\"Initialize the header Column objects from the table ``lines``.\n612|\n613|        Based on the previously set Header attributes find or create the column names.\n614|        Sets ``self.cols`` with the list of Columns.\n615|\n616|        Parameters\n617|        ----------\n618|        lines : list\n619|            List of table lines\n620|\n621|        \"\"\"\n622|        start_line = _get_line_index(self.start_line, self.process_lines(lines))\n623|        if start_line is None:\n624|            # No header line so auto-generate names from n_data_cols\n625|            # Get the data values from the first line of table data to determine n_data_cols\n626|            try:\n627|                first_data_vals = next(self.data.get_str_vals())\n628|            except StopIteration:\n629|                raise InconsistentTableError(\n630|                    \"No data lines found so cannot autogenerate column names\"\n631|                )\n632|            n_data_cols = len(first_data_vals)\n633|            self.names = [self.auto_format.format(i) for i in range(1, n_data_cols + 1)]\n634|\n635|        else:\n636|            for i, line in enumerate(self.process_lines(lines)):\n637|                if i == start_line:\n638|                    break\n639|            else:  # No header line matching\n640|                raise ValueError(\"No header line found in table\")\n641|\n642|            self.names = next(self.splitter([line]))\n643|\n644|        self._set_cols_from_names()\n645|\n646|    def process_lines(self, lines):\n647|        \"\"\"Generator to yield non-blank and non-comment lines.\"\"\"\n648|        re_comment = re.compile(self.comment) if self.comment else None\n649|        # Yield non-comment lines\n650|        for line in lines:\n651|            if line.strip() and (not self.comment or not re_comment.match(line)):\n652|                yield line\n653|\n654|    def write_comments(self, lines, meta):\n655|        if self.write_comment not in (False, None):\n656|            for comment in meta.get(\"comments\", []):\n657|                lines.append(self.write_comment + comment)\n658|\n659|    def write(self, lines):\n660|        if self.start_line is not None:\n661|            for i, spacer_line in zip(\n662|                range(self.start_line), itertools.cycle(self.write_spacer_lines)\n663|            ):\n664|                lines.append(spacer_line)\n665|            lines.append(self.splitter.join([x.info.name for x in self.cols]))\n666|\n667|    @property\n668|    def colnames(self):\n669|        \"\"\"Return the column names of the table.\"\"\"\n670|        return tuple(\n671|            col.name if isinstance(col, Column) else col.info.name for col in self.cols\n672|        )\n673|\n674|    def remove_columns(self, names):\n675|        \"\"\"\n676|        Remove several columns from the table.\n677|\n678|        Parameters\n679|        ----------\n680|        names : list\n681|            A list containing the names of the columns to remove\n682|        \"\"\"\n683|        colnames = self.colnames\n684|        for name in names:\n685|            if name not in colnames:\n686|                raise KeyError(f\"Column {name} does not exist\")\n687|\n688|        self.cols = [col for col in self.cols if col.name not in names]\n689|\n690|    def rename_column(self, name, new_name):\n691|        \"\"\"\n692|        Rename a column.\n693|\n694|        Parameters\n695|        ----------\n696|        name : str\n697|            The current name of the column.\n698|        new_name : str\n699|            The new name for the column\n700|        \"\"\"\n701|        try:\n702|            idx = self.colnames.index(name)\n703|        except ValueError:\n704|            raise KeyError(f\"Column {name} does not exist\")\n705|\n706|        col = self.cols[idx]\n707|\n708|        # For writing self.cols can contain cols that are not Column.  Raise\n709|        # exception in that case.\n710|        if isinstance(col, Column):\n711|            col.name = new_name\n712|        else:\n713|            raise TypeError(f\"got column type {type(col)} instead of required {Column}\")\n714|\n715|    def get_type_map_key(self, col):\n716|        return col.raw_type\n717|\n718|    def get_col_type(self, col):\n719|        try:\n720|            type_map_key = self.get_type_map_key(col)\n721|            return self.col_type_map[type_map_key.lower()]\n722|        except KeyError:\n723|            raise ValueError(\n724|                f'Unknown data type \"\"{col.raw_type}\"\" for column \"{col.name}\"'\n725|            )\n726|\n727|    def check_column_names(self, names, strict_names, guessing):\n728|        \"\"\"\n729|        Check column names.\n730|\n731|        This must be done before applying the names transformation\n732|        so that guessing will fail appropriately if ``names`` is supplied.\n733|        For instance if the basic reader is given a table with no column header\n734|        row.\n735|\n736|        Parameters\n737|        ----------\n738|        names : list\n739|            User-supplied list of column names\n740|        strict_names : bool\n741|            Whether to impose extra requirements on names\n742|        guessing : bool\n743|            True if this method is being called while guessing the table format\n744|        \"\"\"\n745|        if strict_names:\n746|            # Impose strict requirements on column names (normally used in guessing)\n747|            bads = [\" \", \",\", \"|\", \"\\t\", \"'\", '\"']\n748|            for name in self.colnames:\n749|                if (\n750|                    _is_number(name)\n751|                    or len(name) == 0\n752|                    or name[0] in bads\n753|                    or name[-1] in bads\n754|                ):\n755|                    raise InconsistentTableError(\n756|                        f\"Column name {name!r} does not meet strict name requirements\"\n757|                    )\n758|        # When guessing require at least two columns, except for ECSV which can\n759|        # reliably be guessed from the header requirements.\n760|        if (\n761|            guessing\n762|            and len(self.colnames) <= 1\n763|            and self.__class__.__name__ != \"EcsvHeader\"\n764|        ):\n765|            raise ValueError(\n766|                \"Table format guessing requires at least two columns, got {}\".format(\n767|                    list(self.colnames)\n768|                )\n769|            )\n770|\n771|        if names is not None and len(names) != len(self.colnames):\n772|            raise InconsistentTableError(\n773|                \"Length of names argument ({}) does not match number\"\n774|                \" of table columns ({})\".format(len(names), len(self.colnames))\n775|            )\n776|\n777|\n778|class BaseData:\n779|    \"\"\"\n780|    Base table data reader.\n781|    \"\"\"\n782|\n783|    start_line = None\n784|    \"\"\" None, int, or a function of ``lines`` that returns None or int \"\"\"\n785|    end_line = None\n786|    \"\"\" None, int, or a function of ``lines`` that returns None or int \"\"\"\n787|    comment = None\n788|    \"\"\" Regular expression for comment lines \"\"\"\n789|    splitter_class = DefaultSplitter\n790|    \"\"\" Splitter class for splitting data lines into columns \"\"\"\n791|    write_spacer_lines = [\"ASCII_TABLE_WRITE_SPACER_LINE\"]\n792|    fill_include_names = None\n793|    fill_exclude_names = None\n794|    fill_values = [(masked, \"\")]\n795|    formats = {}\n796|\n797|    def __init__(self):\n798|        # Need to make sure fill_values list is instance attribute, not class attribute.\n799|        # On read, this will be overwritten by the default in the ui.read (thus, in\n800|        # the current implementation there can be no different default for different\n801|        # Readers). On write, ui.py does not specify a default, so this line here matters.\n802|        self.fill_values = copy.copy(self.fill_values)\n803|        self.formats = copy.copy(self.formats)\n804|        self.splitter = self.splitter_class()\n805|\n806|    def process_lines(self, lines):\n807|        \"\"\"\n808|        READ: Strip out comment lines and blank lines from list of ``lines``.\n809|\n810|        Parameters\n811|        ----------\n812|        lines : list\n813|            All lines in table\n814|\n815|        Returns\n816|        -------\n817|        lines : list\n818|            List of lines\n819|\n820|        \"\"\"\n821|        nonblank_lines = (x for x in lines if x.strip())\n822|        if self.comment:\n823|            re_comment = re.compile(self.comment)\n824|            return [x for x in nonblank_lines if not re_comment.match(x)]\n825|        else:\n826|            return [x for x in nonblank_lines]\n827|\n828|    def get_data_lines(self, lines):\n829|        \"\"\"\n830|        READ: Set ``data_lines`` attribute to lines slice comprising table data values.\n831|        \"\"\"\n832|        data_lines = self.process_lines(lines)\n833|        start_line = _get_line_index(self.start_line, data_lines)\n834|        end_line = _get_line_index(self.end_line, data_lines)\n835|\n836|        if start_line is not None or end_line is not None:\n837|            self.data_lines = data_lines[slice(start_line, end_line)]\n838|        else:  # Don't copy entire data lines unless necessary\n839|            self.data_lines = data_lines\n840|\n841|    def get_str_vals(self):\n842|        \"\"\"Return a generator that returns a list of column values (as strings)\n843|        for each data line.\n844|        \"\"\"\n845|        return self.splitter(self.data_lines)\n846|\n847|    def masks(self, cols):\n848|        \"\"\"READ: Set fill value for each column and then apply that fill value.\n849|\n850|        In the first step it is evaluated with value from ``fill_values`` applies to\n851|        which column using ``fill_include_names`` and ``fill_exclude_names``.\n852|        In the second step all replacements are done for the appropriate columns.\n853|        \"\"\"\n854|        if self.fill_values:\n855|            self._set_fill_values(cols)\n856|            self._set_masks(cols)\n857|\n858|    def _set_fill_values(self, cols):\n859|        \"\"\"READ, WRITE: Set fill values of individual cols based on fill_values of BaseData.\n860|\n861|        fill values has the following form:\n862|        <fill_spec> = (<bad_value>, <fill_value>, <optional col_name>...)\n863|        fill_values = <fill_spec> or list of <fill_spec>'s\n864|\n865|        \"\"\"\n866|        if self.fill_values:\n867|            # when we write tables the columns may be astropy.table.Columns\n868|            # which don't carry a fill_values by default\n869|            for col in cols:\n870|                if not hasattr(col, \"fill_values\"):\n871|                    col.fill_values = {}\n872|\n873|            # if input is only one <fill_spec>, then make it a list\n874|            with suppress(TypeError):\n875|                self.fill_values[0] + \"\"\n876|                self.fill_values = [self.fill_values]\n877|\n878|            # Step 1: Set the default list of columns which are affected by\n879|            # fill_values\n880|            colnames = set(self.header.colnames)\n881|            if self.fill_include_names is not None:\n882|                colnames.intersection_update(self.fill_include_names)\n883|            if self.fill_exclude_names is not None:\n884|                colnames.difference_update(self.fill_exclude_names)\n885|\n886|            # Step 2a: Find out which columns are affected by this tuple\n887|            # iterate over reversed order, so last condition is set first and\n888|            # overwritten by earlier conditions\n889|            for replacement in reversed(self.fill_values):\n890|                if len(replacement) < 2:\n891|                    raise ValueError(\n892|                        \"Format of fill_values must be \"\n893|                        \"(<bad>, <fill>, <optional col1>, ...)\"\n894|                    )\n895|                elif len(replacement) == 2:\n896|                    affect_cols = colnames\n897|                else:\n898|                    affect_cols = replacement[2:]\n899|\n900|                for i, key in (\n901|                    (i, x)\n902|                    for i, x in enumerate(self.header.colnames)\n903|                    if x in affect_cols\n904|                ):\n905|                    cols[i].fill_values[replacement[0]] = str(replacement[1])\n906|\n907|    def _set_masks(self, cols):\n908|        \"\"\"READ: Replace string values in col.str_vals and set masks.\"\"\"\n909|        if self.fill_values:\n910|            for col in (col for col in cols if col.fill_values):\n911|                col.mask = numpy.zeros(len(col.str_vals), dtype=bool)\n912|                for i, str_val in (\n913|                    (i, x) for i, x in enumerate(col.str_vals) if x in col.fill_values\n914|                ):\n915|                    col.str_vals[i] = col.fill_values[str_val]\n916|                    col.mask[i] = True\n917|\n918|    def _replace_vals(self, cols):\n919|        \"\"\"WRITE: replace string values in col.str_vals.\"\"\"\n920|        if self.fill_values:\n921|            for col in (col for col in cols if col.fill_values):\n922|                for i, str_val in (\n923|                    (i, x) for i, x in enumerate(col.str_vals) if x in col.fill_values\n924|                ):\n925|                    col.str_vals[i] = col.fill_values[str_val]\n926|                if masked in col.fill_values and hasattr(col, \"mask\"):\n927|                    mask_val = col.fill_values[masked]\n928|                    for i in col.mask.nonzero()[0]:\n929|                        col.str_vals[i] = mask_val\n930|\n931|    def str_vals(self):\n932|        \"\"\"WRITE: convert all values in table to a list of lists of strings.\n933|\n934|        This sets the fill values and possibly column formats from the input\n935|        formats={} keyword, then ends up calling table.pprint._pformat_col_iter()\n936|        by a circuitous path. That function does the real work of formatting.\n937|        Finally replace anything matching the fill_values.\n938|\n939|        Returns\n940|        -------\n941|        values : list of list of str\n942|        \"\"\"\n943|        self._set_fill_values(self.cols)\n944|        self._set_col_formats()\n945|        for col in self.cols:\n946|            col.str_vals = list(col.info.iter_str_vals())\n947|        self._replace_vals(self.cols)\n948|        return [col.str_vals for col in self.cols]\n949|\n950|    def write(self, lines):\n951|        \"\"\"Write ``self.cols`` in place to ``lines``.\n952|\n953|        Parameters\n954|        ----------\n955|        lines : list\n956|            List for collecting output of writing self.cols.\n957|        \"\"\"\n958|        if hasattr(self.start_line, \"__call__\"):\n959|            raise TypeError(\"Start_line attribute cannot be callable for write()\")\n960|        else:\n961|            data_start_line = self.start_line or 0\n962|\n963|        while len(lines) < data_start_line:\n964|            lines.append(itertools.cycle(self.write_spacer_lines))\n965|\n966|        col_str_iters = self.str_vals()\n967|        for vals in zip(*col_str_iters):\n968|            lines.append(self.splitter.join(vals))\n969|\n970|    def _set_col_formats(self):\n971|        \"\"\"WRITE: set column formats.\"\"\"\n972|        for col in self.cols:\n973|            if col.info.name in self.formats:\n974|                col.info.format = self.formats[col.info.name]\n975|\n976|\n977|def convert_numpy(numpy_type):\n978|    \"\"\"Return a tuple containing a function which converts a list into a numpy\n979|    array and the type produced by the converter function.\n980|\n981|    Parameters\n982|    ----------\n983|    numpy_type : numpy data-type\n984|        The numpy type required of an array returned by ``converter``. Must be a\n985|        valid `numpy type <https://numpy.org/doc/stable/user/basics.types.html>`_\n986|        (e.g., numpy.uint, numpy.int8, numpy.int64, numpy.float64) or a python\n987|        type covered by a numpy type (e.g., int, float, str, bool).\n988|\n989|    Returns\n990|    -------\n991|    converter : callable\n992|        ``converter`` is a function which accepts a list and converts it to a\n993|        numpy array of type ``numpy_type``.\n994|    converter_type : type\n995|        ``converter_type`` tracks the generic data type produced by the\n996|        converter function.\n997|\n998|    Raises\n999|    ------\n1000|    ValueError\n1001|        Raised by ``converter`` if the list elements could not be converted to\n1002|        the required type.\n1003|    \"\"\"\n1004|    # Infer converter type from an instance of numpy_type.\n1005|    type_name = numpy.array([], dtype=numpy_type).dtype.name\n1006|    if \"int\" in type_name:\n1007|        converter_type = IntType\n1008|    elif \"float\" in type_name:\n1009|        converter_type = FloatType\n1010|    elif \"bool\" in type_name:\n1011|        converter_type = BoolType\n1012|    elif \"str\" in type_name:\n1013|        converter_type = StrType\n1014|    else:\n1015|        converter_type = AllType\n1016|\n1017|    def bool_converter(vals):\n1018|        \"\"\"\n1019|        Convert values \"False\" and \"True\" to bools.  Raise an exception\n1020|        for any other string values.\n1021|        \"\"\"\n1022|        if len(vals) == 0:\n1023|            return numpy.array([], dtype=bool)\n1024|\n1025|        # Try a smaller subset first for a long array\n1026|        if len(vals) > 10000:\n1027|            svals = numpy.asarray(vals[:1000])\n1028|            if not numpy.all(\n1029|                (svals == \"False\") | (svals == \"True\") | (svals == \"0\") | (svals == \"1\")\n1030|            ):\n1031|                raise ValueError('bool input strings must be False, True, 0, 1, or \"\"')\n1032|        vals = numpy.asarray(vals)\n1033|\n1034|        trues = (vals == \"True\") | (vals == \"1\")\n1035|        falses = (vals == \"False\") | (vals == \"0\")\n1036|        if not numpy.all(trues | falses):\n1037|            raise ValueError('bool input strings must be only False, True, 0, 1, or \"\"')\n1038|\n1039|        return trues\n1040|\n1041|    def generic_converter(vals):\n1042|        return numpy.array(vals, numpy_type)\n1043|\n1044|    converter = bool_converter if converter_type is BoolType else generic_converter\n1045|\n1046|    return converter, converter_type\n1047|\n1048|\n1049|class BaseOutputter:\n1050|    \"\"\"Output table as a dict of column objects keyed on column name.  The\n1051|    table data are stored as plain python lists within the column objects.\n1052|    \"\"\"\n1053|\n1054|    # User-defined converters which gets set in ascii.ui if a `converter` kwarg\n1055|    # is supplied.\n1056|    converters = {}\n1057|\n1058|    # Derived classes must define default_converters and __call__\n1059|\n1060|    @staticmethod\n1061|    def _validate_and_copy(col, converters):\n1062|        \"\"\"Validate the format for the type converters and then copy those\n1063|        which are valid converters for this column (i.e. converter type is\n1064|        a subclass of col.type).\n1065|        \"\"\"\n1066|        # Allow specifying a single converter instead of a list of converters.\n1067|        # The input `converters` must be a ``type`` value that can init np.dtype.\n1068|        try:\n1069|            # Don't allow list-like things that dtype accepts\n1070|            assert type(converters) is type\n1071|            converters = [numpy.dtype(converters)]\n1072|        except (AssertionError, TypeError):\n1073|            pass\n1074|\n1075|        converters_out = []\n1076|        try:\n1077|            for converter in converters:\n1078|                try:\n1079|                    converter_func, converter_type = converter\n1080|                except TypeError as err:\n1081|                    if str(err).startswith(\"cannot unpack\"):\n1082|                        converter_func, converter_type = convert_numpy(converter)\n1083|                    else:\n1084|                        raise\n1085|                if not issubclass(converter_type, NoType):\n1086|                    raise ValueError(\"converter_type must be a subclass of NoType\")\n1087|                if issubclass(converter_type, col.type):\n1088|                    converters_out.append((converter_func, converter_type))\n1089|\n1090|        except (ValueError, TypeError) as err:\n1091|            raise ValueError(\n1092|                \"Error: invalid format for converters, see \"\n1093|                f\"documentation\\n{converters}: {err}\"\n1094|            )\n1095|        return converters_out\n1096|\n1097|    def _convert_vals(self, cols):\n1098|        for col in cols:\n1099|            for key, converters in self.converters.items():\n1100|                if fnmatch.fnmatch(col.name, key):\n1101|                    break\n1102|            else:\n1103|                if col.dtype is not None:\n1104|                    converters = [convert_numpy(col.dtype)]\n1105|                else:\n1106|                    converters = self.default_converters\n1107|\n1108|            col.converters = self._validate_and_copy(col, converters)\n1109|\n1110|            # Catch the last error in order to provide additional information\n1111|            # in case all attempts at column conversion fail.  The initial\n1112|            # value of of last_error will apply if no converters are defined\n1113|            # and the first col.converters[0] access raises IndexError.\n1114|            last_err = \"no converters defined\"\n1115|\n1116|            while not hasattr(col, \"data\"):\n1117|                # Try converters, popping the unsuccessful ones from the list.\n1118|                # If there are no converters left here then fail.\n1119|                if not col.converters:\n1120|                    raise ValueError(f\"Column {col.name} failed to convert: {last_err}\")\n1121|\n1122|                converter_func, converter_type = col.converters[0]\n1123|                if not issubclass(converter_type, col.type):\n1124|                    raise TypeError(\"converter type does not match column type\")\n1125|\n1126|                try:\n1127|                    col.data = converter_func(col.str_vals)\n1128|                    col.type = converter_type\n1129|                except (OverflowError, TypeError, ValueError) as err:\n1130|                    # Overflow during conversion (most likely an int that\n1131|                    # doesn't fit in native C long). Put string at the top of\n1132|                    # the converters list for the next while iteration.\n1133|                    # With python/cpython#95778 this has been supplemented with a\n1134|                    # \"ValueError: Exceeds the limit (4300) for integer string conversion\"\n1135|                    # so need to catch that as well.\n1136|                    if isinstance(err, OverflowError) or (\n1137|                        isinstance(err, ValueError)\n1138|                        and str(err).startswith(\"Exceeds the limit\")\n1139|                    ):\n1140|                        warnings.warn(\n1141|                            f\"OverflowError converting to {converter_type.__name__} in\"\n1142|                            f\" column {col.name}, reverting to String.\",\n1143|                            AstropyWarning,\n1144|                        )\n1145|                        col.converters.insert(0, convert_numpy(str))\n1146|                    else:\n1147|                        col.converters.pop(0)\n1148|                    last_err = err\n1149|\n1150|\n1151|def _deduplicate_names(names):\n1152|    \"\"\"Ensure there are no duplicates in ``names``.\n1153|\n1154|    This is done by iteratively adding ``_<N>`` to the name for increasing N\n1155|    until the name is unique.\n1156|    \"\"\"\n1157|    new_names = []\n1158|    existing_names = set()\n1159|\n1160|    for name in names:\n1161|        base_name = name + \"_\"\n1162|        i = 1\n1163|        while name in existing_names:\n1164|            # Iterate until a unique name is found\n1165|            name = base_name + str(i)\n1166|            i += 1\n1167|        new_names.append(name)\n1168|        existing_names.add(name)\n1169|\n1170|    return new_names\n1171|\n1172|\n1173|class TableOutputter(BaseOutputter):\n1174|    \"\"\"\n1175|    Output the table as an astropy.table.Table object.\n1176|    \"\"\"\n1177|\n1178|    default_converters = [convert_numpy(int), convert_numpy(float), convert_numpy(str)]\n1179|\n1180|    def __call__(self, cols, meta):\n1181|        # Sets col.data to numpy array and col.type to io.ascii Type class (e.g.\n1182|        # FloatType) for each col.\n1183|        self._convert_vals(cols)\n1184|\n1185|        t_cols = [\n1186|            numpy.ma.MaskedArray(x.data, mask=x.mask)\n1187|            if hasattr(x, \"mask\") and numpy.any(x.mask)\n1188|            else x.data\n1189|            for x in cols\n1190|        ]\n1191|        out = Table(t_cols, names=[x.name for x in cols], meta=meta[\"table\"])\n1192|\n1193|        for col, out_col in zip(cols, out.columns.values()):\n1194|            for attr in (\"format\", \"unit\", \"description\"):\n1195|                if hasattr(col, attr):\n1196|                    setattr(out_col, attr, getattr(col, attr))\n1197|            if hasattr(col, \"meta\"):\n1198|                out_col.meta.update(col.meta)\n1199|\n1200|        return out\n1201|\n1202|\n1203|class MetaBaseReader(type):\n1204|    def __init__(cls, name, bases, dct):\n1205|        super().__init__(name, bases, dct)\n1206|\n1207|        format = dct.get(\"_format_name\")\n1208|        if format is None:\n1209|            return\n1210|\n1211|        fast = dct.get(\"_fast\")\n1212|        if fast is not None:\n1213|            FAST_CLASSES[format] = cls\n1214|\n1215|        FORMAT_CLASSES[format] = cls\n1216|\n1217|        io_formats = [\"ascii.\" + format] + dct.get(\"_io_registry_format_aliases\", [])\n1218|\n1219|        if dct.get(\"_io_registry_suffix\"):\n1220|            func = functools.partial(connect.io_identify, dct[\"_io_registry_suffix\"])\n1221|            connect.io_registry.register_identifier(io_formats[0], Table, func)\n1222|\n1223|        for io_format in io_formats:\n1224|            func = functools.partial(connect.io_read, io_format)\n1225|            header = f\"ASCII reader '{io_format}' details\\n\"\n1226|            func.__doc__ = (\n1227|                inspect.cleandoc(READ_DOCSTRING).strip()\n1228|                + \"\\n\\n\"\n1229|                + header\n1230|                + re.sub(\".\", \"=\", header)\n1231|                + \"\\n\"\n1232|            )\n1233|            func.__doc__ += inspect.cleandoc(cls.__doc__).strip()\n1234|            connect.io_registry.register_reader(io_format, Table, func)\n1235|\n1236|            if dct.get(\"_io_registry_can_write\", True):\n1237|                func = functools.partial(connect.io_write, io_format)\n1238|                header = f\"ASCII writer '{io_format}' details\\n\"\n1239|                func.__doc__ = (\n1240|                    inspect.cleandoc(WRITE_DOCSTRING).strip()\n1241|                    + \"\\n\\n\"\n1242|                    + header\n1243|                    + re.sub(\".\", \"=\", header)\n1244|                    + \"\\n\"\n1245|                )\n1246|                func.__doc__ += inspect.cleandoc(cls.__doc__).strip()\n1247|                connect.io_registry.register_writer(io_format, Table, func)\n1248|\n1249|\n1250|def _is_number(x):\n1251|    with suppress(ValueError):\n1252|        x = float(x)\n1253|        return True\n1254|    return False\n1255|\n1256|\n1257|def _apply_include_exclude_names(table, names, include_names, exclude_names):\n1258|    \"\"\"\n1259|    Apply names, include_names and exclude_names to a table or BaseHeader.\n1260|\n1261|    For the latter this relies on BaseHeader implementing ``colnames``,\n1262|    ``rename_column``, and ``remove_columns``.\n1263|\n1264|    Parameters\n1265|    ----------\n1266|    table : `~astropy.table.Table`, `~astropy.io.ascii.BaseHeader`\n1267|        Input table or BaseHeader subclass instance\n1268|    names : list\n1269|        List of names to override those in table (set to None to use existing names)\n1270|    include_names : list\n1271|        List of names to include in output\n1272|    exclude_names : list\n1273|        List of names to exclude from output (applied after ``include_names``)\n1274|\n1275|    \"\"\"\n1276|\n1277|    def rename_columns(table, names):\n1278|        # Rename table column names to those passed by user\n1279|        # Temporarily rename with names that are not in `names` or `table.colnames`.\n1280|        # This ensures that rename succeeds regardless of existing names.\n1281|        xxxs = \"x\" * max(len(name) for name in list(names) + list(table.colnames))\n1282|        for ii, colname in enumerate(table.colnames):\n1283|            table.rename_column(colname, xxxs + str(ii))\n1284|\n1285|        for ii, name in enumerate(names):\n1286|            table.rename_column(xxxs + str(ii), name)\n1287|\n1288|    if names is not None:\n1289|        rename_columns(table, names)\n1290|    else:\n1291|        colnames_uniq = _deduplicate_names(table.colnames)\n1292|        if colnames_uniq != list(table.colnames):\n1293|            rename_columns(table, colnames_uniq)\n1294|\n1295|    names_set = set(table.colnames)\n1296|\n1297|    if include_names is not None:\n1298|        names_set.intersection_update(include_names)\n1299|    if exclude_names is not None:\n1300|        names_set.difference_update(exclude_names)\n1301|    if names_set != set(table.colnames):\n1302|        remove_names = set(table.colnames) - names_set\n1303|        table.remove_columns(remove_names)\n1304|\n1305|\n1306|class BaseReader(metaclass=MetaBaseReader):\n1307|    \"\"\"Class providing methods to read and write an ASCII table using the specified\n1308|    header, data, inputter, and outputter instances.\n1309|\n1310|    Typical usage is to instantiate a Reader() object and customize the\n1311|    ``header``, ``data``, ``inputter``, and ``outputter`` attributes.  Each\n1312|    of these is an object of the corresponding class.\n1313|\n1314|    There is one method ``inconsistent_handler`` that can be used to customize the\n1315|    behavior of ``read()`` in the event that a data row doesn't match the header.\n1316|    The default behavior is to raise an InconsistentTableError.\n1317|\n1318|    \"\"\"\n1319|\n1320|    names = None\n1321|    include_names = None\n1322|    exclude_names = None\n1323|    strict_names = False\n1324|    guessing = False\n1325|    encoding = None\n1326|\n1327|    header_class = BaseHeader\n1328|    data_class = BaseData\n1329|    inputter_class = BaseInputter\n1330|    outputter_class = TableOutputter\n1331|\n1332|    # Max column dimension that writer supports for this format. Exceptions\n1333|    # include ECSV (no limit) and HTML (max_ndim=2).\n1334|    max_ndim = 1\n1335|\n1336|    def __init__(self):\n1337|        self.header = self.header_class()\n1338|        self.data = self.data_class()\n1339|        self.inputter = self.inputter_class()\n1340|        self.outputter = self.outputter_class()\n1341|        # Data and Header instances benefit from a little cross-coupling.  Header may need to\n1342|        # know about number of data columns for auto-column name generation and Data may\n1343|        # need to know about header (e.g. for fixed-width tables where widths are spec'd in header.\n1344|        self.data.header = self.header\n1345|        self.header.data = self.data\n1346|\n1347|        # Metadata, consisting of table-level meta and column-level meta.  The latter\n1348|        # could include information about column type, description, formatting, etc,\n1349|        # depending on the table meta format.\n1350|        self.meta = OrderedDict(table=OrderedDict(), cols=OrderedDict())\n1351|\n1352|    def _check_multidim_table(self, table):\n1353|        \"\"\"Check that the dimensions of columns in ``table`` are acceptable.\n1354|\n1355|        The reader class attribute ``max_ndim`` defines the maximum dimension of\n1356|        columns that can be written using this format. The base value is ``1``,\n1357|        corresponding to normal scalar columns with just a length.\n1358|\n1359|        Parameters\n1360|        ----------\n1361|        table : `~astropy.table.Table`\n1362|            Input table.\n1363|\n1364|        Raises\n1365|        ------\n1366|        ValueError\n1367|            If any column exceeds the number of allowed dimensions\n1368|        \"\"\"\n1369|        _check_multidim_table(table, self.max_ndim)\n1370|\n1371|    def read(self, table):\n1372|        \"\"\"Read the ``table`` and return the results in a format determined by\n1373|        the ``outputter`` attribute.\n1374|\n1375|        The ``table`` parameter is any string or object that can be processed\n1376|        by the instance ``inputter``.  For the base Inputter class ``table`` can be\n1377|        one of:\n1378|\n1379|        * File name\n1380|        * File-like object\n1381|        * String (newline separated) with all header and data lines (must have at least 2 lines)\n1382|        * List of strings\n1383|\n1384|        Parameters\n1385|        ----------\n1386|        table : str, file-like, list\n1387|            Input table.\n1388|\n1389|        Returns\n1390|        -------\n1391|        table : `~astropy.table.Table`\n1392|            Output table\n1393|\n1394|        \"\"\"\n1395|        # If ``table`` is a file then store the name in the ``data``\n1396|        # attribute. The ``table`` is a \"file\" if it is a string\n1397|        # without the new line specific to the OS.\n1398|        with suppress(TypeError):\n1399|            # Strings only\n1400|            if os.linesep not in table + \"\":\n1401|                self.data.table_name = os.path.basename(table)\n1402|\n1403|        # If one of the newline chars is set as field delimiter, only\n1404|        # accept the other one as line splitter\n1405|        if self.header.splitter.delimiter == \"\\n\":\n1406|            newline = \"\\r\"\n1407|        elif self.header.splitter.delimiter == \"\\r\":\n1408|            newline = \"\\n\"\n1409|        else:\n1410|            newline = None\n1411|\n1412|        # Get a list of the lines (rows) in the table\n1413|        self.lines = self.inputter.get_lines(table, newline=newline)\n1414|\n1415|        # Set self.data.data_lines to a slice of lines contain the data rows\n1416|        self.data.get_data_lines(self.lines)\n1417|\n1418|        # Extract table meta values (e.g. keywords, comments, etc).  Updates self.meta.\n1419|        self.header.update_meta(self.lines, self.meta)\n1420|\n1421|        # Get the table column definitions\n1422|        self.header.get_cols(self.lines)\n1423|\n1424|        # Make sure columns are valid\n1425|        self.header.check_column_names(self.names, self.strict_names, self.guessing)\n1426|\n1427|        self.cols = cols = self.header.cols\n1428|        self.data.splitter.cols = cols\n1429|        n_cols = len(cols)\n1430|\n1431|        for i, str_vals in enumerate(self.data.get_str_vals()):\n1432|            if len(str_vals) != n_cols:\n1433|                str_vals = self.inconsistent_handler(str_vals, n_cols)\n1434|\n1435|                # if str_vals is None, we skip this row\n1436|                if str_vals is None:\n1437|                    continue\n1438|\n1439|                # otherwise, we raise an error only if it is still inconsistent\n1440|                if len(str_vals) != n_cols:\n1441|                    errmsg = (\n1442|                        \"Number of header columns ({}) inconsistent with\"\n1443|                        \" data columns ({}) at data line {}\\n\"\n1444|                        \"Header values: {}\\n\"\n1445|                        \"Data values: {}\".format(\n1446|                            n_cols, len(str_vals), i, [x.name for x in cols], str_vals\n1447|                        )\n1448|                    )\n1449|\n1450|                    raise InconsistentTableError(errmsg)\n1451|\n1452|            for j, col in enumerate(cols):\n1453|                col.str_vals.append(str_vals[j])\n1454|\n1455|        self.data.masks(cols)\n1456|        if hasattr(self.header, \"table_meta\"):\n1457|            self.meta[\"table\"].update(self.header.table_meta)\n1458|\n1459|        _apply_include_exclude_names(\n1460|            self.header, self.names, self.include_names, self.exclude_names\n1461|        )\n1462|\n1463|        table = self.outputter(self.header.cols, self.meta)\n1464|        self.cols = self.header.cols\n1465|\n1466|        return table\n1467|\n1468|    def inconsistent_handler(self, str_vals, ncols):\n1469|        \"\"\"\n1470|        Adjust or skip data entries if a row is inconsistent with the header.\n1471|\n1472|        The default implementation does no adjustment, and hence will always trigger\n1473|        an exception in read() any time the number of data entries does not match\n1474|        the header.\n1475|\n1476|        Note that this will *not* be called if the row already matches the header.\n1477|\n1478|        Parameters\n1479|        ----------\n1480|        str_vals : list\n1481|            A list of value strings from the current row of the table.\n1482|        ncols : int\n1483|            The expected number of entries from the table header.\n1484|\n1485|        Returns\n1486|        -------\n1487|        str_vals : list\n1488|            List of strings to be parsed into data entries in the output table. If\n1489|            the length of this list does not match ``ncols``, an exception will be\n1490|            raised in read().  Can also be None, in which case the row will be\n1491|            skipped.\n1492|        \"\"\"\n1493|        # an empty list will always trigger an InconsistentTableError in read()\n1494|        return str_vals\n1495|\n1496|    @property\n1497|    def comment_lines(self):\n1498|        \"\"\"Return lines in the table that match header.comment regexp.\"\"\"\n1499|        if not hasattr(self, \"lines\"):\n1500|            raise ValueError(\n1501|                \"Table must be read prior to accessing the header comment lines\"\n1502|            )\n1503|        if self.header.comment:\n1504|            re_comment = re.compile(self.header.comment)\n1505|            comment_lines = [x for x in self.lines if re_comment.match(x)]\n1506|        else:\n1507|            comment_lines = []\n1508|        return comment_lines\n1509|\n1510|    def update_table_data(self, table):\n1511|        \"\"\"\n1512|        Update table columns in place if needed.\n1513|\n1514|        This is a hook to allow updating the table columns after name\n1515|        filtering but before setting up to write the data.  This is currently\n1516|        only used by ECSV and is otherwise just a pass-through.\n1517|\n1518|        Parameters\n1519|        ----------\n1520|        table : `astropy.table.Table`\n1521|            Input table for writing\n1522|\n1523|        Returns\n1524|        -------\n1525|        table : `astropy.table.Table`\n1526|            Output table for writing\n1527|        \"\"\"\n1528|        return table\n1529|\n1530|    def write_header(self, lines, meta):\n1531|        self.header.write_comments(lines, meta)\n1532|        self.header.write(lines)\n1533|\n1534|    def write(self, table):\n1535|        \"\"\"\n1536|        Write ``table`` as list of strings.\n1537|\n1538|        Parameters\n1539|        ----------\n1540|        table : `~astropy.table.Table`\n1541|            Input table data.\n1542|\n1543|        Returns\n1544|        -------\n1545|        lines : list\n1546|            List of strings corresponding to ASCII table\n1547|\n1548|        \"\"\"\n1549|        # Check column names before altering\n1550|        self.header.cols = list(table.columns.values())\n1551|        self.header.check_column_names(self.names, self.strict_names, False)\n1552|\n1553|        # In-place update of columns in input ``table`` to reflect column\n1554|        # filtering.  Note that ``table`` is guaranteed to be a copy of the\n1555|        # original user-supplied table.\n1556|        _apply_include_exclude_names(\n1557|            table, self.names, self.include_names, self.exclude_names\n1558|        )\n1559|\n1560|        # This is a hook to allow updating the table columns after name\n1561|        # filtering but before setting up to write the data.  This is currently\n1562|        # only used by ECSV and is otherwise just a pass-through.\n1563|        table = self.update_table_data(table)\n1564|\n1565|        # Check that table column dimensions are supported by this format class.\n1566|        # Most formats support only 1-d columns, but some like ECSV support N-d.\n1567|        self._check_multidim_table(table)\n1568|\n1569|        # Now use altered columns\n1570|        new_cols = list(table.columns.values())\n1571|        # link information about the columns to the writer object (i.e. self)\n1572|        self.header.cols = new_cols\n1573|        self.data.cols = new_cols\n1574|        self.header.table_meta = table.meta\n1575|\n1576|        # Write header and data to lines list\n1577|        lines = []\n1578|        self.write_header(lines, table.meta)\n1579|        self.data.write(lines)\n1580|\n1581|        return lines\n1582|\n1583|\n1584|class ContinuationLinesInputter(BaseInputter):\n1585|    \"\"\"Inputter where lines ending in ``continuation_char`` are joined with the subsequent line.\n1586|\n1587|    Example::\n1588|\n1589|      col1 col2 col3\n1590|      1 \\\n1591|      2 3\n...\n\n\n\n\n###\n\nPlease provide the class name, function or method name, or the exact line numbers that need to be edited.\nThe possible location outputs should be either \"class\", \"function\" or \"line\".\n\n### Examples:\n```\nfull_path1/file1.py\nline: 10\nclass: MyClass1\nline: 51\n\nfull_path2/file2.py\nfunction: MyClass2.my_method\nline: 12\n\nfull_path3/file3.py\nfunction: my_function\nline: 24\nline: 156\n```\n\nReturn just the location(s) wrapped with ```.\n", "response": ["```\nastropy/io/ascii/qdp.py\nline: 62\nline: 63\nfunction: _line_type\nline: 300\n```", "```\nastropy/io/ascii/qdp.py\nline: 63\nline: 70\nline: 300\n```", "```\nastropy/io/ascii/qdp.py\nline: 63\nline: 300\n```", "```\nastropy/io/ascii/qdp.py\nline: 63\nline: 70\nline: 300\n```"], "usage": {"completion_tokens": 109, "prompt_tokens": 22213}}}
{"instance_id": "astropy__astropy-12907", "found_files": ["astropy/modeling/separable.py", "astropy/modeling/core.py", "astropy/modeling/models.py"], "additional_artifact_loc_file": {}, "file_traj": {}, "found_related_locs": {"astropy/modeling/separable.py": ["function: separability_matrix\nfunction: _coord_matrix\nfunction: _cstack\nfunction: _cdot\nfunction: _separable"], "astropy/modeling/core.py": ["class: CompoundModel\nfunction: CompoundModel._make_leaflist\nfunction: CompoundModel.traverse_postorder\nfunction: CompoundModel._format_expression\nfunction: CompoundModel._format_components"], "astropy/modeling/models.py": [""]}, "additional_artifact_loc_related": [{"raw_output_loc": "```\nastropy/modeling/separable.py\nfunction: separability_matrix\nfunction: _coord_matrix\nfunction: _cstack\nfunction: _cdot\nfunction: _separable\n\nastropy/modeling/core.py\nclass: CompoundModel\nfunction: CompoundModel._make_leaflist\nfunction: CompoundModel.traverse_postorder\nfunction: CompoundModel._format_expression\nfunction: CompoundModel._format_components\n```"}], "related_loc_traj": [{"response": "```\nastropy/modeling/separable.py\nfunction: separability_matrix\nfunction: _coord_matrix\nfunction: _cstack\nfunction: _cdot\nfunction: _separable\n\nastropy/modeling/core.py\nclass: CompoundModel\nfunction: CompoundModel._make_leaflist\nfunction: CompoundModel.traverse_postorder\nfunction: CompoundModel._format_expression\nfunction: CompoundModel._format_components\n```", "usage": {"completion_tokens": 87, "prompt_tokens": 4727}, "prompt": "\nPlease look through the following GitHub Problem Description and the Skeleton of Relevant Files.\nIdentify all locations that need inspection or editing to fix the problem, including directly related areas as well as any potentially related global variables, functions, and classes.\nFor each location you provide, either give the name of the class, the name of a method in a class, the name of a function, or the name of a global variable.\n\n### GitHub Problem Description ###\nModeling's `separability_matrix` does not compute separability correctly for nested CompoundModels\nConsider the following model:\r\n\r\n```python\r\nfrom astropy.modeling import models as m\r\nfrom astropy.modeling.separable import separability_matrix\r\n\r\ncm = m.Linear1D(10) & m.Linear1D(5)\r\n```\r\n\r\nIt's separability matrix as you might expect is a diagonal:\r\n\r\n```python\r\n>>> separability_matrix(cm)\r\narray([[ True, False],\r\n       [False,  True]])\r\n```\r\n\r\nIf I make the model more complex:\r\n```python\r\n>>> separability_matrix(m.Pix2Sky_TAN() & m.Linear1D(10) & m.Linear1D(5))\r\narray([[ True,  True, False, False],\r\n       [ True,  True, False, False],\r\n       [False, False,  True, False],\r\n       [False, False, False,  True]])\r\n```\r\n\r\nThe output matrix is again, as expected, the outputs and inputs to the linear models are separable and independent of each other.\r\n\r\nIf however, I nest these compound models:\r\n```python\r\n>>> separability_matrix(m.Pix2Sky_TAN() & cm)\r\narray([[ True,  True, False, False],\r\n       [ True,  True, False, False],\r\n       [False, False,  True,  True],\r\n       [False, False,  True,  True]])\r\n```\r\nSuddenly the inputs and outputs are no longer separable?\r\n\r\nThis feels like a bug to me, but I might be missing something?\n\n\n### Skeleton of Relevant Files ###\n\n### File: astropy/modeling/separable.py ###\n```python\n# Licensed under a 3-clause BSD style license - see LICENSE.rst\n\n\n\n__all__ = [\"is_separable\", \"separability_matrix\"]\n\n\ndef is_separable(transform):\n...\n\n\ndef separability_matrix(transform):\n...\n\n\ndef _compute_n_outputs(left, right):\n...\n\n\ndef _arith_oper(left, right):\n...\n\n\ndef _coord_matrix(model, pos, noutp):\n...\n\n\ndef _cstack(left, right):\n...\n\n\ndef _cdot(left, right):\n...\n\n\ndef _separable(transform):\n...\n\n\n# Maps modeling operators to a function computing and represents the\n# relationship of axes as an array of 0-es and 1-s\n_operators = {'&': _cstack, '|': _cdot, '+': _arith_oper, '-': _arith_oper,\n              '*': _arith_oper, '/': _arith_oper, '**': _arith_oper}\n\n```\n\n### File: astropy/modeling/core.py ###\n```python\n# Licensed under a 3-clause BSD style license - see LICENSE.rst\n\n\n\n__all__ = ['Model', 'FittableModel', 'Fittable1DModel', 'Fittable2DModel',\n           'CompoundModel', 'fix_inputs', 'custom_model', 'ModelDefinitionError',\n           'bind_bounding_box', 'bind_compound_bounding_box']\n\n\ndef _model_oper(oper, **kwargs):\n...\n\n\nclass ModelDefinitionError(TypeError):\n    pass\n\n\nclass _ModelMeta(abc.ABCMeta):\n\n    _is_dynamic = False\n\n    # Default empty dict for _parameters_, which will be empty on model\n    # classes that don't have any Parameters\n\n    def __new__(mcls, name, bases, members, **kwds):\n...\n\n    def __init__(cls, name, bases, members, **kwds):\n...\n\n    def __repr__(cls):\n...\n\n    def _repr_pretty_(cls, p, cycle):\n...\n\n    def __reduce__(cls):\n...\n\n    @property\n    def name(cls):\n...\n\n    @property\n    def _is_concrete(cls):\n...\n\n    def rename(cls, name=None, inputs=None, outputs=None):\n...\n\n    def _create_inverse_property(cls, members):\n...\n\n    def _create_bounding_box_property(cls, members):\n...\n\n    def _create_bounding_box_subclass(cls, func, sig):\n...\n\n    def _handle_special_methods(cls, members, pdict):\n...\n\n    # *** Arithmetic operators for creating compound models ***\n    __add__ = _model_oper('+')\n    __sub__ = _model_oper('-')\n    __mul__ = _model_oper('*')\n    __truediv__ = _model_oper('/')\n    __pow__ = _model_oper('**')\n    __or__ = _model_oper('|')\n    __and__ = _model_oper('&')\n    _fix_inputs = _model_oper('fix_inputs')\n\n    # *** Other utilities ***\n\n    def _format_cls_repr(cls, keywords=[]):\n...\n\n\nclass Model(metaclass=_ModelMeta):\n\n    parameter_constraints = Parameter.constraints\n\n    model_constraints = ('eqcons', 'ineqcons')\n\n    param_names = ()\n\n    n_inputs = 0\n    n_outputs = 0\n\n    standard_broadcasting = True\n    fittable = False\n    linear = True\n    _separable = None\n    meta = metadata.MetaData()\n\n    # By default models either use their own inverse property or have no\n    # inverse at all, but users may also assign a custom inverse to a model,\n    # optionally; in that case it is of course up to the user to determine\n    # whether their inverse is *actually* an inverse to the model they assign\n    # it to.\n    _inverse = None\n    _user_inverse = None\n\n    _bounding_box = None\n    _user_bounding_box = None\n\n    _has_inverse_bounding_box = False\n\n    # Default n_models attribute, so that __len__ is still defined even when a\n    # model hasn't completed initialization yet\n    _n_models = 1\n\n    # New classes can set this as a boolean value.\n    # It is converted to a dictionary mapping input name to a boolean value.\n    _input_units_strict = False\n\n    # Allow dimensionless input (and corresponding output). If this is True,\n    # input values to evaluate will gain the units specified in input_units. If\n    # this is a dictionary then it should map input name to a bool to allow\n    # dimensionless numbers for that input.\n    # Only has an effect if input_units is defined.\n    _input_units_allow_dimensionless = False\n\n    # Default equivalencies to apply to input values. If set, this should be a\n    # dictionary where each key is a string that corresponds to one of the\n    # model inputs. Only has an effect if input_units is defined.\n    input_units_equivalencies = None\n\n    # Covariance matrix can be set by fitter if available.\n    # If cov_matrix is available, then std will set as well\n    _cov_matrix = None\n    _stds = None\n\n    def __init_subclass__(cls, **kwargs):\n...\n\n    def __init__(self, *args, meta=None, name=None, **kwargs):\n...\n\n    def _default_inputs_outputs(self):\n...\n\n    def _initialize_setters(self, kwargs):\n...\n\n    @property\n    def inputs(self):\n...\n\n    @inputs.setter\n    def inputs(self, val):\n...\n\n    @property\n    def outputs(self):\n...\n\n    @outputs.setter\n    def outputs(self, val):\n...\n\n    @property\n    def n_inputs(self):\n...\n\n    @property\n    def n_outputs(self):\n...\n\n    def _calculate_separability_matrix(self):\n...\n\n    def _initialize_unit_support(self):\n...\n\n    @property\n    def input_units_strict(self):\n...\n\n    @property\n    def input_units_allow_dimensionless(self):\n...\n\n    @property\n    def uses_quantity(self):\n...\n\n    def __repr__(self):\n...\n\n    def __str__(self):\n...\n\n    def __len__(self):\n...\n\n    @staticmethod\n    def _strip_ones(intup):\n...\n\n    def __setattr__(self, attr, value):\n...\n\n    def _pre_evaluate(self, *args, **kwargs):\n...\n\n    def get_bounding_box(self, with_bbox=True):\n...\n\n    @property\n    def _argnames(self):\n...\n\n    def _validate_input_shape(self, _input, idx, argnames, model_set_axis, check_model_set_axis):\n...\n\n    def _validate_input_shapes(self, inputs, argnames, model_set_axis):\n...\n\n    def input_shape(self, inputs):\n...\n\n    def _generic_evaluate(self, evaluate, _inputs, fill_value, with_bbox):\n...\n\n    def _post_evaluate(self, inputs, outputs, broadcasted_shapes, with_bbox, **kwargs):\n...\n\n    @property\n    def bbox_with_units(self):\n...\n\n    def __call__(self, *args, **kwargs):\n...\n\n    def _get_renamed_inputs_as_positional(self, *args, **kwargs):\n...\n\n    # *** Properties ***\n    @property\n    def name(self):\n...\n\n    @name.setter\n    def name(self, val):\n...\n\n    @property\n    def model_set_axis(self):\n...\n\n    @property\n    def param_sets(self):\n...\n\n    @property\n    def parameters(self):\n...\n\n    @parameters.setter\n    def parameters(self, value):\n...\n\n    @property\n    def sync_constraints(self):\n...\n\n    @sync_constraints.setter\n    def sync_constraints(self, value):\n...\n\n    @property\n    def fixed(self):\n...\n\n    @property\n    def bounds(self):\n...\n\n    @property\n    def tied(self):\n...\n\n    @property\n    def eqcons(self):\n...\n\n    @property\n    def ineqcons(self):\n...\n\n    def has_inverse(self):\n...\n\n    @property\n    def inverse(self):\n...\n\n    @inverse.setter\n    def inverse(self, value):\n...\n\n    @inverse.deleter\n    def inverse(self):\n...\n\n    @property\n    def has_user_inverse(self):\n...\n\n    @property\n    def bounding_box(self):\n...\n\n    @bounding_box.setter\n    def bounding_box(self, bounding_box):\n...\n\n    def set_slice_args(self, *args):\n...\n\n    @bounding_box.deleter\n    def bounding_box(self):\n...\n\n    @property\n    def has_user_bounding_box(self):\n...\n\n    @property\n    def cov_matrix(self):\n...\n\n    @cov_matrix.setter\n    def cov_matrix(self, cov):\n...\n\n    @property\n    def stds(self):\n...\n\n    @stds.setter\n    def stds(self, stds):\n...\n\n    @property\n    def separable(self):\n...\n\n    # *** Public methods ***\n\n    def without_units_for_data(self, **kwargs):\n...\n\n    def output_units(self, **kwargs):\n...\n\n    def strip_units_from_tree(self):\n...\n\n    def with_units_from_data(self, **kwargs):\n...\n\n    @property\n    def _has_units(self):\n...\n\n    @property\n    def _supports_unit_fitting(self):\n...\n\n    @abc.abstractmethod\n    def evaluate(self, *args, **kwargs):\n...\n\n    def sum_of_implicit_terms(self, *args, **kwargs):\n...\n\n    def render(self, out=None, coords=None):\n...\n\n    @property\n    def input_units(self):\n...\n\n    @property\n    def return_units(self):\n...\n\n    def _prepare_inputs_single_model(self, params, inputs, **kwargs):\n...\n\n    @staticmethod\n    def _remove_axes_from_shape(shape, axis):\n...\n\n    def _prepare_inputs_model_set(self, params, inputs, model_set_axis_input,\n                                  **kwargs):\n...\n\n    def prepare_inputs(self, *inputs, model_set_axis=None, equivalencies=None,\n                       **kwargs):\n...\n\n    def _validate_input_units(self, inputs, equivalencies=None, inputs_map=None):\n...\n\n    def _process_output_units(self, inputs, outputs):\n...\n\n    @staticmethod\n    def _prepare_output_single_model(output, broadcast_shape):\n...\n\n    def _prepare_outputs_single_model(self, outputs, broadcasted_shapes):\n...\n\n    def _prepare_outputs_model_set(self, outputs, broadcasted_shapes, model_set_axis):\n...\n\n    def prepare_outputs(self, broadcasted_shapes, *outputs, **kwargs):\n...\n\n    def copy(self):\n...\n\n    def deepcopy(self):\n...\n\n    @sharedmethod\n    def rename(self, name):\n...\n\n    def coerce_units(\n        self,\n        input_units=None,\n        return_units=None,\n        input_units_equivalencies=None,\n        input_units_allow_dimensionless=False\n    ):\n...\n\n    @property\n    def n_submodels(self):\n...\n\n    def _initialize_constraints(self, kwargs):\n...\n\n    def _initialize_parameters(self, args, kwargs):\n...\n\n    def _initialize_parameter_value(self, param_name, value):\n...\n\n    def _initialize_slices(self):\n...\n\n    def _parameters_to_array(self):\n...\n\n    def _array_to_parameters(self):\n...\n\n    def _check_param_broadcast(self, max_ndim):\n...\n\n    def _param_sets(self, raw=False, units=False):\n...\n\n    def _format_repr(self, args=[], kwargs={}, defaults={}):\n...\n\n    def _format_str(self, keywords=[], defaults={}):\n...\n\n\nclass FittableModel(Model):\n\n    linear = False\n    # derivative with respect to parameters\n    fit_deriv = None\n    # Flag that indicates if the model derivatives with respect to parameters\n    # are given in columns or rows\n    col_fit_deriv = True\n    fittable = True\n\n\nclass Fittable1DModel(FittableModel):\n    n_inputs = 1\n    n_outputs = 1\n    _separable = True\n\n\nclass Fittable2DModel(FittableModel):\n\n    n_inputs = 2\n    n_outputs = 1\n\n\ndef _make_arithmetic_operator(oper):\n...\n\n\ndef _composition_operator(f, g):\n...\n\n\ndef _join_operator(f, g):\n...\n\n\nBINARY_OPERATORS = {\n    '+': _make_arithmetic_operator(operator.add),\n    '-': _make_arithmetic_operator(operator.sub),\n    '*': _make_arithmetic_operator(operator.mul),\n    '/': _make_arithmetic_operator(operator.truediv),\n    '**': _make_arithmetic_operator(operator.pow),\n    '|': _composition_operator,\n    '&': _join_operator\n}\n\nSPECIAL_OPERATORS = _SpecialOperatorsDict()\n\n\ndef _add_special_operator(sop_name, sop):\n...\n\n\nclass CompoundModel(Model):\n\n    def __init__(self, op, left, right, name=None):\n...\n\n    def _get_left_inputs_from_args(self, args):\n...\n\n    def _get_right_inputs_from_args(self, args):\n...\n\n    def _get_left_params_from_args(self, args):\n...\n\n    def _get_right_params_from_args(self, args):\n...\n\n    def _get_kwarg_model_parameters_as_positional(self, args, kwargs):\n...\n\n    def _apply_operators_to_value_lists(self, leftval, rightval, **kw):\n...\n\n    def evaluate(self, *args, **kw):\n...\n\n    @property\n    def n_submodels(self):\n...\n\n    @property\n    def submodel_names(self):\n...\n\n    def both_inverses_exist(self):\n...\n\n    def _pre_evaluate(self, *args, **kwargs):\n...\n\n    @property\n    def _argnames(self):\n...\n\n    def _post_evaluate(self, inputs, outputs, broadcasted_shapes, with_bbox, **kwargs):\n...\n\n    def _evaluate(self, *args, **kw):\n...\n\n    @property\n    def param_names(self):\n...\n\n    def _make_leaflist(self):\n...\n\n    def __getattr__(self, name):\n...\n\n    def __getitem__(self, index):\n...\n\n    def _str_index_to_int(self, str_index):\n...\n\n    @property\n    def n_inputs(self):\n...\n\n    @n_inputs.setter\n    def n_inputs(self, value):\n...\n\n    @property\n    def n_outputs(self):\n...\n\n    @n_outputs.setter\n    def n_outputs(self, value):\n...\n\n    @property\n    def eqcons(self):\n...\n\n    @eqcons.setter\n    def eqcons(self, value):\n...\n\n    @property\n    def ineqcons(self):\n...\n\n    @ineqcons.setter\n    def ineqcons(self, value):\n...\n\n    def traverse_postorder(self, include_operator=False):\n...\n\n    def _format_expression(self, format_leaf=None):\n...\n\n    def _format_components(self):\n...\n\n    def __str__(self):\n...\n\n    def rename(self, name):\n...\n\n    @property\n    def isleaf(self):\n...\n\n    @property\n    def inverse(self):\n...\n\n    @property\n    def fittable(self):\n...\n\n    __add__ = _model_oper('+')\n    __sub__ = _model_oper('-')\n    __mul__ = _model_oper('*')\n    __truediv__ = _model_oper('/')\n    __pow__ = _model_oper('**')\n    __or__ = _model_oper('|')\n    __and__ = _model_oper('&')\n\n    def _map_parameters(self):\n...\n\n    def _initialize_slices(self):\n...\n\n    @staticmethod\n    def _recursive_lookup(branch, adict, key):\n...\n\n    def inputs_map(self):\n...\n\n    def _parameter_units_for_data_units(self, input_units, output_units):\n...\n\n    @property\n    def input_units(self):\n...\n\n    @property\n    def input_units_equivalencies(self):\n...\n\n    @property\n    def input_units_allow_dimensionless(self):\n...\n\n    @property\n    def input_units_strict(self):\n...\n\n    @property\n    def return_units(self):\n...\n\n    def outputs_map(self):\n...\n\n    @property\n    def has_user_bounding_box(self):\n...\n\n    def render(self, out=None, coords=None):\n...\n\n    def replace_submodel(self, name, model):\n...\n\n    def _set_sub_models_and_parameter_units(self, left, right):\n...\n\n    def without_units_for_data(self, **kwargs):\n...\n\n    def with_units_from_data(self, **kwargs):\n...\n\n\ndef _get_submodel_path(model, name):\n...\n\n\ndef binary_operation(binoperator, left, right):\n...\n\n\ndef get_ops(tree, opset):\n...\n\n\ndef make_subtree_dict(tree, nodepath, tdict, leaflist):\n...\n\n\n_ORDER_OF_OPERATORS = [('fix_inputs',), ('|',), ('&',), ('+', '-'), ('*', '/'), ('**',)]\nOPERATOR_PRECEDENCE = {}\n\n\ndef fix_inputs(modelinstance, values, bounding_boxes=None, selector_args=None):\n...\n\n\ndef bind_bounding_box(modelinstance, bounding_box, ignored=None, order='C'):\n...\n\n\ndef bind_compound_bounding_box(modelinstance, bounding_boxes, selector_args,\n                               create_selector=None, ignored=None, order='C'):\n...\n\n\ndef custom_model(*args, fit_deriv=None):\n...\n\n\ndef _custom_model_inputs(func):\n...\n\n\ndef _custom_model_wrapper(func, fit_deriv=None):\n...\n\n\ndef render_model(model, arr=None, coords=None):\n...\n\n\ndef hide_inverse(model):\n...\n\n```\n\n### File: astropy/modeling/models.py ###\n```python\n# Licensed under a 3-clause BSD style license - see LICENSE.rst\n\n\n\n# Attach a docstring explaining constraints to all models which support them.\n# Note: add new models to this list\n\nCONSTRAINTS_DOC = \"\"\"\n    Other Parameters\n    ----------------\n    fixed : a dict, optional\n        A dictionary ``{parameter_name: boolean}`` of parameters to not be\n        varied during fitting. True means the parameter is held fixed.\n        Alternatively the `~astropy.modeling.Parameter.fixed`\n        property of a parameter may be used.\n    tied : dict, optional\n        A dictionary ``{parameter_name: callable}`` of parameters which are\n        linked to some other parameter. The dictionary values are callables\n        providing the linking relationship.  Alternatively the\n        `~astropy.modeling.Parameter.tied` property of a parameter\n        may be used.\n    bounds : dict, optional\n        A dictionary ``{parameter_name: value}`` of lower and upper bounds of\n        parameters. Keys are parameter names. Values are a list or a tuple\n        of length 2 giving the desired range for the parameter.\n        Alternatively, the\n        `~astropy.modeling.Parameter.min` and\n        `~astropy.modeling.Parameter.max` properties of a parameter\n        may be used.\n    eqcons : list, optional\n        A list of functions of length ``n`` such that ``eqcons[j](x0,*args) ==\n        0.0`` in a successfully optimized problem.\n    ineqcons : list, optional\n        A list of functions of length ``n`` such that ``ieqcons[j](x0,*args) >=\n        0.0`` is a successfully optimized problem.\n\"\"\"\n\n\nMODELS_WITH_CONSTRAINTS = [\n    AiryDisk2D, Moffat1D, Moffat2D, Box1D, Box2D,\n    Const1D, Const2D, Ellipse2D, Disk2D,\n    Gaussian1D, Gaussian2D,\n    Linear1D, Lorentz1D, RickerWavelet1D, RickerWavelet2D,\n    PowerLaw1D, Sersic1D, Sersic2D,\n    Sine1D, Cosine1D, Tangent1D, ArcSine1D, ArcCosine1D, ArcTangent1D,\n    Trapezoid1D, TrapezoidDisk2D,\n    Chebyshev1D, Chebyshev2D, Hermite1D, Hermite2D, Legendre2D, Legendre1D,\n    Polynomial1D, Polynomial2D, Voigt1D, KingProjectedAnalytic1D,\n    NFW\n]\n\n```\n\n\n###\n\nPlease provide the complete set of locations as either a class name, a function name, or a variable name.\nNote that if you include a class, you do not need to list its specific methods.\nYou can include either the entire class or don't include the class name and instead include specific methods in the class.\n### Examples:\n```\nfull_path1/file1.py\nfunction: my_function_1\nclass: MyClass1\nfunction: MyClass2.my_method\n\nfull_path2/file2.py\nvariable: my_var\nfunction: MyClass3.my_method\n\nfull_path3/file3.py\nfunction: my_function_2\nfunction: my_function_3\nfunction: MyClass4.my_method_1\nclass: MyClass5\n```\n\nReturn just the locations wrapped with ```.\n"}], "found_edit_locs": {"astropy/modeling/separable.py": ["\nfunction: _coord_matrix\nline: 199\nline: 200\nline: 207\nline: 208"]}, "additional_artifact_loc_edit_location": [{"raw_output_loc": ["```\nastropy/modeling/separable.py\nfunction: _coord_matrix\nline: 199\nline: 200\nline: 207\nline: 208\n```", "```\nastropy/modeling/separable.py\nline: 67\nfunction: separability_matrix\nline: 69\nline: 100\nline: 101\n\nastropy/modeling/separable.py\nline: 290\nfunction: _separable\nline: 306\nline: 307\nline: 308\n\nastropy/modeling/separable.py\nline: 305\nfunction: _separable\nline: 306\n\nastropy/modeling/core.py\nclass: CompoundModel\nfunction: CompoundModel.__init__\nline: 2902\n\nastropy/modeling/core.py\nclass: CompoundModel\nfunction: CompoundModel._apply_operators_to_value_lists\nline: 3082\nline: 3092\nline: 3100\n```", "```\nastropy/modeling/separable.py\nfunction: _coord_matrix\nline: 189\nline: 202\nline: 206\nline: 210\n\nfunction: _cstack\nline: 237\nline: 242\n\nfunction: _separable\nline: 307\nline: 308\n\nastropy/modeling/core.py\nclass: CompoundModel\nline: 294\n```", "```\nastropy/modeling/separable.py\nline: 290\nline: 307\n```"]}], "edit_loc_traj": {"prompt": "\nPlease review the following GitHub problem description and relevant files, and provide a set of locations that need to be edited to fix the issue.\nThe locations can be specified as class names, function or method names, or exact line numbers that require modification.\n\n### GitHub Problem Description ###\nModeling's `separability_matrix` does not compute separability correctly for nested CompoundModels\nConsider the following model:\r\n\r\n```python\r\nfrom astropy.modeling import models as m\r\nfrom astropy.modeling.separable import separability_matrix\r\n\r\ncm = m.Linear1D(10) & m.Linear1D(5)\r\n```\r\n\r\nIt's separability matrix as you might expect is a diagonal:\r\n\r\n```python\r\n>>> separability_matrix(cm)\r\narray([[ True, False],\r\n       [False,  True]])\r\n```\r\n\r\nIf I make the model more complex:\r\n```python\r\n>>> separability_matrix(m.Pix2Sky_TAN() & m.Linear1D(10) & m.Linear1D(5))\r\narray([[ True,  True, False, False],\r\n       [ True,  True, False, False],\r\n       [False, False,  True, False],\r\n       [False, False, False,  True]])\r\n```\r\n\r\nThe output matrix is again, as expected, the outputs and inputs to the linear models are separable and independent of each other.\r\n\r\nIf however, I nest these compound models:\r\n```python\r\n>>> separability_matrix(m.Pix2Sky_TAN() & cm)\r\narray([[ True,  True, False, False],\r\n       [ True,  True, False, False],\r\n       [False, False,  True,  True],\r\n       [False, False,  True,  True]])\r\n```\r\nSuddenly the inputs and outputs are no longer separable?\r\n\r\nThis feels like a bug to me, but I might be missing something?\n\n\n###\n### astropy/modeling/separable.py\n...\n56|    \"\"\"\n57|    if transform.n_inputs == 1 and transform.n_outputs > 1:\n58|        is_separable = np.array([False] * transform.n_outputs).T\n59|        return is_separable\n60|    separable_matrix = _separable(transform)\n61|    is_separable = separable_matrix.sum(1)\n62|    is_separable = np.where(is_separable != 1, False, True)\n63|    return is_separable\n64|\n65|\n66|def separability_matrix(transform):\n67|    \"\"\"\n68|    Compute the correlation between outputs and inputs.\n69|\n70|    Parameters\n71|    ----------\n72|    transform : `~astropy.modeling.core.Model`\n73|        A (compound) model.\n74|\n75|    Returns\n76|    -------\n77|    separable_matrix : ndarray\n78|        A boolean correlation matrix of shape (n_outputs, n_inputs).\n79|        Indicates the dependence of outputs on inputs. For completely\n80|        independent outputs, the diagonal elements are True and\n81|        off-diagonal elements are False.\n82|\n83|    Examples\n84|    --------\n85|    >>> from astropy.modeling.models import Shift, Scale, Rotation2D, Polynomial2D\n86|    >>> separability_matrix(Shift(1) & Shift(2) | Scale(1) & Scale(2))\n87|        array([[ True, False], [False,  True]]...)\n88|    >>> separability_matrix(Shift(1) & Shift(2) | Rotation2D(2))\n89|        array([[ True,  True], [ True,  True]]...)\n90|    >>> separability_matrix(Shift(1) & Shift(2) | Mapping([0, 1, 0, 1]) | \\\n91|        Polynomial2D(1) & Polynomial2D(2))\n92|        array([[ True,  True], [ True,  True]]...)\n93|    >>> separability_matrix(Shift(1) & Shift(2) | Mapping([0, 1, 0, 1]))\n94|        array([[ True, False], [False,  True], [ True, False], [False,  True]]...)\n95|\n96|    \"\"\"\n97|    if transform.n_inputs == 1 and transform.n_outputs > 1:\n98|        return np.ones((transform.n_outputs, transform.n_inputs),\n99|                       dtype=np.bool_)\n100|    separable_matrix = _separable(transform)\n101|    separable_matrix = np.where(separable_matrix != 0, True, False)\n102|    return separable_matrix\n103|\n104|\n105|def _compute_n_outputs(left, right):\n106|    \"\"\"\n107|    Compute the number of outputs of two models.\n108|\n109|    The two models are the left and right model to an operation in\n110|    the expression tree of a compound model.\n111|\n112|    Parameters\n...\n161|            \"Unsupported operands for arithmetic operator: left (n_inputs={}, \"\n162|            \"n_outputs={}) and right (n_inputs={}, n_outputs={}); \"\n163|            \"models must have the same n_inputs and the same \"\n164|            \"n_outputs for this operator.\".format(\n165|                left_inputs, left_outputs, right_inputs, right_outputs))\n166|\n167|    result = np.ones((left_outputs, left_inputs))\n168|    return result\n169|\n170|\n171|def _coord_matrix(model, pos, noutp):\n172|    \"\"\"\n173|    Create an array representing inputs and outputs of a simple model.\n174|\n175|    The array has a shape (noutp, model.n_inputs).\n176|\n177|    Parameters\n178|    ----------\n179|    model : `astropy.modeling.Model`\n180|        model\n181|    pos : str\n182|        Position of this model in the expression tree.\n183|        One of ['left', 'right'].\n184|    noutp : int\n185|        Number of outputs of the compound model of which the input model\n186|        is a left or right child.\n187|\n188|    \"\"\"\n189|    if isinstance(model, Mapping):\n190|        axes = []\n191|        for i in model.mapping:\n192|            axis = np.zeros((model.n_inputs,))\n193|            axis[i] = 1\n194|            axes.append(axis)\n195|        m = np.vstack(axes)\n196|        mat = np.zeros((noutp, model.n_inputs))\n197|        if pos == 'left':\n198|            mat[: model.n_outputs, :model.n_inputs] = m\n199|        else:\n200|            mat[-model.n_outputs:, -model.n_inputs:] = m\n201|        return mat\n202|    if not model.separable:\n203|        # this does not work for more than 2 coordinates\n204|        mat = np.zeros((noutp, model.n_inputs))\n205|        if pos == 'left':\n206|            mat[:model.n_outputs, : model.n_inputs] = 1\n207|        else:\n208|            mat[-model.n_outputs:, -model.n_inputs:] = 1\n209|    else:\n210|        mat = np.zeros((noutp, model.n_inputs))\n211|\n212|        for i in range(model.n_inputs):\n213|            mat[i, i] = 1\n214|        if pos == 'right':\n215|            mat = np.roll(mat, (noutp - model.n_outputs))\n216|    return mat\n217|\n218|\n219|def _cstack(left, right):\n220|    \"\"\"\n221|    Function corresponding to '&' operation.\n222|\n223|    Parameters\n224|    ----------\n225|    left, right : `astropy.modeling.Model` or ndarray\n226|        If input is of an array, it is the output of `coord_matrix`.\n227|\n228|    Returns\n229|    -------\n230|    result : ndarray\n231|        Result from this operation.\n232|\n233|    \"\"\"\n234|    noutp = _compute_n_outputs(left, right)\n235|\n236|    if isinstance(left, Model):\n237|        cleft = _coord_matrix(left, 'left', noutp)\n238|    else:\n239|        cleft = np.zeros((noutp, left.shape[1]))\n240|        cleft[: left.shape[0], : left.shape[1]] = left\n241|    if isinstance(right, Model):\n242|        cright = _coord_matrix(right, 'right', noutp)\n243|    else:\n244|        cright = np.zeros((noutp, right.shape[1]))\n245|        cright[-right.shape[0]:, -right.shape[1]:] = 1\n246|\n247|    return np.hstack([cleft, cright])\n248|\n249|\n250|def _cdot(left, right):\n251|    \"\"\"\n252|    Function corresponding to \"|\" operation.\n253|\n254|    Parameters\n255|    ----------\n256|    left, right : `astropy.modeling.Model` or ndarray\n257|        If input is of an array, it is the output of `coord_matrix`.\n258|\n259|    Returns\n260|    -------\n261|    result : ndarray\n262|        Result from this operation.\n263|    \"\"\"\n264|\n265|    left, right = right, left\n266|\n267|    def _n_inputs_outputs(input, position):\n268|        \"\"\"\n269|        Return ``n_inputs``, ``n_outputs`` for a model or coord_matrix.\n270|        \"\"\"\n271|        if isinstance(input, Model):\n272|            coords = _coord_matrix(input, position, input.n_outputs)\n273|        else:\n274|            coords = input\n275|        return coords\n276|\n277|    cleft = _n_inputs_outputs(left, 'left')\n278|    cright = _n_inputs_outputs(right, 'right')\n279|\n280|    try:\n281|        result = np.dot(cleft, cright)\n282|    except ValueError:\n283|        raise ModelDefinitionError(\n284|            'Models cannot be combined with the \"|\" operator; '\n285|            'left coord_matrix is {}, right coord_matrix is {}'.format(\n286|                cright, cleft))\n287|    return result\n288|\n289|\n290|def _separable(transform):\n291|    \"\"\"\n292|    Calculate the separability of outputs.\n293|\n294|    Parameters\n295|    ----------\n296|    transform : `astropy.modeling.Model`\n297|        A transform (usually a compound model).\n298|\n299|    Returns :\n300|    is_separable : ndarray of dtype np.bool\n301|        An array of shape (transform.n_outputs,) of boolean type\n302|        Each element represents the separablity of the corresponding output.\n303|    \"\"\"\n304|    if (transform_matrix := transform._calculate_separability_matrix()) is not NotImplemented:\n305|        return transform_matrix\n306|    elif isinstance(transform, CompoundModel):\n307|        sepleft = _separable(transform.left)\n308|        sepright = _separable(transform.right)\n309|        return _operators[transform.op](sepleft, sepright)\n310|    elif isinstance(transform, Model):\n311|        return _coord_matrix(transform, 'left', transform.n_outputs)\n312|\n313|\n314|# Maps modeling operators to a function computing and represents the\n315|# relationship of axes as an array of 0-es and 1-s\n316|_operators = {'&': _cstack, '|': _cdot, '+': _arith_oper, '-': _arith_oper,\n317|              '*': _arith_oper, '/': _arith_oper, '**': _arith_oper}\n\n\n### astropy/modeling/core.py\n...\n2884|    '&': _join_operator\n2885|}\n2886|\n2887|SPECIAL_OPERATORS = _SpecialOperatorsDict()\n2888|\n2889|\n2890|def _add_special_operator(sop_name, sop):\n2891|    return SPECIAL_OPERATORS.add(sop_name, sop)\n2892|\n2893|\n2894|class CompoundModel(Model):\n2895|    '''\n2896|    Base class for compound models.\n2897|\n2898|    While it can be used directly, the recommended way\n2899|    to combine models is through the model operators.\n2900|    '''\n2901|\n2902|    def __init__(self, op, left, right, name=None):\n2903|        self.__dict__['_param_names'] = None\n2904|        self._n_submodels = None\n2905|        self.op = op\n2906|        self.left = left\n2907|        self.right = right\n2908|        self._bounding_box = None\n2909|        self._user_bounding_box = None\n2910|        self._leaflist = None\n2911|        self._tdict = None\n2912|        self._parameters = None\n2913|        self._parameters_ = None\n2914|        self._param_metrics = None\n2915|\n2916|        if op != 'fix_inputs' and len(left) != len(right):\n2917|            raise ValueError(\n2918|                'Both operands must have equal values for n_models')\n2919|        self._n_models = len(left)\n2920|\n2921|        if op != 'fix_inputs' and ((left.model_set_axis != right.model_set_axis)\n2922|                                   or left.model_set_axis):  # not False and not 0\n2923|            raise ValueError(\"model_set_axis must be False or 0 and consistent for operands\")\n2924|        self._model_set_axis = left.model_set_axis\n2925|\n2926|        if op in ['+', '-', '*', '/', '**'] or op in SPECIAL_OPERATORS:\n2927|            if (left.n_inputs != right.n_inputs) or \\\n2928|               (left.n_outputs != right.n_outputs):\n2929|                raise ModelDefinitionError(\n2930|                    'Both operands must match numbers of inputs and outputs')\n2931|            self.n_inputs = left.n_inputs\n2932|            self.n_outputs = left.n_outputs\n2933|            self.inputs = left.inputs\n2934|            self.outputs = left.outputs\n2935|        elif op == '&':\n2936|            self.n_inputs = left.n_inputs + right.n_inputs\n2937|            self.n_outputs = left.n_outputs + right.n_outputs\n2938|            self.inputs = combine_labels(left.inputs, right.inputs)\n2939|            self.outputs = combine_labels(left.outputs, right.outputs)\n2940|        elif op == '|':\n2941|            if left.n_outputs != right.n_inputs:\n2942|                raise ModelDefinitionError(\n2943|                    \"Unsupported operands for |: {0} (n_inputs={1}, \"\n2944|                    \"n_outputs={2}) and {3} (n_inputs={4}, n_outputs={5}); \"\n2945|                    \"n_outputs for the left-hand model must match n_inputs \"\n2946|                    \"for the right-hand model.\".format(\n2947|                        left.name, left.n_inputs, left.n_outputs, right.name,\n2948|                        right.n_inputs, right.n_outputs))\n2949|\n2950|            self.n_inputs = left.n_inputs\n2951|            self.n_outputs = right.n_outputs\n2952|            self.inputs = left.inputs\n2953|            self.outputs = right.outputs\n2954|        elif op == 'fix_inputs':\n2955|            if not isinstance(left, Model):\n2956|                raise ValueError('First argument to \"fix_inputs\" must be an instance of an astropy Model.')\n2957|            if not isinstance(right, dict):\n2958|                raise ValueError('Expected a dictionary for second argument of \"fix_inputs\".')\n2959|\n2960|            # Dict keys must match either possible indices\n2961|            # for model on left side, or names for inputs.\n2962|            self.n_inputs = left.n_inputs - len(right)\n2963|            # Assign directly to the private attribute (instead of using the setter)\n2964|            # to avoid asserting the new number of outputs matches the old one.\n2965|            self._outputs = left.outputs\n2966|            self.n_outputs = left.n_outputs\n2967|            newinputs = list(left.inputs)\n2968|            keys = right.keys()\n2969|            input_ind = []\n2970|            for key in keys:\n2971|                if np.issubdtype(type(key), np.integer):\n2972|                    if key >= left.n_inputs or key < 0:\n2973|                        raise ValueError(\n2974|                            'Substitution key integer value '\n2975|                            'not among possible input choices.')\n2976|                    if key in input_ind:\n2977|                        raise ValueError(\"Duplicate specification of \"\n2978|                                         \"same input (index/name).\")\n2979|                    input_ind.append(key)\n2980|                elif isinstance(key, str):\n2981|                    if key not in left.inputs:\n2982|                        raise ValueError(\n2983|                            'Substitution key string not among possible '\n2984|                            'input choices.')\n2985|                    # Check to see it doesn't match positional\n2986|                    # specification.\n2987|                    ind = left.inputs.index(key)\n2988|                    if ind in input_ind:\n2989|                        raise ValueError(\"Duplicate specification of \"\n2990|                                         \"same input (index/name).\")\n2991|                    input_ind.append(ind)\n2992|            # Remove substituted inputs\n2993|            input_ind.sort()\n2994|            input_ind.reverse()\n2995|            for ind in input_ind:\n2996|                del newinputs[ind]\n2997|            self.inputs = tuple(newinputs)\n2998|            # Now check to see if the input model has bounding_box defined.\n2999|            # If so, remove the appropriate dimensions and set it for this\n3000|            # instance.\n3001|            try:\n3002|                self.bounding_box = \\\n3003|                    self.left.bounding_box.fix_inputs(self, right)\n3004|            except NotImplementedError:\n3005|                pass\n3006|\n3007|        else:\n3008|            raise ModelDefinitionError('Illegal operator: ', self.op)\n3009|        self.name = name\n3010|        self._fittable = None\n3011|        self.fit_deriv = None\n3012|        self.col_fit_deriv = None\n3013|        if op in ('|', '+', '-'):\n3014|            self.linear = left.linear and right.linear\n3015|        else:\n3016|            self.linear = False\n3017|        self.eqcons = []\n3018|        self.ineqcons = []\n3019|        self.n_left_params = len(self.left.parameters)\n3020|        self._map_parameters()\n3021|\n3022|    def _get_left_inputs_from_args(self, args):\n3023|        return args[:self.left.n_inputs]\n3024|\n3025|    def _get_right_inputs_from_args(self, args):\n3026|        op = self.op\n3027|        if op == '&':\n3028|            # Args expected to look like (*left inputs, *right inputs, *left params, *right params)\n3029|            return args[self.left.n_inputs: self.left.n_inputs + self.right.n_inputs]\n3030|        elif op == '|' or  op == 'fix_inputs':\n3031|            return None\n3032|        else:\n3033|            return args[:self.left.n_inputs]\n3034|\n3035|    def _get_left_params_from_args(self, args):\n3036|        op = self.op\n3037|        if op == '&':\n3038|            # Args expected to look like (*left inputs, *right inputs, *left params, *right params)\n3039|            n_inputs = self.left.n_inputs + self.right.n_inputs\n3040|            return args[n_inputs: n_inputs + self.n_left_params]\n3041|        else:\n3042|            return args[self.left.n_inputs: self.left.n_inputs + self.n_left_params]\n3043|\n3044|    def _get_right_params_from_args(self, args):\n3045|        op = self.op\n3046|        if op == 'fix_inputs':\n3047|            return None\n3048|        if op == '&':\n3049|            # Args expected to look like (*left inputs, *right inputs, *left params, *right params)\n3050|            return args[self.left.n_inputs + self.right.n_inputs + self.n_left_params:]\n3051|        else:\n3052|            return args[self.left.n_inputs + self.n_left_params:]\n3053|\n3054|    def _get_kwarg_model_parameters_as_positional(self, args, kwargs):\n3055|        # could do it with inserts but rebuilding seems like simpilist way\n3056|\n3057|        #TODO: Check if any param names are in kwargs maybe as an intersection of sets?\n3058|        if self.op == \"&\":\n3059|            new_args = list(args[:self.left.n_inputs + self.right.n_inputs])\n3060|            args_pos = self.left.n_inputs + self.right.n_inputs\n3061|        else:\n3062|            new_args = list(args[:self.left.n_inputs])\n3063|            args_pos = self.left.n_inputs\n3064|\n3065|        for param_name in self.param_names:\n3066|            kw_value = kwargs.pop(param_name, None)\n3067|            if kw_value is not None:\n3068|                value = kw_value\n3069|            else:\n3070|                try:\n3071|                    value = args[args_pos]\n3072|                except IndexError:\n3073|                    raise IndexError(\"Missing parameter or input\")\n3074|\n3075|                args_pos += 1\n3076|            new_args.append(value)\n3077|\n3078|        return new_args, kwargs\n3079|\n3080|    def _apply_operators_to_value_lists(self, leftval, rightval, **kw):\n3081|        op = self.op\n3082|        if op == '+':\n3083|            return binary_operation(operator.add, leftval, rightval)\n3084|        elif op == '-':\n3085|            return binary_operation(operator.sub, leftval, rightval)\n3086|        elif op == '*':\n3087|            return binary_operation(operator.mul, leftval, rightval)\n3088|        elif op == '/':\n3089|            return binary_operation(operator.truediv, leftval, rightval)\n3090|        elif op == '**':\n3091|            return binary_operation(operator.pow, leftval, rightval)\n3092|        elif op == '&':\n3093|            if not isinstance(leftval, tuple):\n3094|                leftval = (leftval,)\n3095|            if not isinstance(rightval, tuple):\n3096|                rightval = (rightval,)\n3097|            return leftval + rightval\n3098|        elif op in SPECIAL_OPERATORS:\n3099|            return binary_operation(SPECIAL_OPERATORS[op], leftval, rightval)\n3100|        else:\n3101|            raise ModelDefinitionError('Unrecognized operator {op}')\n3102|\n3103|    def evaluate(self, *args, **kw):\n3104|        op = self.op\n3105|        args, kw = self._get_kwarg_model_parameters_as_positional(args, kw)\n3106|        left_inputs = self._get_left_inputs_from_args(args)\n3107|        left_params = self._get_left_params_from_args(args)\n3108|\n3109|        if op == 'fix_inputs':\n3110|            pos_index = dict(zip(self.left.inputs, range(self.left.n_inputs)))\n3111|            fixed_inputs = {\n3112|                key if np.issubdtype(type(key), np.integer) else pos_index[key]: value\n3113|                for key, value in self.right.items()\n3114|            }\n3115|            left_inputs = [\n3116|                fixed_inputs[ind] if ind in fixed_inputs.keys() else inp\n3117|                for ind, inp in enumerate(left_inputs)\n3118|            ]\n3119|\n3120|        leftval = self.left.evaluate(*itertools.chain(left_inputs, left_params))\n3121|\n3122|        if op == 'fix_inputs':\n3123|            return leftval\n3124|\n3125|        right_inputs = self._get_right_inputs_from_args(args)\n3126|        right_params = self._get_right_params_from_args(args)\n3127|\n3128|        if op == \"|\":\n3129|            if isinstance(leftval, tuple):\n3130|                return self.right.evaluate(*itertools.chain(leftval, right_params))\n3131|            else:\n3132|                return self.right.evaluate(leftval, *right_params)\n3133|        else:\n3134|            rightval = self.right.evaluate(*itertools.chain(right_inputs, right_params))\n3135|\n3136|        return self._apply_operators_to_value_lists(leftval, rightval, **kw)\n3137|\n3138|    @property\n3139|    def n_submodels(self):\n3140|        if self._leaflist is None:\n3141|            self._make_leaflist()\n3142|        return len(self._leaflist)\n3143|\n3144|    @property\n3145|    def submodel_names(self):\n3146|        \"\"\" Return the names of submodels in a ``CompoundModel``.\"\"\"\n3147|        if self._leaflist is None:\n3148|            self._make_leaflist()\n3149|        names = [item.name for item in self._leaflist]\n3150|        nonecount = 0\n3151|        newnames = []\n3152|        for item in names:\n3153|            if item is None:\n3154|                newnames.append(f'None_{nonecount}')\n3155|                nonecount += 1\n3156|            else:\n3157|                newnames.append(item)\n3158|        return tuple(newnames)\n3159|\n3160|    def both_inverses_exist(self):\n3161|        '''\n3162|        if both members of this compound model have inverses return True\n3163|        '''\n3164|        warnings.warn(\n3165|            \"CompoundModel.both_inverses_exist is deprecated. \"\n3166|            \"Use has_inverse instead.\",\n3167|            AstropyDeprecationWarning\n3168|        )\n3169|\n3170|        try:\n3171|            linv = self.left.inverse\n3172|            rinv = self.right.inverse\n3173|        except NotImplementedError:\n3174|            return False\n3175|\n3176|        return True\n3177|\n3178|    def _pre_evaluate(self, *args, **kwargs):\n3179|        \"\"\"\n3180|        CompoundModel specific input setup that needs to occur prior to\n3181|            model evaluation.\n3182|\n3183|        Note\n3184|        ----\n3185|            All of the _pre_evaluate for each component model will be\n3186|            performed at the time that the individual model is evaluated.\n3187|        \"\"\"\n3188|\n3189|        # If equivalencies are provided, necessary to map parameters and pass\n3190|        # the leaflist as a keyword input for use by model evaluation so that\n3191|        # the compound model input names can be matched to the model input\n3192|        # names.\n3193|        if 'equivalencies' in kwargs:\n3194|            # Restructure to be useful for the individual model lookup\n3195|            kwargs['inputs_map'] = [(value[0], (value[1], key)) for\n3196|                                    key, value in self.inputs_map().items()]\n3197|\n3198|        # Setup actual model evaluation method\n3199|        def evaluate(_inputs):\n3200|            return self._evaluate(*_inputs, **kwargs)\n3201|\n3202|        return evaluate, args, None, kwargs\n3203|\n3204|    @property\n3205|    def _argnames(self):\n3206|        \"\"\"No inputs should be used to determine input_shape when handling compound models\"\"\"\n3207|        return ()\n3208|\n3209|    def _post_evaluate(self, inputs, outputs, broadcasted_shapes, with_bbox, **kwargs):\n3210|        \"\"\"\n3211|        CompoundModel specific post evaluation processing of outputs\n3212|\n3213|        Note\n3214|        ----\n3215|            All of the _post_evaluate for each component model will be\n3216|            performed at the time that the individual model is evaluated.\n3217|        \"\"\"\n3218|        if self.get_bounding_box(with_bbox) is not None and self.n_outputs == 1:\n3219|            return outputs[0]\n3220|        return outputs\n3221|\n3222|    def _evaluate(self, *args, **kw):\n3223|        op = self.op\n3224|        if op != 'fix_inputs':\n3225|            if op != '&':\n3226|                leftval = self.left(*args, **kw)\n3227|                if op != '|':\n3228|                    rightval = self.right(*args, **kw)\n3229|                else:\n3230|                    rightval = None\n3231|\n3232|            else:\n3233|                leftval = self.left(*(args[:self.left.n_inputs]), **kw)\n3234|                rightval = self.right(*(args[self.left.n_inputs:]), **kw)\n3235|\n3236|            if op != \"|\":\n3237|                return self._apply_operators_to_value_lists(leftval, rightval, **kw)\n3238|\n3239|            elif op == '|':\n3240|                if isinstance(leftval, tuple):\n3241|                    return self.right(*leftval, **kw)\n3242|                else:\n3243|                    return self.right(leftval, **kw)\n3244|\n3245|        else:\n3246|            subs = self.right\n3247|            newargs = list(args)\n3248|            subinds = []\n3249|            subvals = []\n3250|            for key in subs.keys():\n3251|                if np.issubdtype(type(key), np.integer):\n3252|                    subinds.append(key)\n3253|                elif isinstance(key, str):\n3254|                    ind = self.left.inputs.index(key)\n3255|                    subinds.append(ind)\n3256|                subvals.append(subs[key])\n3257|            # Turn inputs specified in kw into positional indices.\n3258|            # Names for compound inputs do not propagate to sub models.\n3259|            kwind = []\n3260|            kwval = []\n3261|            for kwkey in list(kw.keys()):\n3262|                if kwkey in self.inputs:\n3263|                    ind = self.inputs.index(kwkey)\n3264|                    if ind < len(args):\n3265|                        raise ValueError(\"Keyword argument duplicates \"\n3266|                                         \"positional value supplied.\")\n3267|                    kwind.append(ind)\n3268|                    kwval.append(kw[kwkey])\n3269|                    del kw[kwkey]\n3270|            # Build new argument list\n3271|            # Append keyword specified args first\n3272|            if kwind:\n3273|                kwargs = list(zip(kwind, kwval))\n3274|                kwargs.sort()\n3275|                kwindsorted, kwvalsorted = list(zip(*kwargs))\n3276|                newargs = newargs + list(kwvalsorted)\n3277|            if subinds:\n3278|                subargs = list(zip(subinds, subvals))\n3279|                subargs.sort()\n3280|                # subindsorted, subvalsorted = list(zip(*subargs))\n3281|                # The substitutions must be inserted in order\n3282|                for ind, val in subargs:\n3283|                    newargs.insert(ind, val)\n3284|            return self.left(*newargs, **kw)\n3285|\n3286|    @property\n3287|    def param_names(self):\n3288|        \"\"\" An ordered list of parameter names.\"\"\"\n3289|        return self._param_names\n3290|\n3291|    def _make_leaflist(self):\n3292|        tdict = {}\n3293|        leaflist = []\n3294|        make_subtree_dict(self, '', tdict, leaflist)\n3295|        self._leaflist = leaflist\n3296|        self._tdict = tdict\n3297|\n3298|    def __getattr__(self, name):\n3299|        \"\"\"\n3300|        If someone accesses an attribute not already defined, map the\n3301|        parameters, and then see if the requested attribute is one of\n3302|        the parameters\n3303|        \"\"\"\n3304|        # The following test is needed to avoid infinite recursion\n3305|        # caused by deepcopy. There may be other such cases discovered.\n3306|        if name == '__setstate__':\n3307|            raise AttributeError\n3308|        if name in self._param_names:\n3309|            return self.__dict__[name]\n3310|        else:\n3311|            raise AttributeError(f'Attribute \"{name}\" not found')\n3312|\n3313|    def __getitem__(self, index):\n3314|        if self._leaflist is None:\n3315|            self._make_leaflist()\n3316|        leaflist = self._leaflist\n3317|        tdict = self._tdict\n3318|        if isinstance(index, slice):\n3319|            if index.step:\n3320|                raise ValueError('Steps in slices not supported '\n3321|                                 'for compound models')\n3322|            if index.start is not None:\n3323|                if isinstance(index.start, str):\n3324|                    start = self._str_index_to_int(index.start)\n3325|                else:\n3326|                    start = index.start\n3327|            else:\n3328|                start = 0\n3329|            if index.stop is not None:\n3330|                if isinstance(index.stop, str):\n3331|                    stop = self._str_index_to_int(index.stop)\n3332|                else:\n3333|                    stop = index.stop - 1\n3334|            else:\n3335|                stop = len(leaflist) - 1\n3336|            if index.stop == 0:\n3337|                raise ValueError(\"Slice endpoint cannot be 0\")\n3338|            if start < 0:\n3339|                start = len(leaflist) + start\n3340|            if stop < 0:\n3341|                stop = len(leaflist) + stop\n3342|            # now search for matching node:\n3343|            if stop == start:  # only single value, get leaf instead in code below\n3344|                index = start\n3345|            else:\n3346|                for key in tdict:\n3347|                    node, leftind, rightind = tdict[key]\n3348|                    if leftind == start and rightind == stop:\n3349|                        return node\n3350|                raise IndexError(\"No appropriate subtree matches slice\")\n3351|        if isinstance(index, type(0)):\n3352|            return leaflist[index]\n3353|        elif isinstance(index, type('')):\n3354|            return leaflist[self._str_index_to_int(index)]\n3355|        else:\n3356|            raise TypeError('index must be integer, slice, or model name string')\n3357|\n3358|    def _str_index_to_int(self, str_index):\n3359|        # Search through leaflist for item with that name\n3360|        found = []\n3361|        for nleaf, leaf in enumerate(self._leaflist):\n3362|            if getattr(leaf, 'name', None) == str_index:\n3363|                found.append(nleaf)\n3364|        if len(found) == 0:\n3365|            raise IndexError(f\"No component with name '{str_index}' found\")\n3366|        if len(found) > 1:\n3367|            raise IndexError(\"Multiple components found using '{}' as name\\n\"\n3368|                             \"at indices {}\".format(str_index, found))\n3369|        return found[0]\n3370|\n3371|    @property\n3372|    def n_inputs(self):\n3373|        \"\"\" The number of inputs of a model.\"\"\"\n3374|        return self._n_inputs\n3375|\n3376|    @n_inputs.setter\n3377|    def n_inputs(self, value):\n3378|        self._n_inputs = value\n3379|\n3380|    @property\n3381|    def n_outputs(self):\n3382|        \"\"\" The number of outputs of a model.\"\"\"\n3383|        return self._n_outputs\n3384|\n3385|    @n_outputs.setter\n3386|    def n_outputs(self, value):\n3387|        self._n_outputs = value\n3388|\n3389|    @property\n3390|    def eqcons(self):\n3391|        return self._eqcons\n3392|\n3393|    @eqcons.setter\n3394|    def eqcons(self, value):\n3395|        self._eqcons = value\n3396|\n3397|    @property\n3398|    def ineqcons(self):\n3399|        return self._eqcons\n3400|\n3401|    @ineqcons.setter\n3402|    def ineqcons(self, value):\n3403|        self._eqcons = value\n3404|\n3405|    def traverse_postorder(self, include_operator=False):\n3406|        \"\"\" Postorder traversal of the CompoundModel tree.\"\"\"\n3407|        res = []\n3408|        if isinstance(self.left, CompoundModel):\n3409|            res = res + self.left.traverse_postorder(include_operator)\n3410|        else:\n3411|            res = res + [self.left]\n3412|        if isinstance(self.right, CompoundModel):\n3413|            res = res + self.right.traverse_postorder(include_operator)\n3414|        else:\n3415|            res = res + [self.right]\n3416|        if include_operator:\n3417|            res.append(self.op)\n3418|        else:\n3419|            res.append(self)\n3420|        return res\n3421|\n3422|    def _format_expression(self, format_leaf=None):\n3423|        leaf_idx = 0\n3424|        operands = deque()\n3425|\n3426|        if format_leaf is None:\n3427|            format_leaf = lambda i, l: f'[{i}]'\n3428|\n3429|        for node in self.traverse_postorder():\n3430|            if not isinstance(node, CompoundModel):\n3431|                operands.append(format_leaf(leaf_idx, node))\n3432|                leaf_idx += 1\n3433|                continue\n3434|\n3435|            right = operands.pop()\n3436|            left = operands.pop()\n3437|            if node.op in OPERATOR_PRECEDENCE:\n3438|                oper_order = OPERATOR_PRECEDENCE[node.op]\n3439|\n3440|                if isinstance(node, CompoundModel):\n3441|                    if (isinstance(node.left, CompoundModel) and\n3442|                            OPERATOR_PRECEDENCE[node.left.op] < oper_order):\n3443|                        left = f'({left})'\n3444|                    if (isinstance(node.right, CompoundModel) and\n3445|                            OPERATOR_PRECEDENCE[node.right.op] < oper_order):\n3446|                        right = f'({right})'\n3447|\n3448|                operands.append(' '.join((left, node.op, right)))\n3449|            else:\n3450|                left = f'(({left}),'\n3451|                right = f'({right}))'\n3452|                operands.append(' '.join((node.op[0], left, right)))\n3453|\n3454|        return ''.join(operands)\n3455|\n3456|    def _format_components(self):\n3457|        if self._parameters_ is None:\n3458|            self._map_parameters()\n3459|        return '\\n\\n'.join('[{0}]: {1!r}'.format(idx, m)\n3460|                           for idx, m in enumerate(self._leaflist))\n3461|\n3462|    def __str__(self):\n3463|        expression = self._format_expression()\n3464|        components = self._format_components()\n3465|        keywords = [\n3466|            ('Expression', expression),\n3467|            ('Components', '\\n' + indent(components))\n3468|        ]\n3469|        return super()._format_str(keywords=keywords)\n3470|\n3471|    def rename(self, name):\n3472|        self.name = name\n3473|        return self\n3474|\n3475|    @property\n3476|    def isleaf(self):\n3477|        return False\n3478|\n3479|    @property\n3480|    def inverse(self):\n3481|        if self.op == '|':\n3482|            return self.right.inverse | self.left.inverse\n3483|        elif self.op == '&':\n3484|            return self.left.inverse & self.right.inverse\n3485|        else:\n3486|            return NotImplemented\n3487|\n3488|    @property\n3489|    def fittable(self):\n3490|        \"\"\" Set the fittable attribute on a compound model.\"\"\"\n3491|        if self._fittable is None:\n3492|            if self._leaflist is None:\n3493|                self._map_parameters()\n3494|            self._fittable = all(m.fittable for m in self._leaflist)\n3495|        return self._fittable\n3496|\n3497|    __add__ = _model_oper('+')\n3498|    __sub__ = _model_oper('-')\n3499|    __mul__ = _model_oper('*')\n3500|    __truediv__ = _model_oper('/')\n3501|    __pow__ = _model_oper('**')\n3502|    __or__ = _model_oper('|')\n3503|    __and__ = _model_oper('&')\n3504|\n3505|    def _map_parameters(self):\n3506|        \"\"\"\n3507|        Map all the constituent model parameters to the compound object,\n3508|        renaming as necessary by appending a suffix number.\n3509|\n3510|        This can be an expensive operation, particularly for a complex\n3511|        expression tree.\n3512|\n3513|        All the corresponding parameter attributes are created that one\n3514|        expects for the Model class.\n3515|\n3516|        The parameter objects that the attributes point to are the same\n3517|        objects as in the constiutent models. Changes made to parameter\n3518|        values to either are seen by both.\n3519|\n3520|        Prior to calling this, none of the associated attributes will\n3521|        exist. This method must be called to make the model usable by\n3522|        fitting engines.\n3523|\n3524|        If oldnames=True, then parameters are named as in the original\n3525|        implementation of compound models.\n3526|        \"\"\"\n3527|        if self._parameters is not None:\n3528|            # do nothing\n3529|            return\n3530|        if self._leaflist is None:\n3531|            self._make_leaflist()\n3532|        self._parameters_ = {}\n3533|        param_map = {}\n3534|        self._param_names = []\n3535|        for lindex, leaf in enumerate(self._leaflist):\n3536|            if not isinstance(leaf, dict):\n3537|                for param_name in leaf.param_names:\n3538|                    param = getattr(leaf, param_name)\n3539|                    new_param_name = f\"{param_name}_{lindex}\"\n3540|                    self.__dict__[new_param_name] = param\n3541|                    self._parameters_[new_param_name] = param\n3542|                    self._param_names.append(new_param_name)\n3543|                    param_map[new_param_name] = (lindex, param_name)\n3544|        self._param_metrics = {}\n3545|        self._param_map = param_map\n3546|        self._param_map_inverse = dict((v, k) for k, v in param_map.items())\n3547|        self._initialize_slices()\n3548|        self._param_names = tuple(self._param_names)\n3549|\n3550|    def _initialize_slices(self):\n3551|        param_metrics = self._param_metrics\n3552|        total_size = 0\n3553|\n3554|        for name in self.param_names:\n3555|            param = getattr(self, name)\n3556|            value = param.value\n3557|            param_size = np.size(value)\n3558|            param_shape = np.shape(value)\n3559|            param_slice = slice(total_size, total_size + param_size)\n3560|            param_metrics[name] = {}\n3561|            param_metrics[name]['slice'] = param_slice\n3562|            param_metrics[name]['shape'] = param_shape\n3563|            param_metrics[name]['size'] = param_size\n3564|            total_size += param_size\n3565|        self._parameters = np.empty(total_size, dtype=np.float64)\n3566|\n3567|    @staticmethod\n3568|    def _recursive_lookup(branch, adict, key):\n3569|        if isinstance(branch, CompoundModel):\n3570|            return adict[key]\n3571|        return branch, key\n3572|\n3573|    def inputs_map(self):\n3574|        \"\"\"\n3575|        Map the names of the inputs to this ExpressionTree to the inputs to the leaf models.\n3576|        \"\"\"\n3577|        inputs_map = {}\n3578|        if not isinstance(self.op, str):  # If we don't have an operator the mapping is trivial\n3579|            return {inp: (self, inp) for inp in self.inputs}\n3580|\n3581|        elif self.op == '|':\n3582|            if isinstance(self.left, CompoundModel):\n3583|                l_inputs_map = self.left.inputs_map()\n3584|            for inp in self.inputs:\n3585|                if isinstance(self.left, CompoundModel):\n3586|                    inputs_map[inp] = l_inputs_map[inp]\n3587|                else:\n3588|                    inputs_map[inp] = self.left, inp\n3589|        elif self.op == '&':\n3590|            if isinstance(self.left, CompoundModel):\n3591|                l_inputs_map = self.left.inputs_map()\n3592|            if isinstance(self.right, CompoundModel):\n3593|                r_inputs_map = self.right.inputs_map()\n3594|            for i, inp in enumerate(self.inputs):\n3595|                if i < len(self.left.inputs):  # Get from left\n3596|                    if isinstance(self.left, CompoundModel):\n3597|                        inputs_map[inp] = l_inputs_map[self.left.inputs[i]]\n3598|                    else:\n3599|                        inputs_map[inp] = self.left, self.left.inputs[i]\n3600|                else:  # Get from right\n3601|                    if isinstance(self.right, CompoundModel):\n3602|                        inputs_map[inp] = r_inputs_map[self.right.inputs[i - len(self.left.inputs)]]\n3603|                    else:\n3604|                        inputs_map[inp] = self.right, self.right.inputs[i - len(self.left.inputs)]\n3605|        elif self.op == 'fix_inputs':\n3606|            fixed_ind = list(self.right.keys())\n3607|            ind = [list(self.left.inputs).index(i) if isinstance(i, str) else i for i in fixed_ind]\n3608|            inp_ind = list(range(self.left.n_inputs))\n3609|            for i in ind:\n3610|                inp_ind.remove(i)\n3611|            for i in inp_ind:\n3612|                inputs_map[self.left.inputs[i]] = self.left, self.left.inputs[i]\n3613|        else:\n3614|            if isinstance(self.left, CompoundModel):\n3615|                l_inputs_map = self.left.inputs_map()\n3616|            for inp in self.left.inputs:\n3617|                if isinstance(self.left, CompoundModel):\n3618|                    inputs_map[inp] = l_inputs_map[inp]\n3619|                else:\n3620|                    inputs_map[inp] = self.left, inp\n3621|        return inputs_map\n3622|\n3623|    def _parameter_units_for_data_units(self, input_units, output_units):\n3624|        if self._leaflist is None:\n3625|            self._map_parameters()\n3626|        units_for_data = {}\n3627|        for imodel, model in enumerate(self._leaflist):\n3628|            units_for_data_leaf = model._parameter_units_for_data_units(input_units, output_units)\n3629|            for param_leaf in units_for_data_leaf:\n3630|                param = self._param_map_inverse[(imodel, param_leaf)]\n3631|                units_for_data[param] = units_for_data_leaf[param_leaf]\n3632|        return units_for_data\n3633|\n3634|    @property\n3635|    def input_units(self):\n3636|        inputs_map = self.inputs_map()\n3637|        input_units_dict = {key: inputs_map[key][0].input_units[orig_key]\n3638|                            for key, (mod, orig_key) in inputs_map.items()\n3639|                            if inputs_map[key][0].input_units is not None}\n3640|        if input_units_dict:\n3641|            return input_units_dict\n3642|        return None\n3643|\n3644|    @property\n3645|    def input_units_equivalencies(self):\n3646|        inputs_map = self.inputs_map()\n3647|        input_units_equivalencies_dict = {\n3648|            key: inputs_map[key][0].input_units_equivalencies[orig_key]\n3649|            for key, (mod, orig_key) in inputs_map.items()\n3650|            if inputs_map[key][0].input_units_equivalencies is not None\n3651|        }\n3652|        if not input_units_equivalencies_dict:\n3653|            return None\n3654|\n3655|        return input_units_equivalencies_dict\n3656|\n3657|    @property\n3658|    def input_units_allow_dimensionless(self):\n3659|        inputs_map = self.inputs_map()\n3660|        return {key: inputs_map[key][0].input_units_allow_dimensionless[orig_key]\n3661|                for key, (mod, orig_key) in inputs_map.items()}\n3662|\n3663|    @property\n3664|    def input_units_strict(self):\n3665|        inputs_map = self.inputs_map()\n3666|        return {key: inputs_map[key][0].input_units_strict[orig_key]\n3667|                for key, (mod, orig_key) in inputs_map.items()}\n3668|\n3669|    @property\n3670|    def return_units(self):\n3671|        outputs_map = self.outputs_map()\n3672|        return {key: outputs_map[key][0].return_units[orig_key]\n3673|                for key, (mod, orig_key) in outputs_map.items()\n3674|                if outputs_map[key][0].return_units is not None}\n3675|\n3676|    def outputs_map(self):\n3677|        \"\"\"\n3678|        Map the names of the outputs to this ExpressionTree to the outputs to the leaf models.\n3679|        \"\"\"\n3680|        outputs_map = {}\n3681|        if not isinstance(self.op, str):  # If we don't have an operator the mapping is trivial\n3682|            return {out: (self, out) for out in self.outputs}\n3683|\n3684|        elif self.op == '|':\n3685|            if isinstance(self.right, CompoundModel):\n3686|                r_outputs_map = self.right.outputs_map()\n3687|            for out in self.outputs:\n3688|                if isinstance(self.right, CompoundModel):\n3689|                    outputs_map[out] = r_outputs_map[out]\n3690|                else:\n3691|                    outputs_map[out] = self.right, out\n3692|\n3693|        elif self.op == '&':\n3694|            if isinstance(self.left, CompoundModel):\n3695|                l_outputs_map = self.left.outputs_map()\n3696|            if isinstance(self.right, CompoundModel):\n3697|                r_outputs_map = self.right.outputs_map()\n3698|            for i, out in enumerate(self.outputs):\n3699|                if i < len(self.left.outputs):  # Get from left\n3700|                    if isinstance(self.left, CompoundModel):\n3701|                        outputs_map[out] = l_outputs_map[self.left.outputs[i]]\n3702|                    else:\n3703|                        outputs_map[out] = self.left, self.left.outputs[i]\n3704|                else:  # Get from right\n3705|                    if isinstance(self.right, CompoundModel):\n3706|                        outputs_map[out] = r_outputs_map[self.right.outputs[i - len(self.left.outputs)]]\n3707|                    else:\n3708|                        outputs_map[out] = self.right, self.right.outputs[i - len(self.left.outputs)]\n3709|        elif self.op == 'fix_inputs':\n3710|            return self.left.outputs_map()\n3711|        else:\n3712|            if isinstance(self.left, CompoundModel):\n3713|                l_outputs_map = self.left.outputs_map()\n3714|            for out in self.left.outputs:\n3715|                if isinstance(self.left, CompoundModel):\n3716|                    outputs_map[out] = l_outputs_map()[out]\n3717|                else:\n3718|                    outputs_map[out] = self.left, out\n3719|        return outputs_map\n3720|\n3721|    @property\n3722|    def has_user_bounding_box(self):\n3723|        \"\"\"\n3724|        A flag indicating whether or not a custom bounding_box has been\n3725|        assigned to this model by a user, via assignment to\n3726|        ``model.bounding_box``.\n3727|        \"\"\"\n3728|\n3729|        return self._user_bounding_box is not None\n3730|\n3731|    def render(self, out=None, coords=None):\n3732|        \"\"\"\n3733|        Evaluate a model at fixed positions, respecting the ``bounding_box``.\n3734|\n3735|        The key difference relative to evaluating the model directly is that\n3736|        this method is limited to a bounding box if the `Model.bounding_box`\n3737|        attribute is set.\n3738|\n3739|        Parameters\n3740|        ----------\n3741|        out : `numpy.ndarray`, optional\n3742|            An array that the evaluated model will be added to.  If this is not\n3743|            given (or given as ``None``), a new array will be created.\n3744|        coords : array-like, optional\n3745|            An array to be used to translate from the model's input coordinates\n3746|            to the ``out`` array. It should have the property that\n3747|            ``self(coords)`` yields the same shape as ``out``.  If ``out`` is\n3748|            not specified, ``coords`` will be used to determine the shape of\n3749|            the returned array. If this is not provided (or None), the model\n3750|            will be evaluated on a grid determined by `Model.bounding_box`.\n3751|\n3752|        Returns\n3753|        -------\n3754|        out : `numpy.ndarray`\n3755|            The model added to ``out`` if  ``out`` is not ``None``, or else a\n3756|            new array from evaluating the model over ``coords``.\n3757|            If ``out`` and ``coords`` are both `None`, the returned array is\n3758|            limited to the `Model.bounding_box` limits. If\n3759|            `Model.bounding_box` is `None`, ``arr`` or ``coords`` must be\n3760|            passed.\n3761|\n3762|        Raises\n3763|        ------\n3764|        ValueError\n3765|            If ``coords`` are not given and the the `Model.bounding_box` of\n3766|            this model is not set.\n3767|\n3768|        Examples\n3769|        --------\n3770|        :ref:`astropy:bounding-boxes`\n3771|        \"\"\"\n3772|\n3773|        bbox = self.get_bounding_box()\n3774|\n3775|        ndim = self.n_inputs\n3776|\n3777|        if (coords is None) and (out is None) and (bbox is None):\n3778|            raise ValueError('If no bounding_box is set, '\n3779|                             'coords or out must be input.')\n3780|\n3781|        # for consistent indexing\n3782|        if ndim == 1:\n3783|            if coords is not None:\n3784|                coords = [coords]\n3785|            if bbox is not None:\n3786|                bbox = [bbox]\n3787|\n3788|        if coords is not None:\n3789|            coords = np.asanyarray(coords, dtype=float)\n3790|            # Check dimensions match out and model\n3791|            assert len(coords) == ndim\n3792|            if out is not None:\n3793|                if coords[0].shape != out.shape:\n3794|                    raise ValueError('inconsistent shape of the output.')\n3795|            else:\n3796|                out = np.zeros(coords[0].shape)\n3797|\n3798|        if out is not None:\n3799|            out = np.asanyarray(out)\n3800|            if out.ndim != ndim:\n3801|                raise ValueError('the array and model must have the same '\n3802|                                 'number of dimensions.')\n3803|\n3804|        if bbox is not None:\n3805|            # Assures position is at center pixel, important when using\n3806|            # add_array.\n3807|            pd = np.array([(np.mean(bb), np.ceil((bb[1] - bb[0]) / 2))\n3808|                           for bb in bbox]).astype(int).T\n3809|            pos, delta = pd\n3810|\n3811|            if coords is not None:\n3812|                sub_shape = tuple(delta * 2 + 1)\n3813|                sub_coords = np.array([extract_array(c, sub_shape, pos)\n3814|                                       for c in coords])\n3815|            else:\n3816|                limits = [slice(p - d, p + d + 1, 1) for p, d in pd.T]\n3817|                sub_coords = np.mgrid[limits]\n3818|\n3819|            sub_coords = sub_coords[::-1]\n3820|\n3821|            if out is None:\n3822|                out = self(*sub_coords)\n3823|            else:\n3824|                try:\n3825|                    out = add_array(out, self(*sub_coords), pos)\n3826|                except ValueError:\n3827|                    raise ValueError(\n3828|                        'The `bounding_box` is larger than the input out in '\n3829|                        'one or more dimensions. Set '\n3830|                        '`model.bounding_box = None`.')\n3831|        else:\n3832|            if coords is None:\n3833|                im_shape = out.shape\n3834|                limits = [slice(i) for i in im_shape]\n3835|                coords = np.mgrid[limits]\n3836|\n3837|            coords = coords[::-1]\n3838|\n3839|            out += self(*coords)\n3840|\n3841|        return out\n3842|\n3843|    def replace_submodel(self, name, model):\n3844|        \"\"\"\n3845|        Construct a new `~astropy.modeling.CompoundModel` instance from an\n3846|        existing CompoundModel, replacing the named submodel with a new model.\n3847|\n3848|        In order to ensure that inverses and names are kept/reconstructed, it's\n3849|        necessary to rebuild the CompoundModel from the replaced node all the\n3850|        way back to the base. The original CompoundModel is left untouched.\n3851|\n3852|        Parameters\n3853|        ----------\n3854|        name : str\n3855|            name of submodel to be replaced\n3856|        model : `~astropy.modeling.Model`\n3857|            replacement model\n3858|        \"\"\"\n3859|        submodels = [m for m in self.traverse_postorder()\n3860|                     if getattr(m, 'name', None) == name]\n3861|        if submodels:\n3862|            if len(submodels) > 1:\n3863|                raise ValueError(f\"More than one submodel named {name}\")\n3864|\n3865|            old_model = submodels.pop()\n3866|            if len(old_model) != len(model):\n3867|                raise ValueError(\"New and old models must have equal values \"\n3868|                                 \"for n_models\")\n3869|\n3870|            # Do this check first in order to raise a more helpful Exception,\n3871|            # although it would fail trying to construct the new CompoundModel\n3872|            if (old_model.n_inputs != model.n_inputs or\n3873|                        old_model.n_outputs != model.n_outputs):\n3874|                raise ValueError(\"New model must match numbers of inputs and \"\n3875|                                 \"outputs of existing model\")\n3876|\n3877|            tree = _get_submodel_path(self, name)\n3878|            while tree:\n3879|                branch = self.copy()\n3880|                for node in tree[:-1]:\n3881|                    branch = getattr(branch, node)\n3882|                setattr(branch, tree[-1], model)\n3883|                model = CompoundModel(branch.op, branch.left, branch.right,\n3884|                                      name=branch.name)\n3885|                tree = tree[:-1]\n3886|            return model\n3887|\n3888|        else:\n3889|            raise ValueError(f\"No submodels found named {name}\")\n3890|\n3891|    def _set_sub_models_and_parameter_units(self, left, right):\n3892|        \"\"\"\n3893|        Provides a work-around to properly set the sub models and respective\n3894|        parameters's units/values when using ``without_units_for_data``\n3895|        or ``without_units_for_data`` methods.\n3896|        \"\"\"\n3897|        model = CompoundModel(self.op, left, right)\n3898|\n3899|        self.left = left\n3900|        self.right = right\n3901|\n3902|        for name in model.param_names:\n3903|            model_parameter = getattr(model, name)\n3904|            parameter = getattr(self, name)\n3905|\n3906|            parameter.value = model_parameter.value\n3907|            parameter._set_unit(model_parameter.unit, force=True)\n3908|\n3909|    def without_units_for_data(self, **kwargs):\n3910|        \"\"\"\n3911|        See `~astropy.modeling.Model.without_units_for_data` for overview\n3912|        of this method.\n3913|\n3914|        Notes\n3915|        -----\n3916|        This modifies the behavior of the base method to account for the\n3917|        case where the sub-models of a compound model have different output\n3918|        units. This is only valid for compound * and / compound models as\n3919|        in that case it is reasonable to mix the output units. It does this\n3920|        by modifying the output units of each sub model by using the output\n3921|        units of the other sub model so that we can apply the original function\n3922|        and get the desired result.\n3923|\n3924|        Additional data has to be output in the mixed output unit case\n3925|        so that the units can be properly rebuilt by\n3926|        `~astropy.modeling.CompoundModel.with_units_from_data`.\n3927|\n3928|        Outside the mixed output units, this method is identical to the\n3929|        base method.\n3930|        \"\"\"\n3931|        if self.op in ['*', '/']:\n3932|            model = self.copy()\n3933|            inputs = {inp: kwargs[inp] for inp in self.inputs}\n3934|\n3935|            left_units = self.left.output_units(**kwargs)\n3936|            right_units = self.right.output_units(**kwargs)\n3937|\n3938|            if self.op == '*':\n3939|                left_kwargs = {out: kwargs[out] / right_units[out]\n3940|                               for out in self.left.outputs if kwargs[out] is not None}\n3941|                right_kwargs = {out: kwargs[out] / left_units[out]\n3942|                                for out in self.right.outputs if kwargs[out] is not None}\n3943|            else:\n3944|                left_kwargs = {out: kwargs[out] * right_units[out]\n3945|                               for out in self.left.outputs if kwargs[out] is not None}\n3946|                right_kwargs = {out: 1 / kwargs[out] * left_units[out]\n3947|                                for out in self.right.outputs if kwargs[out] is not None}\n3948|\n3949|            left_kwargs.update(inputs.copy())\n3950|            right_kwargs.update(inputs.copy())\n3951|\n3952|            left = self.left.without_units_for_data(**left_kwargs)\n3953|            if isinstance(left, tuple):\n3954|                left_kwargs['_left_kwargs'] = left[1]\n3955|                left_kwargs['_right_kwargs'] = left[2]\n3956|                left = left[0]\n3957|\n3958|            right = self.right.without_units_for_data(**right_kwargs)\n3959|            if isinstance(right, tuple):\n3960|                right_kwargs['_left_kwargs'] = right[1]\n3961|                right_kwargs['_right_kwargs'] = right[2]\n3962|                right = right[0]\n3963|\n3964|            model._set_sub_models_and_parameter_units(left, right)\n3965|\n3966|            return model, left_kwargs, right_kwargs\n3967|        else:\n3968|            return super().without_units_for_data(**kwargs)\n3969|\n3970|    def with_units_from_data(self, **kwargs):\n3971|        \"\"\"\n3972|        See `~astropy.modeling.Model.with_units_from_data` for overview\n3973|        of this method.\n3974|\n3975|        Notes\n3976|        -----\n3977|        This modifies the behavior of the base method to account for the\n3978|        case where the sub-models of a compound model have different output\n3979|        units. This is only valid for compound * and / compound models as\n3980|        in that case it is reasonable to mix the output units. In order to\n3981|        do this it requires some additional information output by\n3982|        `~astropy.modeling.CompoundModel.without_units_for_data` passed as\n3983|        keyword arguments under the keywords ``_left_kwargs`` and ``_right_kwargs``.\n3984|\n3985|        Outside the mixed output units, this method is identical to the\n3986|        base method.\n3987|        \"\"\"\n3988|\n3989|        if self.op in ['*', '/']:\n3990|            left_kwargs = kwargs.pop('_left_kwargs')\n3991|            right_kwargs = kwargs.pop('_right_kwargs')\n3992|\n3993|            left = self.left.with_units_from_data(**left_kwargs)\n3994|            right = self.right.with_units_from_data(**right_kwargs)\n3995|\n3996|            model = self.copy()\n3997|            model._set_sub_models_and_parameter_units(left, right)\n3998|\n3999|            return model\n4000|        else:\n4001|            return super().with_units_from_data(**kwargs)\n4002|\n4003|\n4004|def _get_submodel_path(model, name):\n4005|    \"\"\"Find the route down a CompoundModel's tree to the model with the\n4006|    specified name (whether it's a leaf or not)\"\"\"\n4007|    if getattr(model, 'name', None) == name:\n4008|        return []\n4009|    try:\n4010|        return ['left'] + _get_submodel_path(model.left, name)\n4011|    except (AttributeError, TypeError):\n...\n\n\n\n\n###\n\nPlease provide the class name, function or method name, or the exact line numbers that need to be edited.\nThe possible location outputs should be either \"class\", \"function\" or \"line\".\n\n### Examples:\n```\nfull_path1/file1.py\nline: 10\nclass: MyClass1\nline: 51\n\nfull_path2/file2.py\nfunction: MyClass2.my_method\nline: 12\n\nfull_path3/file3.py\nfunction: my_function\nline: 24\nline: 156\n```\n\nReturn just the location(s) wrapped with ```.\n", "response": ["```\nastropy/modeling/separable.py\nfunction: _coord_matrix\nline: 199\nline: 200\nline: 207\nline: 208\n```", "```\nastropy/modeling/separable.py\nline: 67\nfunction: separability_matrix\nline: 69\nline: 100\nline: 101\n\nastropy/modeling/separable.py\nline: 290\nfunction: _separable\nline: 306\nline: 307\nline: 308\n\nastropy/modeling/separable.py\nline: 305\nfunction: _separable\nline: 306\n\nastropy/modeling/core.py\nclass: CompoundModel\nfunction: CompoundModel.__init__\nline: 2902\n\nastropy/modeling/core.py\nclass: CompoundModel\nfunction: CompoundModel._apply_operators_to_value_lists\nline: 3082\nline: 3092\nline: 3100\n```", "```\nastropy/modeling/separable.py\nfunction: _coord_matrix\nline: 189\nline: 202\nline: 206\nline: 210\n\nfunction: _cstack\nline: 237\nline: 242\n\nfunction: _separable\nline: 307\nline: 308\n\nastropy/modeling/core.py\nclass: CompoundModel\nline: 294\n```", "```\nastropy/modeling/separable.py\nline: 290\nline: 307\n```"], "usage": {"completion_tokens": 310, "prompt_tokens": 15083}}}
{"instance_id": "django__django-10914", "found_files": ["django/core/files/storage.py", "django/core/files/uploadedfile.py", "django/core/files/temp.py", "django/setup.py", "django/__init__.py", "django/__main__.py"], "additional_artifact_loc_file": {}, "file_traj": {}, "found_related_locs": {"django/core/files/storage.py": ["function: FileSystemStorage.__init__\nfunction: FileSystemStorage._save\nfunction: FileSystemStorage.file_permissions_mode"], "django/core/files/uploadedfile.py": ["class: TemporaryUploadedFile\nclass: InMemoryUploadedFile"], "django/core/files/temp.py": ["variable: gettempdir"]}, "additional_artifact_loc_related": [{"raw_output_loc": "```\ndjango/core/files/storage.py\nfunction: FileSystemStorage.__init__\nfunction: FileSystemStorage._save\nfunction: FileSystemStorage.file_permissions_mode\n\ndjango/core/files/uploadedfile.py\nclass: TemporaryUploadedFile\nclass: InMemoryUploadedFile\n\ndjango/core/files/temp.py\nvariable: gettempdir\n```"}], "related_loc_traj": [{"response": "```\ndjango/core/files/storage.py\nfunction: FileSystemStorage.__init__\nfunction: FileSystemStorage._save\nfunction: FileSystemStorage.file_permissions_mode\n\ndjango/core/files/uploadedfile.py\nclass: TemporaryUploadedFile\nclass: InMemoryUploadedFile\n\ndjango/core/files/temp.py\nvariable: gettempdir\n```", "usage": {"completion_tokens": 67, "prompt_tokens": 1377}, "prompt": "\nPlease look through the following GitHub Problem Description and the Skeleton of Relevant Files.\nIdentify all locations that need inspection or editing to fix the problem, including directly related areas as well as any potentially related global variables, functions, and classes.\nFor each location you provide, either give the name of the class, the name of a method in a class, the name of a function, or the name of a global variable.\n\n### GitHub Problem Description ###\nSet default FILE_UPLOAD_PERMISSION to 0o644.\nDescription\n\t\nHello,\nAs far as I can see, the \u200bFile Uploads documentation page does not mention any permission issues.\nWhat I would like to see is a warning that in absence of explicitly configured FILE_UPLOAD_PERMISSIONS, the permissions for a file uploaded to FileSystemStorage might not be consistent depending on whether a MemoryUploadedFile or a TemporaryUploadedFile was used for temporary storage of the uploaded data (which, with the default FILE_UPLOAD_HANDLERS, in turn depends on the uploaded data size).\nThe tempfile.NamedTemporaryFile + os.rename sequence causes the resulting file permissions to be 0o0600 on some systems (I experience it here on CentOS 7.4.1708 and Python 3.6.5). In all probability, the implementation of Python's built-in tempfile module explicitly sets such permissions for temporary files due to security considerations.\nI found mentions of this issue \u200bon GitHub, but did not manage to find any existing bug report in Django's bug tracker.\n\n\n### Skeleton of Relevant Files ###\n\n### File: django/core/files/storage.py ###\n```python\n\n__all__ = (\n    'Storage', 'FileSystemStorage', 'DefaultStorage', 'default_storage',\n    'get_storage_class',\n)\n\n\nclass Storage:\n\n    # The following methods represent a public interface to private methods.\n    # These shouldn't be overridden by subclasses unless absolutely necessary.\n\n    def open(self, name, mode='rb'):\n...\n\n    def save(self, name, content, max_length=None):\n...\n\n    # These methods are part of the public API, with default implementations.\n\n    def get_valid_name(self, name):\n...\n\n    def get_available_name(self, name, max_length=None):\n...\n\n    def generate_filename(self, filename):\n...\n\n    def path(self, name):\n...\n\n    # The following methods form the public API for storage systems, but with\n    # no default implementations. Subclasses must implement *all* of these.\n\n    def delete(self, name):\n...\n\n    def exists(self, name):\n...\n\n    def listdir(self, path):\n...\n\n    def size(self, name):\n...\n\n    def url(self, name):\n...\n\n    def get_accessed_time(self, name):\n...\n\n    def get_created_time(self, name):\n...\n\n    def get_modified_time(self, name):\n...\n\n\n@deconstructible\nclass FileSystemStorage(Storage):\n    # The combination of O_CREAT and O_EXCL makes os.open() raise OSError if\n    # the file already exists before it's opened.\n    OS_OPEN_FLAGS = os.O_WRONLY | os.O_CREAT | os.O_EXCL | getattr(os, 'O_BINARY', 0)\n\n    def __init__(self, location=None, base_url=None, file_permissions_mode=None,\n                 directory_permissions_mode=None):\n...\n\n    def _clear_cached_properties(self, setting, **kwargs):\n...\n\n    def _value_or_setting(self, value, setting):\n...\n\n    @cached_property\n    def base_location(self):\n...\n\n    @cached_property\n    def location(self):\n...\n\n    @cached_property\n    def base_url(self):\n...\n\n    @cached_property\n    def file_permissions_mode(self):\n...\n\n    @cached_property\n    def directory_permissions_mode(self):\n...\n\n    def _open(self, name, mode='rb'):\n...\n\n    def _save(self, name, content):\n...\n\n    def delete(self, name):\n...\n\n    def exists(self, name):\n...\n\n    def listdir(self, path):\n...\n\n    def path(self, name):\n...\n\n    def size(self, name):\n...\n\n    def url(self, name):\n...\n\n    def _datetime_from_timestamp(self, ts):\n...\n\n    def get_accessed_time(self, name):\n...\n\n    def get_created_time(self, name):\n...\n\n    def get_modified_time(self, name):\n...\n\n\ndef get_storage_class(import_path=None):\n...\n\n\nclass DefaultStorage(LazyObject):\n    def _setup(self):\n...\n\n\ndefault_storage = DefaultStorage()\n\n```\n\n### File: django/core/files/uploadedfile.py ###\n```python\n\n__all__ = ('UploadedFile', 'TemporaryUploadedFile', 'InMemoryUploadedFile',\n           'SimpleUploadedFile')\n\n\nclass UploadedFile(File):\n\n    def __init__(self, file=None, name=None, content_type=None, size=None, charset=None, content_type_extra=None):\n...\n\n    def __repr__(self):\n...\n\n    def _get_name(self):\n...\n\n    def _set_name(self, name):\n...\n\n    name = property(_get_name, _set_name)\n\n\nclass TemporaryUploadedFile(UploadedFile):\n    def __init__(self, name, content_type, size, charset, content_type_extra=None):\n...\n\n    def temporary_file_path(self):\n...\n\n    def close(self):\n...\n\n\nclass InMemoryUploadedFile(UploadedFile):\n    def __init__(self, file, field_name, name, content_type, size, charset, content_type_extra=None):\n...\n\n    def open(self, mode=None):\n...\n\n    def chunks(self, chunk_size=None):\n...\n\n    def multiple_chunks(self, chunk_size=None):\n...\n\n\nclass SimpleUploadedFile(InMemoryUploadedFile):\n    def __init__(self, name, content, content_type='text/plain'):\n...\n\n    @classmethod\n    def from_dict(cls, file_dict):\n...\n\n```\n\n### File: django/core/files/temp.py ###\n```python\n\n__all__ = ('NamedTemporaryFile', 'gettempdir',)\n\ngettempdir = tempfile.gettempdir\n\n```\n\n\n###\n\nPlease provide the complete set of locations as either a class name, a function name, or a variable name.\nNote that if you include a class, you do not need to list its specific methods.\nYou can include either the entire class or don't include the class name and instead include specific methods in the class.\n### Examples:\n```\nfull_path1/file1.py\nfunction: my_function_1\nclass: MyClass1\nfunction: MyClass2.my_method\n\nfull_path2/file2.py\nvariable: my_var\nfunction: MyClass3.my_method\n\nfull_path3/file3.py\nfunction: my_function_2\nfunction: my_function_3\nfunction: MyClass4.my_method_1\nclass: MyClass5\n```\n\nReturn just the locations wrapped with ```.\n"}], "found_edit_locs": {"django/core/files/storage.py": ["\nline: 217\nline: 260"], "django/core/files/uploadedfile.py": ["\nline: 61"]}, "additional_artifact_loc_edit_location": [{"raw_output_loc": ["```\ndjango/core/files/storage.py\nline: 217\nline: 260\n\ndjango/core/files/uploadedfile.py\nline: 61\n```", "```\ndjango/core/files/storage.py\nline: 178\nline: 283\n\ndjango/core/files/uploadedfile.py\nline: 61\n```", "```\ndjango/core/files/storage.py\nline: 178\nline: 283\n\ndjango/core/files/uploadedfile.py\nline: 61\n```\n", "```\ndjango/core/files/storage.py\nline: 217\nline: 283\n\ndjango/core/files/uploadedfile.py\nline: 61\n```"]}], "edit_loc_traj": {"prompt": "\nPlease review the following GitHub problem description and relevant files, and provide a set of locations that need to be edited to fix the issue.\nThe locations can be specified as class names, function or method names, or exact line numbers that require modification.\n\n### GitHub Problem Description ###\nSet default FILE_UPLOAD_PERMISSION to 0o644.\nDescription\n\t\nHello,\nAs far as I can see, the \u200bFile Uploads documentation page does not mention any permission issues.\nWhat I would like to see is a warning that in absence of explicitly configured FILE_UPLOAD_PERMISSIONS, the permissions for a file uploaded to FileSystemStorage might not be consistent depending on whether a MemoryUploadedFile or a TemporaryUploadedFile was used for temporary storage of the uploaded data (which, with the default FILE_UPLOAD_HANDLERS, in turn depends on the uploaded data size).\nThe tempfile.NamedTemporaryFile + os.rename sequence causes the resulting file permissions to be 0o0600 on some systems (I experience it here on CentOS 7.4.1708 and Python 3.6.5). In all probability, the implementation of Python's built-in tempfile module explicitly sets such permissions for temporary files due to security considerations.\nI found mentions of this issue \u200bon GitHub, but did not manage to find any existing bug report in Django's bug tracker.\n\n\n###\n### django/core/files/storage.py\n...\n168|\n169|@deconstructible\n170|class FileSystemStorage(Storage):\n171|    \"\"\"\n172|    Standard filesystem storage\n173|    \"\"\"\n174|    # The combination of O_CREAT and O_EXCL makes os.open() raise OSError if\n175|    # the file already exists before it's opened.\n176|    OS_OPEN_FLAGS = os.O_WRONLY | os.O_CREAT | os.O_EXCL | getattr(os, 'O_BINARY', 0)\n177|\n178|    def __init__(self, location=None, base_url=None, file_permissions_mode=None,\n179|                 directory_permissions_mode=None):\n180|        self._location = location\n181|        self._base_url = base_url\n182|        self._file_permissions_mode = file_permissions_mode\n183|        self._directory_permissions_mode = directory_permissions_mode\n184|        setting_changed.connect(self._clear_cached_properties)\n185|\n186|    def _clear_cached_properties(self, setting, **kwargs):\n187|        \"\"\"Reset setting based property values.\"\"\"\n188|        if setting == 'MEDIA_ROOT':\n189|            self.__dict__.pop('base_location', None)\n190|            self.__dict__.pop('location', None)\n191|        elif setting == 'MEDIA_URL':\n192|            self.__dict__.pop('base_url', None)\n193|        elif setting == 'FILE_UPLOAD_PERMISSIONS':\n194|            self.__dict__.pop('file_permissions_mode', None)\n...\n206|    def location(self):\n207|        return os.path.abspath(self.base_location)\n208|\n209|    @cached_property\n210|    def base_url(self):\n211|        if self._base_url is not None and not self._base_url.endswith('/'):\n212|            self._base_url += '/'\n213|        return self._value_or_setting(self._base_url, settings.MEDIA_URL)\n214|\n215|    @cached_property\n216|    def file_permissions_mode(self):\n217|        return self._value_or_setting(self._file_permissions_mode, settings.FILE_UPLOAD_PERMISSIONS)\n218|\n219|    @cached_property\n220|    def directory_permissions_mode(self):\n221|        return self._value_or_setting(self._directory_permissions_mode, settings.FILE_UPLOAD_DIRECTORY_PERMISSIONS)\n222|\n223|    def _open(self, name, mode='rb'):\n224|        return File(open(self.path(name), mode))\n225|\n226|    def _save(self, name, content):\n227|        full_path = self.path(name)\n228|\n229|        # Create any intermediate directories that do not exist.\n230|        directory = os.path.dirname(full_path)\n231|        try:\n232|            if self.directory_permissions_mode is not None:\n233|                # os.makedirs applies the global umask, so we reset it,\n234|                # for consistency with file_permissions_mode behavior.\n235|                old_umask = os.umask(0)\n236|                try:\n237|                    os.makedirs(directory, self.directory_permissions_mode, exist_ok=True)\n238|                finally:\n239|                    os.umask(old_umask)\n240|            else:\n241|                os.makedirs(directory, exist_ok=True)\n242|        except FileExistsError:\n243|            raise FileExistsError('%s exists and is not a directory.' % directory)\n244|\n245|        # There's a potential race condition between get_available_name and\n246|        # saving the file; it's possible that two threads might return the\n247|        # same name, at which point all sorts of fun happens. So we need to\n248|        # try to create the file, but if it already exists we have to go back\n249|        # to get_available_name() and try again.\n250|\n251|        while True:\n252|            try:\n253|                # This file has a file path that we can move.\n254|                if hasattr(content, 'temporary_file_path'):\n255|                    file_move_safe(content.temporary_file_path(), full_path)\n256|\n257|                # This is a normal uploadedfile that we can stream.\n258|                else:\n259|                    # The current umask value is masked out by os.open!\n260|                    fd = os.open(full_path, self.OS_OPEN_FLAGS, 0o666)\n261|                    _file = None\n262|                    try:\n263|                        locks.lock(fd, locks.LOCK_EX)\n264|                        for chunk in content.chunks():\n265|                            if _file is None:\n266|                                mode = 'wb' if isinstance(chunk, bytes) else 'wt'\n267|                                _file = os.fdopen(fd, mode)\n268|                            _file.write(chunk)\n269|                    finally:\n270|                        locks.unlock(fd)\n271|                        if _file is not None:\n272|                            _file.close()\n273|                        else:\n274|                            os.close(fd)\n275|            except FileExistsError:\n276|                # A new name is needed if the file exists.\n277|                name = self.get_available_name(name)\n278|                full_path = self.path(name)\n279|            else:\n280|                # OK, the file save worked. Break out of the loop.\n281|                break\n282|\n283|        if self.file_permissions_mode is not None:\n284|            os.chmod(full_path, self.file_permissions_mode)\n285|\n286|        # Store filenames with forward slashes, even on Windows.\n287|        return name.replace('\\\\', '/')\n288|\n289|    def delete(self, name):\n290|        assert name, \"The name argument is not allowed to be empty.\"\n291|        name = self.path(name)\n292|        # If the file or directory exists, delete it from the filesystem.\n293|        try:\n294|            if os.path.isdir(name):\n295|                os.rmdir(name)\n296|            else:\n297|                os.remove(name)\n...\n\n\n### django/core/files/uploadedfile.py\n...\n45|            if len(name) > 255:\n46|                name, ext = os.path.splitext(name)\n47|                ext = ext[:255]\n48|                name = name[:255 - len(ext)] + ext\n49|\n50|        self._name = name\n51|\n52|    name = property(_get_name, _set_name)\n53|\n54|\n55|class TemporaryUploadedFile(UploadedFile):\n56|    \"\"\"\n57|    A file uploaded to a temporary location (i.e. stream-to-disk).\n58|    \"\"\"\n59|    def __init__(self, name, content_type, size, charset, content_type_extra=None):\n60|        _, ext = os.path.splitext(name)\n61|        file = tempfile.NamedTemporaryFile(suffix='.upload' + ext, dir=settings.FILE_UPLOAD_TEMP_DIR)\n62|        super().__init__(file, name, content_type, size, charset, content_type_extra)\n63|\n64|    def temporary_file_path(self):\n65|        \"\"\"Return the full path of this file.\"\"\"\n66|        return self.file.name\n67|\n68|    def close(self):\n69|        try:\n70|            return self.file.close()\n71|        except FileNotFoundError:\n72|            # The file was moved or deleted before the tempfile could unlink\n73|            # it. Still sets self.file.close_called and calls\n74|            # self.file.file.close() before the exception.\n75|            pass\n76|\n77|\n78|class InMemoryUploadedFile(UploadedFile):\n79|    \"\"\"\n80|    A file uploaded into memory (i.e. stream-to-memory).\n81|    \"\"\"\n82|    def __init__(self, file, field_name, name, content_type, size, charset, content_type_extra=None):\n83|        super().__init__(file, name, content_type, size, charset, content_type_extra)\n84|        self.field_name = field_name\n85|\n86|    def open(self, mode=None):\n87|        self.file.seek(0)\n88|        return self\n89|\n90|    def chunks(self, chunk_size=None):\n91|        self.file.seek(0)\n92|        yield self.read()\n93|\n94|    def multiple_chunks(self, chunk_size=None):\n95|        # Since it's in memory, we'll never have multiple chunks.\n96|        return False\n97|\n98|\n99|class SimpleUploadedFile(InMemoryUploadedFile):\n100|    \"\"\"\n101|    A simple representation of a file, which just has content, size, and a name.\n102|    \"\"\"\n103|    def __init__(self, name, content, content_type='text/plain'):\n104|        content = content or b''\n105|        super().__init__(BytesIO(content), None, name, content_type, len(content), None, None)\n106|\n...\n\n\n### django/core/files/temp.py\n...\n64|            self.file.__enter__()\n65|            return self\n66|\n67|        def __exit__(self, exc, value, tb):\n68|            self.file.__exit__(exc, value, tb)\n69|\n70|    NamedTemporaryFile = TemporaryFile\n71|else:\n72|    NamedTemporaryFile = tempfile.NamedTemporaryFile\n73|\n74|gettempdir = tempfile.gettempdir\n\n\n\n\n###\n\nPlease provide the class name, function or method name, or the exact line numbers that need to be edited.\nThe possible location outputs should be either \"class\", \"function\" or \"line\".\n\n### Examples:\n```\nfull_path1/file1.py\nline: 10\nclass: MyClass1\nline: 51\n\nfull_path2/file2.py\nfunction: MyClass2.my_method\nline: 12\n\nfull_path3/file3.py\nfunction: my_function\nline: 24\nline: 156\n```\n\nReturn just the location(s) wrapped with ```.\n", "response": ["```\ndjango/core/files/storage.py\nline: 217\nline: 260\n\ndjango/core/files/uploadedfile.py\nline: 61\n```", "```\ndjango/core/files/storage.py\nline: 178\nline: 283\n\ndjango/core/files/uploadedfile.py\nline: 61\n```", "```\ndjango/core/files/storage.py\nline: 178\nline: 283\n\ndjango/core/files/uploadedfile.py\nline: 61\n```\n", "```\ndjango/core/files/storage.py\nline: 217\nline: 283\n\ndjango/core/files/uploadedfile.py\nline: 61\n```"], "usage": {"completion_tokens": 133, "prompt_tokens": 2335}}}
