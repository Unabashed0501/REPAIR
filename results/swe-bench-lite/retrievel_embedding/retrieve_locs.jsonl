{"instance_id": "astropy__astropy-12907", "found_files": [], "node_info": [], "traj": {"usage": {"embedding_tokens": 0}}}
{"instance_id": "astropy__astropy-14182", "found_files": [], "node_info": [], "traj": {"usage": {"embedding_tokens": 0}}}
{"instance_id": "astropy__astropy-7746", "found_files": ["astropy/wcs/wcs.py", "astropy/wcs/utils.py", "astropy/wcs/docstrings.py", "astropy/wcs/__init__.py"], "node_info": [{"code": "SingularMatrixError\n            Linear transformation matrix is singular.\n\n        InconsistentAxisTypesError\n            Inconsistent or unrecognized coordinate axis types.\n\n        ValueError\n            Invalid parameter value.\n\n        ValueError\n            Invalid coordinate transformation parameters.\n\n        ValueError\n            x- and y-coordinate arrays are not the same size.\n\n        InvalidTransformError\n            Invalid coordinate transformation parameters.\n\n        InvalidTransformError\n            Ill-conditioned coordinate transformation parameters.\n        \"\"\".format(__.TWO_OR_MORE_ARGS('naxis', 8),\n                   __.RA_DEC_ORDER(8),\n                   __.RETURNS('sky coordinates, in degrees', 8))\n\n    def wcs_pix2world(self, *args, **kwargs):\n        if self.wcs is None:\n            raise ValueError(\"No basic WCS settings were created.\")\n        return self._array_converter(\n            lambda xy, o: self.wcs.p2s(xy, o)['world'],\n            'output', *args, **kwargs)\n    wcs_pix2world.__doc__ = \"\"\"\n        Transforms pixel coordinates to world coordinates by doing\n        only the basic `wcslib`_ transformation.\n\n        No `SIP`_ or `distortion paper`_ table lookup correction is\n        applied.  To perform distortion correction, see\n        `~astropy.wcs.WCS.all_pix2world`,\n        `~astropy.wcs.WCS.sip_pix2foc`, `~astropy.wcs.WCS.p4_pix2foc`,\n        or `~astropy.wcs.WCS.pix2foc`.\n\n        Parameters\n        ----------\n        {0}\n\n            For a transformation that is not two-dimensional, the\n            two-argument form must be used.\n\n        {1}\n\n        Returns\n        -------\n\n        {2}\n\n        Raises\n        ------\n        MemoryError\n            Memory allocation failed.\n\n        SingularMatrixError\n            Linear transformation matrix is singular.\n\n        InconsistentAxisTypesError\n            Inconsistent or unrecognized coordinate axis types.\n\n        ValueError\n            Invalid parameter value.", "metadata": {"file_name": "astropy/wcs/wcs.py", "File Name": "astropy/wcs/wcs.py", "Classes": "NoConvergence, FITSFixedWarning, WCS, _WcsValidateWcsResult, _WcsValidateHduResult, _WcsValidateResults", "Functions": "_parse_keysel, __WCS_unpickle__, find_all_wcs, validate, write_d2i, write_dist, write_array, _return_list_of_arrays, _return_single_array"}}, {"code": "\"\"\".format(__.TWO_OR_MORE_ARGS('naxis', 8),\n                   __.RA_DEC_ORDER(8),\n                   __.RETURNS('pixel coordinates', 8))\n\n    def wcs_world2pix(self, *args, **kwargs):\n        if self.wcs is None:\n            raise ValueError(\"No basic WCS settings were created.\")\n        return self._array_converter(\n            lambda xy, o: self.wcs.s2p(xy, o)['pixcrd'],\n            'input', *args, **kwargs)\n    wcs_world2pix.__doc__ = \"\"\"\n        Transforms world coordinates to pixel coordinates, using only\n        the basic `wcslib`_ WCS transformation.  No `SIP`_ or\n        `distortion paper`_ table lookup transformation is applied.\n\n        Parameters\n        ----------\n        {0}\n\n            For a transformation that is not two-dimensional, the\n            two-argument form must be used.\n\n        {1}\n\n        Returns\n        -------\n\n        {2}\n\n        Notes\n        -----\n        The order of the axes for the input world array is determined by\n        the ``CTYPEia`` keywords in the FITS header, therefore it may\n        not always be of the form (*ra*, *dec*).  The\n        `~astropy.wcs.Wcsprm.lat`, `~astropy.wcs.Wcsprm.lng`,\n        `~astropy.wcs.Wcsprm.lattyp` and `~astropy.wcs.Wcsprm.lngtyp`\n        members can be used to determine the order of the axes.\n\n        Raises\n        ------\n        MemoryError\n            Memory allocation failed.\n\n        SingularMatrixError\n            Linear transformation matrix is singular.\n\n        InconsistentAxisTypesError\n            Inconsistent or unrecognized coordinate axis types.\n\n        ValueError\n            Invalid parameter value.\n\n        ValueError\n            Invalid coordinate transformation parameters.\n\n        ValueError\n            x- and y-coordinate arrays are not the same size.\n\n        InvalidTransformError\n            Invalid coordinate transformation parameters.", "metadata": {"file_name": "astropy/wcs/wcs.py", "File Name": "astropy/wcs/wcs.py", "Classes": "NoConvergence, FITSFixedWarning, WCS, _WcsValidateWcsResult, _WcsValidateHduResult, _WcsValidateResults", "Functions": "_parse_keysel, __WCS_unpickle__, find_all_wcs, validate, write_d2i, write_dist, write_array, _return_list_of_arrays, _return_single_array"}}, {"code": "return _return_list_of_arrays(axes, origin)\n\n        raise TypeError(\n            \"WCS projection has {0} dimensions, so expected 2 (an Nx{0} array \"\n            \"and the origin argument) or {1} arguments (the position in each \"\n            \"dimension, and the origin argument). Instead, {2} arguments were \"\n            \"given.\".format(\n                self.naxis, self.naxis + 1, len(args)))\n\n    def all_pix2world(self, *args, **kwargs):\n        return self._array_converter(\n            self._all_pix2world, 'output', *args, **kwargs)\n    all_pix2world.__doc__ = \"\"\"\n        Transforms pixel coordinates to world coordinates.\n\n        Performs all of the following in series:\n\n            - Detector to image plane correction (if present in the\n              FITS file)\n\n            - `SIP`_ distortion correction (if present in the FITS\n              file)\n\n            - `distortion paper`_ table-lookup correction (if present\n              in the FITS file)\n\n            - `wcslib`_ \"core\" WCS transformation\n\n        Parameters\n        ----------\n        {0}\n\n            For a transformation that is not two-dimensional, the\n            two-argument form must be used.\n\n        {1}\n\n        Returns\n        -------\n\n        {2}\n\n        Notes\n        -----\n        The order of the axes for the result is determined by the\n        ``CTYPEia`` keywords in the FITS header, therefore it may not\n        always be of the form (*ra*, *dec*).  The\n        `~astropy.wcs.Wcsprm.lat`, `~astropy.wcs.Wcsprm.lng`,\n        `~astropy.wcs.Wcsprm.lattyp` and `~astropy.wcs.Wcsprm.lngtyp`\n        members can be used to determine the order of the axes.\n\n        Raises\n        ------\n        MemoryError\n            Memory allocation failed.", "metadata": {"file_name": "astropy/wcs/wcs.py", "File Name": "astropy/wcs/wcs.py", "Classes": "NoConvergence, FITSFixedWarning, WCS, _WcsValidateWcsResult, _WcsValidateHduResult, _WcsValidateResults", "Functions": "_parse_keysel, __WCS_unpickle__, find_all_wcs, validate, write_d2i, write_dist, write_array, _return_list_of_arrays, _return_single_array"}}, {"code": "ValueError\n            Invalid coordinate transformation parameters.\n\n        ValueError\n            x- and y-coordinate arrays are not the same size.\n\n        InvalidTransformError\n            Invalid coordinate transformation parameters.\n\n        InvalidTransformError\n            Ill-conditioned coordinate transformation parameters.\n\n        Notes\n        -----\n        The order of the axes for the result is determined by the\n        ``CTYPEia`` keywords in the FITS header, therefore it may not\n        always be of the form (*ra*, *dec*).  The\n        `~astropy.wcs.Wcsprm.lat`, `~astropy.wcs.Wcsprm.lng`,\n        `~astropy.wcs.Wcsprm.lattyp` and `~astropy.wcs.Wcsprm.lngtyp`\n        members can be used to determine the order of the axes.", "metadata": {"file_name": "astropy/wcs/wcs.py", "File Name": "astropy/wcs/wcs.py", "Classes": "NoConvergence, FITSFixedWarning, WCS, _WcsValidateWcsResult, _WcsValidateHduResult, _WcsValidateResults", "Functions": "_parse_keysel, __WCS_unpickle__, find_all_wcs, validate, write_d2i, write_dist, write_array, _return_list_of_arrays, _return_single_array"}}, {"code": "_normalize_sky(output)\n                return (output[:, 0].reshape(axes[0].shape),\n                        output[:, 1].reshape(axes[0].shape))\n            return [output[:, i].reshape(axes[0].shape)\n                    for i in range(output.shape[1])]\n\n        def _return_single_array(xy, origin):\n            if xy.shape[-1] != self.naxis:\n                raise ValueError(\n                    \"When providing two arguments, the array must be \"\n                    \"of shape (N, {0})\".format(self.naxis))\n            if ra_dec_order and sky == 'input':\n                xy = self._denormalize_sky(xy)\n            result = func(xy, origin)\n            if ra_dec_order and sky == 'output':\n                result = self._normalize_sky(result)\n            return result\n\n        if len(args) == 2:\n            try:\n                xy, origin = args\n                xy = np.asarray(xy)\n                origin = int(origin)\n            except Exception:\n                raise TypeError(\n                    \"When providing two arguments, they must be \"\n                    \"(coords[N][{0}], origin)\".format(self.naxis))\n            if self.naxis == 1 and len(xy.shape) == 1:\n                return _return_list_of_arrays([xy], origin)\n            return _return_single_array(xy, origin)\n\n        elif len(args) == self.naxis + 1:\n            axes = args[:-1]\n            origin = args[-1]\n            try:\n                axes = [np.asarray(x) for x in axes]\n                origin = int(origin)\n            except Exception:\n                raise TypeError(\n                    \"When providing more than two arguments, they must be \" +\n                    \"a 1-D array for each axis, followed by an origin.\")", "metadata": {"file_name": "astropy/wcs/wcs.py", "File Name": "astropy/wcs/wcs.py", "Classes": "NoConvergence, FITSFixedWarning, WCS, _WcsValidateWcsResult, _WcsValidateHduResult, _WcsValidateResults", "Functions": "_parse_keysel, __WCS_unpickle__, find_all_wcs, validate, write_d2i, write_dist, write_array, _return_list_of_arrays, _return_single_array"}}, {"code": "raise TypeError(\"'{0}' object is not iterable\".format(self.__class__.__name__))\n\n    @property\n    def axis_type_names(self):\n        \"\"\"\n        World names for each coordinate axis\n\n        Returns\n        -------\n        A list of names along each axis\n        \"\"\"\n        names = list(self.wcs.cname)\n        types = self.wcs.ctype\n        for i in range(len(names)):\n            if len(names[i]) > 0:\n                continue\n            names[i] = types[i].split('-')[0]\n        return names\n\n    @property\n    def celestial(self):\n        \"\"\"\n        A copy of the current WCS with only the celestial axes included\n        \"\"\"\n        return self.sub([WCSSUB_CELESTIAL])\n\n    @property\n    def is_celestial(self):\n        return self.has_celestial and self.naxis == 2\n\n    @property\n    def has_celestial(self):\n        try:\n            return self.celestial.naxis == 2\n        except InconsistentAxisTypesError:\n            return False\n\n    @property\n    def pixel_scale_matrix(self):\n\n        try:\n            cdelt = np.matrix(np.diag(self.wcs.get_cdelt()))\n            pc = np.matrix(self.wcs.get_pc())\n        except InconsistentAxisTypesError:\n            try:\n                # for non-celestial axes, get_cdelt doesn't work\n                cdelt = np.matrix(self.wcs.cd) * np.matrix(np.diag(self.wcs.cdelt))\n            except AttributeError:\n                cdelt = np.matrix(np.diag(self.wcs.cdelt))\n\n            try:\n                pc = np.matrix(self.wcs.pc)\n            except AttributeError:\n                pc = 1\n\n        pccd = np.array(cdelt * pc)\n\n        return pccd\n\n    def _as_mpl_axes(self):\n        \"\"\"\n        Compatibility hook for Matplotlib and WCSAxes.", "metadata": {"file_name": "astropy/wcs/wcs.py", "File Name": "astropy/wcs/wcs.py", "Classes": "NoConvergence, FITSFixedWarning, WCS, _WcsValidateWcsResult, _WcsValidateHduResult, _WcsValidateResults", "Functions": "_parse_keysel, __WCS_unpickle__, find_all_wcs, validate, write_d2i, write_dist, write_array, _return_list_of_arrays, _return_single_array"}}, {"code": "translate_units : str, optional\n        Specify which potentially unsafe translations of non-standard\n        unit strings to perform.  By default, performs none.  See\n        `WCS.fix` for more information about this parameter.  Only\n        effective when ``fix`` is `True`.\n\n    Raises\n    ------\n    MemoryError\n         Memory allocation failed.\n\n    ValueError\n         Invalid key.\n\n    KeyError\n         Key not found in FITS header.\n\n    ValueError\n         Lookup table distortion present in the header but *fobj* was\n         not provided.\n\n    Notes\n    -----\n\n    1. astropy.wcs supports arbitrary *n* dimensions for the core WCS\n       (the transformations handled by WCSLIB).  However, the\n       `distortion paper`_ lookup table and `SIP`_ distortions must be\n       two dimensional.  Therefore, if you try to create a WCS object\n       where the core WCS has a different number of dimensions than 2\n       and that object also contains a `distortion paper`_ lookup\n       table or `SIP`_ distortion, a `ValueError`\n       exception will be raised.  To avoid this, consider using the\n       *naxis* kwarg to select two dimensions from the core WCS.\n\n    2. The number of coordinate axes in the transformation is not\n       determined directly from the ``NAXIS`` keyword but instead from\n       the highest of:\n\n           - ``NAXIS`` keyword\n\n           - ``WCSAXESa`` keyword\n\n           - The highest axis number in any parameterized WCS keyword.\n             The keyvalue, as well as the keyword, must be\n             syntactically valid otherwise it will not be considered.\n\n       If none of these keyword types is present, i.e. if the header\n       only contains auxiliary WCS keywords for a particular\n       coordinate representation, then no coordinate description is\n       constructed for it.", "metadata": {"file_name": "astropy/wcs/wcs.py", "File Name": "astropy/wcs/wcs.py", "Classes": "NoConvergence, FITSFixedWarning, WCS, _WcsValidateWcsResult, _WcsValidateHduResult, _WcsValidateResults", "Functions": "_parse_keysel, __WCS_unpickle__, find_all_wcs, validate, write_d2i, write_dist, write_array, _return_list_of_arrays, _return_single_array"}}, {"code": "#\n        #\n        #                ###  IMPORTANT NOTE:  ###\n        #\n        # If, in the future releases of the `~astropy.wcs`,\n        # `pix2foc` will not apply all the required distortion\n        # corrections then in the code below, calls to `pix2foc` will\n        # have to be replaced with\n        # wcs_world2pix(all_pix2world(pix_list, origin), origin)\n        #\n\n        # ############################################################\n        # #            INITIALIZE ITERATIVE PROCESS:                ##\n        # ############################################################\n\n        # initial approximation (linear WCS based only)\n        pix0 = self.wcs_world2pix(world, origin)\n\n        # Check that an iterative solution is required at all\n        # (when any of the non-CD-matrix-based corrections are\n        # present). If not required return the initial\n        # approximation (pix0).\n        if self.sip is None and \\\n           self.cpdis1 is None and self.cpdis2 is None and \\\n           self.det2im1 is None and self.det2im2 is None:\n            # No non-WCS corrections detected so\n            # simply return initial approximation:\n            return pix0\n\n        pix = pix0.copy()  # 0-order solution\n\n        # initial correction:\n        dpix = self.pix2foc(pix, origin) - pix0\n\n        # Update initial solution:\n        pix -= dpix\n\n        # Norm (L2) squared of the correction:\n        dn = np.sum(dpix*dpix, axis=1)\n        dnprev = dn.copy()  # if adaptive else dn\n        tol2 = tolerance**2\n\n        # Prepare for iterative process\n        k = 1\n        ind = None\n        inddiv = None\n\n        # Turn off numpy runtime warnings for 'invalid' and 'over':\n        old_invalid = np.geterr()['invalid']\n        old_over = np.geterr()['over']\n        np.", "metadata": {"file_name": "astropy/wcs/wcs.py", "File Name": "astropy/wcs/wcs.py", "Classes": "NoConvergence, FITSFixedWarning, WCS, _WcsValidateWcsResult, _WcsValidateHduResult, _WcsValidateResults", "Functions": "_parse_keysel, __WCS_unpickle__, find_all_wcs, validate, write_d2i, write_dist, write_array, _return_list_of_arrays, _return_single_array"}}, {"code": "It is prone to \"RA wrapping\" issues as described in\n        # https://github.com/astropy/astropy/issues/1977\n        # (essentially because `all_pix2world` may return points with\n        # a different phase than user's input `w`).\n        #\n        #\n        #      ### Description of the Method Used here ###\n        #\n        #\n        # By applying inverse linear WCS transformation (`W^{-1}`)\n        # to both sides of equation (2) and introducing notation `x'`\n        # (prime) for the pixels coordinates obtained from the world\n        # coordinates by applying inverse *linear* WCS transformation\n        # (\"focal plane coordinates\"):\n        #\n        # (3)   x' = W^{-1}(w)\n        #\n        # we obtain the following equation:\n        #\n        # (4)   x' = x+f(x),\n        #\n        # or,\n        #\n        # (5)   x = x'-f(x)\n        #\n        # This equation is well suited for solving using the method\n        # of fixed-point iterations\n        # (http://en.wikipedia.org/wiki/Fixed-point_iteration):\n        #\n        # (6)   x_{i+1} = x'-f(x_i)\n        #\n        # As an initial value of the pixel coordinate `x_0` we take\n        # \"focal plane coordinate\" `x'=W^{-1}(w)=wcs_world2pix(w)`.\n        # We stop iterations when `|x_{i+1}-x_i|<tolerance`.", "metadata": {"file_name": "astropy/wcs/wcs.py", "File Name": "astropy/wcs/wcs.py", "Classes": "NoConvergence, FITSFixedWarning, WCS, _WcsValidateWcsResult, _WcsValidateHduResult, _WcsValidateResults", "Functions": "_parse_keysel, __WCS_unpickle__, find_all_wcs, validate, write_d2i, write_dist, write_array, _return_list_of_arrays, _return_single_array"}}, {"code": ".. note::\n               Indices of the diverging inverse solutions will be\n               reported in the ``divergent`` attribute of the\n               raised :py:class:`NoConvergence` exception object.\n\n        Returns\n        -------\n\n        {2}\n\n        Notes\n        -----\n        The order of the axes for the input world array is determined by\n        the ``CTYPEia`` keywords in the FITS header, therefore it may\n        not always be of the form (*ra*, *dec*).  The\n        `~astropy.wcs.Wcsprm.lat`, `~astropy.wcs.Wcsprm.lng`,\n        `~astropy.wcs.Wcsprm.lattyp`, and\n        `~astropy.wcs.Wcsprm.lngtyp`\n        members can be used to determine the order of the axes.\n\n        Using the method of fixed-point iterations approximations we\n        iterate starting with the initial approximation, which is\n        computed using the non-distortion-aware\n        :py:meth:`wcs_world2pix` (or equivalent).\n\n        The :py:meth:`all_world2pix` function uses a vectorized\n        implementation of the method of consecutive approximations and\n        therefore it is highly efficient (>30x) when *all* data points\n        that need to be converted from sky coordinates to image\n        coordinates are passed at *once*. Therefore, it is advisable,\n        whenever possible, to pass as input a long array of all points\n        that need to be converted to :py:meth:`all_world2pix` instead\n        of calling :py:meth:`all_world2pix` for each data point. Also\n        see the note to the ``adaptive`` parameter.\n\n        Raises\n        ------\n        NoConvergence\n            The method did not converge to a\n            solution to the required accuracy within a specified\n            number of maximum iterations set by the ``maxiter``\n            parameter. To turn off this exception, set ``quiet`` to\n            `True`.", "metadata": {"file_name": "astropy/wcs/wcs.py", "File Name": "astropy/wcs/wcs.py", "Classes": "NoConvergence, FITSFixedWarning, WCS, _WcsValidateWcsResult, _WcsValidateHduResult, _WcsValidateResults", "Functions": "_parse_keysel, __WCS_unpickle__, find_all_wcs, validate, write_d2i, write_dist, write_array, _return_list_of_arrays, _return_single_array"}}, {"code": "# However, I prefer to use `w` for \"intermediate world\n        # coordinates\", `x` for pixel coordinates, and assume that\n        # transformation `W` performs the **linear**\n        # (CD matrix + projection onto celestial sphere) part of the\n        # conversion from pixel coordinates to world coordinates.\n        # Then we can re-write (1) as:\n        #\n        # (2)   w = W*(x+f(x)) = T(x)\n        #\n        # In `astropy.wcs.WCS` transformation `W` is represented by\n        # the `wcs_pix2world` member, while the combined (\"total\")\n        # transformation (linear part + distortions) is performed by\n        # `all_pix2world`. Below I summarize the notations and their\n        # equivalents in `astropy.wcs.WCS`:\n        #\n        # | Equation term | astropy.WCS/meaning          |\n        # | ------------- | ---------------------------- |\n        # | `x`           | pixel coordinates            |\n        # | `w`           | world coordinates            |\n        # | `W`           | `wcs_pix2world()`            |\n        # | `W^{-1}`      | `wcs_world2pix()`            |\n        # | `T`           | `all_pix2world()`            |\n        # | `x+f(x)`      | `pix2foc()`                  |\n        #\n        #\n        #      ### Direct Solving of Equation (2)  ###\n        #\n        #\n        # In order to find the pixel coordinates that correspond to\n        # given world coordinates `w`, it is necessary to invert\n        # equation (2): `x=T^{-1}(w)`, or solve equation `w==T(x)`\n        # for `x`. However, this approach has the following\n        # disadvantages:\n        #    1. It requires unnecessary transformations (see next\n        #       section).\n        #    2.", "metadata": {"file_name": "astropy/wcs/wcs.py", "File Name": "astropy/wcs/wcs.py", "Classes": "NoConvergence, FITSFixedWarning, WCS, _WcsValidateWcsResult, _WcsValidateHduResult, _WcsValidateResults", "Functions": "_parse_keysel, __WCS_unpickle__, find_all_wcs, validate, write_d2i, write_dist, write_array, _return_list_of_arrays, _return_single_array"}}, {"code": "if numpy_order:\n                wcs_index = self.wcs.naxis - 1 - i\n            else:\n                wcs_index = i\n\n            if iview.step is not None and iview.start is None:\n                # Slice from \"None\" is equivalent to slice from 0 (but one\n                # might want to downsample, so allow slices with\n                # None,None,step or None,stop,step)\n                iview = slice(0, iview.stop, iview.step)\n\n            if iview.start is not None:\n                if iview.step not in (None, 1):\n                    crpix = self.wcs.crpix[wcs_index]\n                    cdelt = self.wcs.cdelt[wcs_index]\n                    # equivalently (keep this comment so you can compare eqns):\n                    # wcs_new.wcs.crpix[wcs_index] =\n                    # (crpix - iview.start)*iview.step + 0.5 - iview.step/2.\n                    crp = ((crpix - iview.start - 1.)/iview.step\n                           + 0.5 + 1./iview.step/2.)\n                    wcs_new.wcs.crpix[wcs_index] = crp\n                    if wcs_new.sip is not None:\n                        sip_crpix[wcs_index] = crp\n                    wcs_new.wcs.cdelt[wcs_index] = cdelt * iview.step\n                else:\n                    wcs_new.wcs.crpix[wcs_index] -= iview.start\n                    if wcs_new.sip is not None:\n                        sip_crpix[wcs_index] -= iview.start\n\n            try:\n                # range requires integers but the other attributes can also\n                # handle arbitary values, so this needs to be in a try/except.", "metadata": {"file_name": "astropy/wcs/wcs.py", "File Name": "astropy/wcs/wcs.py", "Classes": "NoConvergence, FITSFixedWarning, WCS, _WcsValidateWcsResult, _WcsValidateHduResult, _WcsValidateResults", "Functions": "_parse_keysel, __WCS_unpickle__, find_all_wcs, validate, write_d2i, write_dist, write_array, _return_list_of_arrays, _return_single_array"}}, {"code": ".format(k), best_solution=pix,\n                    accuracy=np.abs(dpix), niter=k,\n                    slow_conv=ind, divergent=inddiv)\n\n        return pix\n\n    def all_world2pix(self, *args, tolerance=1e-4, maxiter=20, adaptive=False,\n                      detect_divergence=True, quiet=False, **kwargs):\n        if self.wcs is None:\n            raise ValueError(\"No basic WCS settings were created.\")\n\n        return self._array_converter(\n            lambda *args, **kwargs:\n            self._all_world2pix(\n                *args, tolerance=tolerance, maxiter=maxiter,\n                adaptive=adaptive, detect_divergence=detect_divergence,\n                quiet=quiet),\n            'input', *args, **kwargs\n        )\n\n    all_world2pix.__doc__ = \"\"\"\n        all_world2pix(*arg, accuracy=1.0e-4, maxiter=20,\n        adaptive=False, detect_divergence=True, quiet=False)\n\n        Transforms world coordinates to pixel coordinates, using\n        numerical iteration to invert the full forward transformation\n        `~astropy.wcs.WCS.all_pix2world` with complete\n        distortion model.\n\n\n        Parameters\n        ----------\n        {0}\n\n            For a transformation that is not two-dimensional, the\n            two-argument form must be used.\n\n        {1}\n\n        tolerance : float, optional (Default = 1.0e-4)\n            Tolerance of solution. Iteration terminates when the\n            iterative solver estimates that the \"true solution\" is\n            within this many pixels current estimate, more\n            specifically, when the correction to the solution found\n            during the previous iteration is smaller\n            (in the sense of the L2 norm) than ``tolerance``.\n\n        maxiter : int, optional (Default = 20)\n            Maximum number of iterations allowed to reach a solution.", "metadata": {"file_name": "astropy/wcs/wcs.py", "File Name": "astropy/wcs/wcs.py", "Classes": "NoConvergence, FITSFixedWarning, WCS, _WcsValidateWcsResult, _WcsValidateHduResult, _WcsValidateResults", "Functions": "_parse_keysel, __WCS_unpickle__, find_all_wcs, validate, write_d2i, write_dist, write_array, _return_list_of_arrays, _return_single_array"}}, {"code": "By using `pix2foc` at each iteration instead\n        # of `all_pix2world` we get about 25% increase in performance\n        # (by not performing the linear `W` transformation at each\n        # step) and we also avoid the \"RA wrapping\" issue described\n        # above (by working in focal plane coordinates and avoiding\n        # pix->world transformations).\n        #\n        # As an added benefit, the process converges to the correct\n        # solution in just one iteration when distortions are not\n        # present (compare to\n        # https://github.com/astropy/astropy/issues/1977 and\n        # https://github.com/astropy/astropy/pull/2294): in this case\n        # `pix2foc` is the identical transformation\n        # `x_i=pix2foc(x_i)` and from equation (7) we get:\n        #\n        # x' = x_0 = wcs_world2pix(w)\n        # x_1 = x' - pix2foc(x_0) + x_0 = x' - pix2foc(x') + x' = x'\n        #     = wcs_world2pix(w) = x_0\n        # =>\n        # |x_1-x_0| = 0 < tolerance (with tolerance > 0)\n        #\n        # However, for performance reasons, it is still better to\n        # avoid iterations altogether and return the exact linear\n        # solution (`wcs_world2pix`) right-away when non-linear\n        # distortions are not present by checking that attributes\n        # `sip`, `cpdis1`, `cpdis2`, `det2im1`, and `det2im2` are\n        # *all* `None`.", "metadata": {"file_name": "astropy/wcs/wcs.py", "File Name": "astropy/wcs/wcs.py", "Classes": "NoConvergence, FITSFixedWarning, WCS, _WcsValidateWcsResult, _WcsValidateHduResult, _WcsValidateResults", "Functions": "_parse_keysel, __WCS_unpickle__, find_all_wcs, validate, write_d2i, write_dist, write_array, _return_list_of_arrays, _return_single_array"}}, {"code": "Indices of the points for which the requested\n            accuracy was not achieved (if any) will be listed in the\n            ``slow_conv`` attribute of the\n            raised :py:class:`NoConvergence` exception object.\n\n            See :py:class:`NoConvergence` documentation for\n            more details.\n\n        MemoryError\n            Memory allocation failed.\n\n        SingularMatrixError\n            Linear transformation matrix is singular.\n\n        InconsistentAxisTypesError\n            Inconsistent or unrecognized coordinate axis types.\n\n        ValueError\n            Invalid parameter value.\n\n        ValueError\n            Invalid coordinate transformation parameters.\n\n        ValueError\n            x- and y-coordinate arrays are not the same size.\n\n        InvalidTransformError\n            Invalid coordinate transformation parameters.\n\n        InvalidTransformError\n            Ill-conditioned coordinate transformation parameters.\n\n        Examples\n        --------\n        >>> import astropy.io.fits as fits\n        >>> import astropy.wcs as wcs\n        >>> import numpy as np\n        >>> import os\n\n        >>> filename = os.path.join(wcs.__path__[0], 'tests/data/j94f05bgq_flt.fits')\n        >>> hdulist = fits.open(filename)\n        >>> w = wcs.WCS(hdulist[('sci',1)].header, hdulist)\n        >>> hdulist.close()\n\n        >>> ra, dec = w.all_pix2world([1,2,3], [1,1,1], 1)\n        >>> print(ra)  # doctest: +FLOAT_CMP\n        [ 5.52645627  5.52649663  5.52653698]\n        >>> print(dec)  # doctest: +FLOAT_CMP\n        [-72.05171757 -72.05171276 -72.05170795]\n        >>> radec = w.all_pix2world([[1,1], [2,1], [3,1]], 1)\n        >>> print(radec)  # doctest: +FLOAT_CMP\n        [[  5.", "metadata": {"file_name": "astropy/wcs/wcs.py", "File Name": "astropy/wcs/wcs.py", "Classes": "NoConvergence, FITSFixedWarning, WCS, _WcsValidateWcsResult, _WcsValidateHduResult, _WcsValidateResults", "Functions": "_parse_keysel, __WCS_unpickle__, find_all_wcs, validate, write_d2i, write_dist, write_array, _return_list_of_arrays, _return_single_array"}}, {"code": "lattyp != 'DEC':\n            raise ValueError(\n                \"WCS does not have longitude type of 'DEC', therefore \" +\n                \"(ra, dec) data can not be returned\")\n        if self.wcs.naxis == 2:\n            if self.wcs.lng == 0 and self.wcs.lat == 1:\n                return sky\n            elif self.wcs.lng == 1 and self.wcs.lat == 0:\n                # Reverse the order of the columns\n                return sky[:, ::-1]\n            else:\n                raise ValueError(\n                    \"WCS does not have longitude and latitude celestial \"\n                    \"axes, therefore (ra, dec) data can not be returned\")\n        else:\n            if self.wcs.lng < 0 or self.wcs.lat < 0:\n                raise ValueError(\n                    \"WCS does not have both longitude and latitude celestial \"\n                    \"axes, therefore (ra, dec) data can not be returned\")\n            out = np.empty((sky.shape[0], 2))\n            out[:, 0] = sky[:, self.wcs.lng]\n            out[:, 1] = sky[:, self.wcs.lat]\n            return out\n\n    def _array_converter(self, func, sky, *args, ra_dec_order=False):\n        \"\"\"\n        A helper function to support reading either a pair of arrays\n        or a single Nx2 array.\n        \"\"\"\n\n        def _return_list_of_arrays(axes, origin):\n            try:\n                axes = np.broadcast_arrays(*axes)\n            except ValueError:\n                raise ValueError(\n                    \"Coordinate arrays are not broadcastable to each other\")\n\n            xy = np.hstack([x.reshape((x.size, 1)) for x in axes])\n\n            if ra_dec_order and sky == 'input':\n                xy = self._denormalize_sky(xy)\n            output = func(xy, origin)\n            if ra_dec_order and sky == 'output':\n                output = self.", "metadata": {"file_name": "astropy/wcs/wcs.py", "File Name": "astropy/wcs/wcs.py", "Classes": "NoConvergence, FITSFixedWarning, WCS, _WcsValidateWcsResult, _WcsValidateHduResult, _WcsValidateResults", "Functions": "_parse_keysel, __WCS_unpickle__, find_all_wcs, validate, write_d2i, write_dist, write_array, _return_list_of_arrays, _return_single_array"}}, {"code": "import _wcs\nexcept ImportError:\n    if not _ASTROPY_SETUP_:\n        raise\n    else:\n        _wcs = None\n\nfrom ..utils.compat import possible_filename\nfrom ..utils.exceptions import AstropyWarning, AstropyUserWarning, AstropyDeprecationWarning\n\n__all__ = ['FITSFixedWarning', 'WCS', 'find_all_wcs',\n           'DistortionLookupTable', 'Sip', 'Tabprm', 'Wcsprm',\n           'WCSBase', 'validate', 'WcsError', 'SingularMatrixError',\n           'InconsistentAxisTypesError', 'InvalidTransformError',\n           'InvalidCoordinateError', 'NoSolutionError',\n           'InvalidSubimageSpecificationError', 'NoConvergence',\n           'NonseparableSubimageCoordinateSystemError',\n           'NoWcsKeywordsFoundError', 'InvalidTabularParametersError']\n\n\n__doctest_skip__ = ['WCS.all_world2pix']\n\n\nif _wcs is not None:\n    _parsed_version = _wcs.__version__.split('.')\n    if int(_parsed_version[0]) == 5 and int(_parsed_version[1]) < 8:\n        raise ImportError(\n            \"astropy.wcs is built with wcslib {0}, but only versions 5.8 and \"\n            \"later on the 5.x series are known to work.  The version of wcslib \"\n            \"that ships with astropy may be used.\")\n\n    if not _wcs._sanity_check():\n        raise RuntimeError(\n        \"astropy.wcs did not pass its sanity check for your build \"\n        \"on your platform.\")", "metadata": {"file_name": "astropy/wcs/wcs.py", "File Name": "astropy/wcs/wcs.py", "Classes": "NoConvergence, FITSFixedWarning, WCS, _WcsValidateWcsResult, _WcsValidateHduResult, _WcsValidateResults", "Functions": "_parse_keysel, __WCS_unpickle__, find_all_wcs, validate, write_d2i, write_dist, write_array, _return_list_of_arrays, _return_single_array"}}, {"code": "fobj, err=minerr)\n            cpdis = self._read_distortion_kw(\n                header, fobj, dist='CPDIS', err=minerr)\n            sip = self._read_sip_kw(header, wcskey=key)\n            self._remove_sip_kw(header)\n\n            header_string = header.tostring()\n            header_string = header_string.replace('END' + ' ' * 77, '')\n\n            if isinstance(header_string, str):\n                header_bytes = header_string.encode('ascii')\n                header_string = header_string\n            else:\n                header_bytes = header_string\n                header_string = header_string.decode('ascii')\n\n            try:\n                wcsprm = _wcs.Wcsprm(header=header_bytes, key=key,\n                                     relax=relax, keysel=keysel_flags,\n                                     colsel=colsel)\n            except _wcs.NoWcsKeywordsFoundError:\n                # The header may have SIP or distortions, but no core\n                # WCS.  That isn't an error -- we want a \"default\"\n                # (identity) core Wcs transformation in that case.\n                if colsel is None:\n                    wcsprm = _wcs.Wcsprm(header=None, key=key,\n                                         relax=relax, keysel=keysel_flags,\n                                         colsel=colsel)\n                else:\n                    raise\n\n            if naxis is not None:\n                wcsprm = wcsprm.sub(naxis)\n            self.naxis = wcsprm.naxis\n\n            if (wcsprm.naxis != 2 and\n                (det2im[0] or det2im[1] or cpdis[0] or cpdis[1] or sip)):\n                raise ValueError(\n                    \"\"\"\nFITS WCS distortion paper lookup tables and SIP distortions only work\nin 2 dimensions.  However, WCSLIB has detected {0} dimensions in the\ncore WCS keywords.", "metadata": {"file_name": "astropy/wcs/wcs.py", "File Name": "astropy/wcs/wcs.py", "Classes": "NoConvergence, FITSFixedWarning, WCS, _WcsValidateWcsResult, _WcsValidateHduResult, _WcsValidateResults", "Functions": "_parse_keysel, __WCS_unpickle__, find_all_wcs, validate, write_d2i, write_dist, write_array, _return_list_of_arrays, _return_single_array"}}, {"code": "if cls is None:\n        cls = SkyCoord\n\n    if _has_distortion(wcs) and wcs.naxis != 2:\n        raise ValueError(\"Can only handle WCS with distortions for 2-dimensional WCS\")\n\n    # Keep only the celestial part of the axes, also re-orders lon/lat\n    wcs = wcs.sub([WCSSUB_LONGITUDE, WCSSUB_LATITUDE])\n\n    if wcs.naxis != 2:\n        raise ValueError(\"WCS should contain celestial component\")\n\n    # Check which frame the WCS uses\n    frame = wcs_to_celestial_frame(wcs)\n\n    # Check what unit the WCS gives\n    lon_unit = u.Unit(wcs.wcs.cunit[0])\n    lat_unit = u.Unit(wcs.wcs.cunit[1])\n\n    # Convert pixel coordinates to celestial coordinates\n    if mode == 'all':\n        lon, lat = wcs.all_pix2world(xp, yp, origin)\n    elif mode == 'wcs':\n        lon, lat = wcs.wcs_pix2world(xp, yp, origin)\n    else:\n        raise ValueError(\"mode should be either 'all' or 'wcs'\")\n\n    # Add units to longitude/latitude\n    lon = lon * lon_unit\n    lat = lat * lat_unit\n\n    # Create a SkyCoord-like object\n    data = UnitSphericalRepresentation(lon=lon, lat=lat)\n    coords = cls(frame.realize_frame(data))\n\n    return coords", "metadata": {"file_name": "astropy/wcs/utils.py", "File Name": "astropy/wcs/utils.py", "Classes": "custom_wcs_to_frame_mappings, custom_frame_to_wcs_mappings", "Functions": "add_stokes_axis_to_wcs, _wcs_to_celestial_frame_builtin, _celestial_frame_to_wcs_builtin, wcs_to_celestial_frame, celestial_frame_to_wcs, proj_plane_pixel_scales, proj_plane_pixel_area, is_proj_plane_distorted, _is_cd_orthogonal, non_celestial_pixel_scales, _has_distortion, skycoord_to_pixel, pixel_to_skycoord"}}, {"code": "52645627 -72.05171757]\n         [  5.52649663 -72.05171276]\n         [  5.52653698 -72.05170795]]\n        >>> x, y = w.all_world2pix(ra, dec, 1)\n        >>> print(x)  # doctest: +FLOAT_CMP\n        [ 1.00000238  2.00000237  3.00000236]\n        >>> print(y)  # doctest: +FLOAT_CMP\n        [ 0.99999996  0.99999997  0.99999997]\n        >>> xy = w.all_world2pix(radec, 1)\n        >>> print(xy)  # doctest: +FLOAT_CMP\n        [[ 1.00000238  0.99999996]\n         [ 2.00000237  0.99999997]\n         [ 3.00000236  0.99999997]]\n        >>> xy = w.all_world2pix(radec, 1, maxiter=3,\n        ...                      tolerance=1.0e-10, quiet=False)\n        Traceback (most recent call last):\n        ...\n        NoConvergence: 'WCS.all_world2pix' failed to converge to the\n        requested accuracy. After 3 iterations, the solution is\n        diverging at least for one input point.\n\n        >>> # Now try to use some diverging data:\n        >>> divradec = w.all_pix2world([[1.0, 1.0],\n        ...                             [10000.0, 50000.0],\n        ...                             [3.0, 1.0]], 1)\n        >>> print(divradec)  # doctest: +FLOAT_CMP\n        [[  5.52645627 -72.05171757]\n         [  7.", "metadata": {"file_name": "astropy/wcs/wcs.py", "File Name": "astropy/wcs/wcs.py", "Classes": "NoConvergence, FITSFixedWarning, WCS, _WcsValidateWcsResult, _WcsValidateHduResult, _WcsValidateResults", "Functions": "_parse_keysel, __WCS_unpickle__, find_all_wcs, validate, write_d2i, write_dist, write_array, _return_list_of_arrays, _return_single_array"}}, {"code": "WCSBase = _wcs._Wcs\n    DistortionLookupTable = _wcs.DistortionLookupTable\n    Sip = _wcs.Sip\n    Wcsprm = _wcs.Wcsprm\n    Tabprm = _wcs.Tabprm\n    WcsError = _wcs.WcsError\n    SingularMatrixError = _wcs.SingularMatrixError\n    InconsistentAxisTypesError = _wcs.InconsistentAxisTypesError\n    InvalidTransformError = _wcs.InvalidTransformError\n    InvalidCoordinateError = _wcs.InvalidCoordinateError\n    NoSolutionError = _wcs.NoSolutionError\n    InvalidSubimageSpecificationError = _wcs.InvalidSubimageSpecificationError\n    NonseparableSubimageCoordinateSystemError = _wcs.NonseparableSubimageCoordinateSystemError\n    NoWcsKeywordsFoundError = _wcs.NoWcsKeywordsFoundError\n    InvalidTabularParametersError = _wcs.InvalidTabularParametersError\n\n    # Copy all the constants from the C extension into this module's namespace\n    for key, val in _wcs.__dict__.items():\n        if key.startswith(('WCSSUB', 'WCSHDR', 'WCSHDO')):\n            locals()[key] = val\n            __all__.append(key)\nelse:\n    WCSBase = object\n    Wcsprm = object\n    DistortionLookupTable = object\n    Sip = object\n    Tabprm = object\n    WcsError = None\n    SingularMatrixError = None\n    InconsistentAxisTypesError = None\n    InvalidTransformError = None\n    InvalidCoordinateError = None\n    NoSolutionError = None\n    InvalidSubimageSpecificationError = None\n    NonseparableSubimageCoordinateSystemError = None\n    NoWcsKeywordsFoundError = None\n    InvalidTabularParametersError = None", "metadata": {"file_name": "astropy/wcs/wcs.py", "File Name": "astropy/wcs/wcs.py", "Classes": "NoConvergence, FITSFixedWarning, WCS, _WcsValidateWcsResult, _WcsValidateHduResult, _WcsValidateResults", "Functions": "_parse_keysel, __WCS_unpickle__, find_all_wcs, validate, write_d2i, write_dist, write_array, _return_list_of_arrays, _return_single_array"}}, {"code": "- 'stokes': Stokes coordinate.\n\n              - 'celestial': Celestial coordinate (including ``CUBEFACE``).\n\n              - 'spectral': Spectral coordinate.\n\n            - 'scale':\n\n              - 'linear': Linear axis.\n\n              - 'quantized': Quantized axis (``STOKES``, ``CUBEFACE``).\n\n              - 'non-linear celestial': Non-linear celestial axis.\n\n              - 'non-linear spectral': Non-linear spectral axis.\n\n              - 'logarithmic': Logarithmic axis.\n\n              - 'tabular': Tabular axis.\n\n            - 'group'\n\n              - Group number, e.g. lookup table number\n\n            - 'number'\n\n              - For celestial axes:\n\n                - 0: Longitude coordinate.\n\n                - 1: Latitude coordinate.\n\n                - 2: ``CUBEFACE`` number.\n\n              - For lookup tables:\n\n                - the axis number in a multidimensional table.\n\n            ``CTYPEia`` in ``\"4-3\"`` form with unrecognized algorithm code will\n            generate an error.\n        \"\"\"\n        if self.wcs is None:\n            raise AttributeError(\n                \"This WCS object does not have a wcsprm object.\")", "metadata": {"file_name": "astropy/wcs/wcs.py", "File Name": "astropy/wcs/wcs.py", "Classes": "NoConvergence, FITSFixedWarning, WCS, _WcsValidateWcsResult, _WcsValidateHduResult, _WcsValidateResults", "Functions": "_parse_keysel, __WCS_unpickle__, find_all_wcs, validate, write_d2i, write_dist, write_array, _return_list_of_arrays, _return_single_array"}}, {"code": "\"\"\".format(__.TWO_OR_MORE_ARGS('naxis', 8),\n                   __.RA_DEC_ORDER(8),\n                   __.RETURNS('world coordinates, in degrees', 8))\n\n    def _all_world2pix(self, world, origin, tolerance, maxiter, adaptive,\n                       detect_divergence, quiet):\n        # ############################################################\n        # #          DESCRIPTION OF THE NUMERICAL METHOD            ##\n        # ############################################################\n        # In this section I will outline the method of solving\n        # the inverse problem of converting world coordinates to\n        # pixel coordinates (*inverse* of the direct transformation\n        # `all_pix2world`) and I will summarize some of the aspects\n        # of the method proposed here and some of the issues of the\n        # original `all_world2pix` (in relation to this method)\n        # discussed in https://github.com/astropy/astropy/issues/1977\n        # A more detailed discussion can be found here:\n        # https://github.com/astropy/astropy/pull/2373\n        #\n        #\n        #                  ### Background ###\n        #\n        #\n        # I will refer here to the [SIP Paper]\n        # (http://fits.gsfc.nasa.gov/registry/sip/SIP_distortion_v1_0.pdf).\n        # According to this paper, the effect of distortions as\n        # described in *their* equation (1) is:\n        #\n        # (1)   x = CD*(u+f(u)),\n        #\n        # where `x` is a *vector* of \"intermediate spherical\n        # coordinates\" (equivalent to (x,y) in the paper) and `u`\n        # is a *vector* of \"pixel coordinates\", and `f` is a vector\n        # function describing geometrical distortions\n        # (see equations 2 and 3 in SIP Paper.", "metadata": {"file_name": "astropy/wcs/wcs.py", "File Name": "astropy/wcs/wcs.py", "Classes": "NoConvergence, FITSFixedWarning, WCS, _WcsValidateWcsResult, _WcsValidateHduResult, _WcsValidateResults", "Functions": "_parse_keysel, __WCS_unpickle__, find_all_wcs, validate, write_d2i, write_dist, write_array, _return_list_of_arrays, _return_single_array"}}, {"code": "To use core WCS in conjunction with FITS WCS\ndistortion paper lookup tables or SIP distortion, you must select or\nreduce these to 2 dimensions using the naxis kwarg.\n\"\"\".format(wcsprm.naxis))\n\n            header_naxis = header.get('NAXIS', None)\n            if header_naxis is not None and header_naxis < wcsprm.naxis:\n                warnings.warn(\n                    \"The WCS transformation has more axes ({0:d}) than the \"\n                    \"image it is associated with ({1:d})\".format(\n                        wcsprm.naxis, header_naxis), FITSFixedWarning)\n\n        self._get_naxis(header)\n        WCSBase.__init__(self, sip, cpdis, wcsprm, det2im)\n\n        if fix:\n            self.fix(translate_units=translate_units)\n\n        if _do_set:\n            self.wcs.set()\n\n        for fd in close_fds:\n            fd.close()\n\n    def __copy__(self):\n        new_copy = self.__class__()\n        WCSBase.__init__(new_copy, self.sip,\n                         (self.cpdis1, self.cpdis2),\n                         self.wcs,\n                         (self.det2im1, self.det2im2))\n        new_copy.__dict__.update(self.__dict__)\n        return new_copy\n\n    def __deepcopy__(self, memo):\n        from copy import deepcopy\n\n        new_copy = self.__class__()\n        new_copy.naxis = deepcopy(self.naxis, memo)\n        WCSBase.__init__(new_copy, deepcopy(self.sip, memo),\n                         (deepcopy(self.cpdis1, memo),\n                          deepcopy(self.cpdis2, memo)),\n                         deepcopy(self.wcs, memo),\n                         (deepcopy(self.det2im1, memo),\n                          deepcopy(self.det2im2, memo)))\n        for key, val in self.__dict__.items():\n            new_copy.__dict__[key] = deepcopy(val, memo)\n        return new_copy\n\n    def copy(self):\n        \"\"\"\n        Return a shallow copy of the object.", "metadata": {"file_name": "astropy/wcs/wcs.py", "File Name": "astropy/wcs/wcs.py", "Classes": "NoConvergence, FITSFixedWarning, WCS, _WcsValidateWcsResult, _WcsValidateHduResult, _WcsValidateResults", "Functions": "_parse_keysel, __WCS_unpickle__, find_all_wcs, validate, write_d2i, write_dist, write_array, _return_list_of_arrays, _return_single_array"}}, {"code": "NoConvergence: 'WCS.all_world2pix' failed to converge to the\n        requested accuracy.  After 5 iterations, the solution is\n        diverging at least for one input point.\n\n        >>> # This time turn detect_divergence off:\n        >>> try:  # doctest: +FLOAT_CMP\n        ...   xy = w.all_world2pix(divradec, 1, maxiter=20,\n        ...                        tolerance=1.0e-4, adaptive=False,\n        ...                        detect_divergence=False,\n        ...                        quiet=False)\n        ... except wcs.wcs.NoConvergence as e:\n        ...   print(\"Indices of diverging points: {{0}}\"\n        ...         .format(e.divergent))\n        ...   print(\"Indices of poorly converging points: {{0}}\"\n        ...         .format(e.slow_conv))\n        ...   print(\"Best solution:\\\\n{{0}}\".format(e.best_solution))\n        ...   print(\"Achieved accuracy:\\\\n{{0}}\".format(e.accuracy))\n        Indices of diverging points: [1]\n        Indices of poorly converging points: None\n        Best solution:\n        [[ 1.00000009  1.        ]\n         [        nan         nan]\n         [ 3.00000009  1.        ]]\n        Achieved accuracy:\n        [[  2.29417358e-06   3.21222995e-08]\n         [             nan              nan]\n         [  2.27407877e-06   3.13005639e-08]]\n        >>> raise e\n        Traceback (most recent call last):\n        ...\n        NoConvergence: 'WCS.all_world2pix' failed to converge to the\n        requested accuracy.  After 6 iterations, the solution is\n        diverging at least for one input point.", "metadata": {"file_name": "astropy/wcs/wcs.py", "File Name": "astropy/wcs/wcs.py", "Classes": "NoConvergence, FITSFixedWarning, WCS, _WcsValidateWcsResult, _WcsValidateHduResult, _WcsValidateResults", "Functions": "_parse_keysel, __WCS_unpickle__, find_all_wcs, validate, write_d2i, write_dist, write_array, _return_list_of_arrays, _return_single_array"}}, {"code": "For celestial axes,\n         ``imgcrd[][self.lng]`` and ``imgcrd[][self.lat]`` are the\n         projected *x*-, and *y*-coordinates, in pseudo \\\"degrees\\\".\n         For quadcube projections with a ``CUBEFACE`` axis, the face\n         number is also returned in ``imgcrd[][self.cubeface]``.  For\n         spectral axes, ``imgcrd[][self.spec]`` is the intermediate\n         spectral coordinate, in SI units.\n\n    - *pixcrd*: double array[ncoord][nelem]\n\n        - Array of pixel coordinates.  Pixel coordinates are\n          zero-based.\n\n    - *stat*: int array[ncoord]\n\n        - Status return value for each coordinate. ``0`` for success,\n          ``1+`` for invalid pixel coordinate.\n\nRaises\n------\nMemoryError\n    Memory allocation failed.\n\nSingularMatrixError\n    Linear transformation matrix is singular.\n\nInconsistentAxisTypesError\n    Inconsistent or unrecognized coordinate axis types.\n\nValueError\n    Invalid parameter value.\n\nInvalidTransformError\n   Invalid coordinate transformation parameters.\n\nInvalidTransformError\n    Ill-conditioned coordinate transformation parameters.\n\nSee also\n--------\nastropy.wcs.Wcsprm.lat, astropy.wcs.Wcsprm.lng\n    Definition of the latitude and longitude axes\n\"\"\".format(__.ORIGIN())\n\nsense = \"\"\"\n``int array[M]`` +1 if monotonically increasing, -1 if decreasing.\n\nA vector of length `~astropy.wcs.Tabprm.M` whose elements\nindicate whether the corresponding indexing vector is monotonically\nincreasing (+1), or decreasing (-1).\n\"\"\"\n\nset = \"\"\"\nset()\n\nSets up a WCS object for use according to information supplied within\nit.\n\nNote that this routine need not be called directly; it will be invoked\nby `~astropy.wcs.Wcsprm.p2s` and `~astropy.wcs.Wcsprm.s2p` if\nnecessary.\n\nSome attributes that are based on other attributes (such as\n`~astropy.wcs.Wcsprm.lattyp` on `~astropy.wcs.Wcsprm.ctype`) may not\nbe correct until after `~astropy.wcs.Wcsprm.set` is called.\n\n`~astropy.wcs.Wcsprm.set` strips off trailing blanks in all string\nmembers.", "metadata": {"file_name": "astropy/wcs/docstrings.py", "File Name": "astropy/wcs/docstrings.py"}}, {"code": "coordinate_type_map = {\n            0: None,\n            1: 'stokes',\n            2: 'celestial',\n            3: 'spectral'}\n\n        scale_map = {\n            0: 'linear',\n            1: 'quantized',\n            2: 'non-linear celestial',\n            3: 'non-linear spectral',\n            4: 'logarithmic',\n            5: 'tabular'}\n\n        result = []\n        for axis_type in self.wcs.axis_types:\n            subresult = {}\n\n            coordinate_type = (axis_type // 1000) % 10\n            subresult['coordinate_type'] = coordinate_type_map[coordinate_type]\n\n            scale = (axis_type // 100) % 10\n            subresult['scale'] = scale_map[scale]\n\n            group = (axis_type // 10) % 10\n            subresult['group'] = group\n\n            number = axis_type % 10\n            subresult['number'] = number\n\n            result.append(subresult)\n\n        return result\n\n    def __reduce__(self):\n        \"\"\"\n        Support pickling of WCS objects.  This is done by serializing\n        to an in-memory FITS file and dumping that as a string.\n        \"\"\"\n\n        hdulist = self.to_fits(relax=True)\n\n        buffer = io.BytesIO()\n        hdulist.writeto(buffer)\n\n        return (__WCS_unpickle__,\n                (self.__class__, self.__dict__, buffer.getvalue(),))\n\n    def dropaxis(self, dropax):\n        \"\"\"\n        Remove an axis from the WCS.", "metadata": {"file_name": "astropy/wcs/wcs.py", "File Name": "astropy/wcs/wcs.py", "Classes": "NoConvergence, FITSFixedWarning, WCS, _WcsValidateWcsResult, _WcsValidateHduResult, _WcsValidateResults", "Functions": "_parse_keysel, __WCS_unpickle__, find_all_wcs, validate, write_d2i, write_dist, write_array, _return_list_of_arrays, _return_single_array"}}, {"code": "# Licensed under a 3-clause BSD style license - see LICENSE.rst\n\n# It gets to be really tedious to type long docstrings in ANSI C\n# syntax (since multi-line string literals are not valid).\n# Therefore, the docstrings are written here in doc/docstrings.py,\n# which are then converted by setup.py into docstrings.h, which is\n# included by pywcs.c\n\nfrom . import _docutil as __\n\na = \"\"\"\n``double array[a_order+1][a_order+1]`` Focal plane transformation\nmatrix.\n\nThe `SIP`_ ``A_i_j`` matrix used for pixel to focal plane\ntransformation.\n\nIts values may be changed in place, but it may not be resized, without\ncreating a new `~astropy.wcs.Sip` object.\n\"\"\"\n\na_order = \"\"\"\n``int`` (read-only) Order of the polynomial (``A_ORDER``).\n\"\"\"\n\nall_pix2world = \"\"\"\nall_pix2world(pixcrd, origin) -> ``double array[ncoord][nelem]``\n\nTransforms pixel coordinates to world coordinates.\n\nDoes the following:\n\n    - Detector to image plane correction (if present)\n\n    - SIP distortion correction (if present)\n\n    - FITS WCS distortion correction (if present)\n\n    - wcslib \"core\" WCS transformation\n\nThe first three (the distortion corrections) are done in parallel.\n\nParameters\n----------\npixcrd : double array[ncoord][nelem]\n    Array of pixel coordinates.\n\n{0}\n\nReturns\n-------\nworld : double array[ncoord][nelem]\n    Returns an array of world coordinates.\n\nRaises\n------\nMemoryError\n    Memory allocation failed.\n\nSingularMatrixError\n    Linear transformation matrix is singular.\n\nInconsistentAxisTypesError\n    Inconsistent or unrecognized coordinate axis types.\n\nValueError\n    Invalid parameter value.\n\nValueError\n    Invalid coordinate transformation parameters.\n\nValueError\n    x- and y-coordinate arrays are not the same size.\n\nInvalidTransformError\n    Invalid coordinate transformation.\n\nInvalidTransformError\n    Ill-conditioned coordinate transformation parameters.\n\"\"\".format(__.ORIGIN())\n\nalt = \"\"\"\n``str`` Character code for alternate coordinate descriptions.\n\nFor example, the ``\"a\"`` in keyword names such as ``CTYPEia``.  This\nis a space character for the primary coordinate description, or one of\nthe 26 upper-case letters, A-Z.\n\"\"\"", "metadata": {"file_name": "astropy/wcs/docstrings.py", "File Name": "astropy/wcs/docstrings.py"}}, {"code": "csyer = \"\"\"\n``double array[naxis]`` The systematic error in the coordinate value\naxes, ``CSYERia``.\n\nAn undefined value is represented by NaN.\n\"\"\"\n\nctype = \"\"\"\n``list of strings[naxis]`` List of ``CTYPEia`` keyvalues.\n\nThe `~astropy.wcs.Wcsprm.ctype` keyword values must be in upper case\nand there must be zero or one pair of matched celestial axis types,\nand zero or one spectral axis.\n\"\"\"\n\ncubeface = \"\"\"\n``int`` Index into the ``pixcrd`` (pixel coordinate) array for the\n``CUBEFACE`` axis.\n\nThis is used for quadcube projections where the cube faces are stored\non a separate axis.\n\nThe quadcube projections (``TSC``, ``CSC``, ``QSC``) may be\nrepresented in FITS in either of two ways:\n\n    - The six faces may be laid out in one plane and numbered as\n      follows::\n\n\n                                       0\n\n                              4  3  2  1  4  3  2\n\n                                       5\n\n      Faces 2, 3 and 4 may appear on one side or the other (or both).\n      The world-to-pixel routines map faces 2, 3 and 4 to the left but\n      the pixel-to-world routines accept them on either side.\n\n    - The ``COBE`` convention in which the six faces are stored in a\n      three-dimensional structure using a ``CUBEFACE`` axis indexed\n      from 0 to 5 as above.\n\nThese routines support both methods; `~astropy.wcs.Wcsprm.set`\ndetermines which is being used by the presence or absence of a\n``CUBEFACE`` axis in `~astropy.wcs.Wcsprm.ctype`.\n`~astropy.wcs.Wcsprm.p2s` and `~astropy.wcs.Wcsprm.s2p` translate the\n``CUBEFACE`` axis representation to the single plane representation\nunderstood by the lower-level projection routines.\n\"\"\"\n\ncunit = \"\"\"\n``list of astropy.UnitBase[naxis]`` List of ``CUNITia`` keyvalues as\n`astropy.units.UnitBase` instances.\n\nThese define the units of measurement of the ``CRVALia``, ``CDELTia``\nand ``CDi_ja`` keywords.", "metadata": {"file_name": "astropy/wcs/docstrings.py", "File Name": "astropy/wcs/docstrings.py"}}, {"code": "# Still better than nothing...\n        inddiv, = np.where(((dn >= tol2) & (dn >= dnprev)) | invalid)\n        if inddiv.shape[0] == 0:\n            inddiv = None\n\n        # Identify points that did not converge within 'maxiter'\n        # iterations:\n        if k >= maxiter:\n            ind, = np.where((dn >= tol2) & (dn < dnprev) & (~invalid))\n            if ind.shape[0] == 0:\n                ind = None\n        else:\n            ind = None\n\n        # Restore previous numpy error settings:\n        np.seterr(invalid=old_invalid, over=old_over)\n\n        # ############################################################\n        # #  RAISE EXCEPTION IF DIVERGING OR TOO SLOWLY CONVERGING  ##\n        # #  DATA POINTS HAVE BEEN DETECTED:                        ##\n        # ############################################################\n        if (ind is not None or inddiv is not None) and not quiet:\n            if inddiv is None:\n                raise NoConvergence(\n                    \"'WCS.all_world2pix' failed to \"\n                    \"converge to the requested accuracy after {:d} \"\n                    \"iterations.\".format(k), best_solution=pix,\n                    accuracy=np.abs(dpix), niter=k,\n                    slow_conv=ind, divergent=None)\n            else:\n                raise NoConvergence(\n                    \"'WCS.all_world2pix' failed to \"\n                    \"converge to the requested accuracy.\\n\"\n                    \"After {0:d} iterations, the solution is diverging \"\n                    \"at least for one input point.\"", "metadata": {"file_name": "astropy/wcs/wcs.py", "File Name": "astropy/wcs/wcs.py", "Classes": "NoConvergence, FITSFixedWarning, WCS, _WcsValidateWcsResult, _WcsValidateHduResult, _WcsValidateResults", "Functions": "_parse_keysel, __WCS_unpickle__, find_all_wcs, validate, write_d2i, write_dist, write_array, _return_list_of_arrays, _return_single_array"}}, {"code": "sip.bp)\n\n        return keywords\n\n    def _denormalize_sky(self, sky):\n        if self.wcs.lngtyp != 'RA':\n            raise ValueError(\n                \"WCS does not have longitude type of 'RA', therefore \" +\n                \"(ra, dec) data can not be used as input\")\n        if self.wcs.lattyp != 'DEC':\n            raise ValueError(\n                \"WCS does not have longitude type of 'DEC', therefore \" +\n                \"(ra, dec) data can not be used as input\")\n        if self.wcs.naxis == 2:\n            if self.wcs.lng == 0 and self.wcs.lat == 1:\n                return sky\n            elif self.wcs.lng == 1 and self.wcs.lat == 0:\n                # Reverse the order of the columns\n                return sky[:, ::-1]\n            else:\n                raise ValueError(\n                    \"WCS does not have longitude and latitude celestial \" +\n                    \"axes, therefore (ra, dec) data can not be used as input\")\n        else:\n            if self.wcs.lng < 0 or self.wcs.lat < 0:\n                raise ValueError(\n                    \"WCS does not have both longitude and latitude \"\n                    \"celestial axes, therefore (ra, dec) data can not be \" +\n                    \"used as input\")\n            out = np.zeros((sky.shape[0], self.wcs.naxis))\n            out[:, self.wcs.lng] = sky[:, 0]\n            out[:, self.wcs.lat] = sky[:, 1]\n            return out\n\n    def _normalize_sky(self, sky):\n        if self.wcs.lngtyp != 'RA':\n            raise ValueError(\n                \"WCS does not have longitude type of 'RA', therefore \" +\n                \"(ra, dec) data can not be returned\")\n        if self.wcs.", "metadata": {"file_name": "astropy/wcs/wcs.py", "File Name": "astropy/wcs/wcs.py", "Classes": "NoConvergence, FITSFixedWarning, WCS, _WcsValidateWcsResult, _WcsValidateHduResult, _WcsValidateResults", "Functions": "_parse_keysel, __WCS_unpickle__, find_all_wcs, validate, write_d2i, write_dist, write_array, _return_list_of_arrays, _return_single_array"}}, {"code": "if isinstance(header, (str, bytes)):\n            # TODO: Parse SIP from a string without pyfits around\n            return None\n\n        if str(\"A_ORDER\") in header and header[str('A_ORDER')] > 1:\n            if str(\"B_ORDER\") not in header:\n                raise ValueError(\n                    \"A_ORDER provided without corresponding B_ORDER \"\n                    \"keyword for SIP distortion\")\n\n            m = int(header[str(\"A_ORDER\")])\n            a = np.zeros((m + 1, m + 1), np.double)\n            for i in range(m + 1):\n                for j in range(m - i + 1):\n                    key = str(\"A_{0}_{1}\").format(i, j)\n                    if key in header:\n                        a[i, j] = header[key]\n                        del header[key]\n\n            m = int(header[str(\"B_ORDER\")])\n            if m > 1:\n                b = np.zeros((m + 1, m + 1), np.double)\n                for i in range(m + 1):\n                    for j in range(m - i + 1):\n                        key = str(\"B_{0}_{1}\").format(i, j)\n                        if key in header:\n                            b[i, j] = header[key]\n                            del header[key]\n            else:\n                a = None\n                b = None\n\n            del header[str('A_ORDER')]\n            del header[str('B_ORDER')]\n\n            ctype = [header['CTYPE{0}{1}'.format(nax, wcskey)] for nax in range(1, self.naxis + 1)]\n            if any(not ctyp.endswith('-SIP') for ctyp in ctype):\n                message = \"\"\"\n                Inconsistent SIP distortion information is present in the FITS header and the WCS object:\n                SIP coefficients were detected, but CTYPE is missing a \"-SIP\" suffix.\n                astropy.wcs is using the SIP distortion coefficients,\n                therefore the coordinates calculated here might be incorrect.", "metadata": {"file_name": "astropy/wcs/wcs.py", "File Name": "astropy/wcs/wcs.py", "Classes": "NoConvergence, FITSFixedWarning, WCS, _WcsValidateWcsResult, _WcsValidateHduResult, _WcsValidateResults", "Functions": "_parse_keysel, __WCS_unpickle__, find_all_wcs, validate, write_d2i, write_dist, write_array, _return_list_of_arrays, _return_single_array"}}, {"code": "- `int`: a bit field selecting specific extensions to\n              write.  See :ref:`relaxwrite` for details.\n\n            If the ``relax`` keyword argument is not given and any\n            keywords were omitted from the output, an\n            `~astropy.utils.exceptions.AstropyWarning` is displayed.\n            To override this, explicitly pass a value to ``relax``.\n\n        key : str\n            The name of a particular WCS transform to use.  This may be\n            either ``' '`` or ``'A'``-``'Z'`` and corresponds to the ``\"a\"``\n            part of the ``CTYPEia`` cards.\n\n        Returns\n        -------\n        header : `astropy.io.fits.Header`\n\n        Notes\n        -----\n        The output header will almost certainly differ from the input in a\n        number of respects:\n\n          1. The output header only contains WCS-related keywords.  In\n             particular, it does not contain syntactically-required\n             keywords such as ``SIMPLE``, ``NAXIS``, ``BITPIX``, or\n             ``END``.\n\n          2. Deprecated (e.g. ``CROTAn``) or non-standard usage will\n             be translated to standard (this is partially dependent on\n             whether ``fix`` was applied).\n\n          3. Quantities will be converted to the units used internally,\n             basically SI with the addition of degrees.\n\n          4. Floating-point quantities may be given to a different decimal\n             precision.\n\n          5. Elements of the ``PCi_j`` matrix will be written if and\n             only if they differ from the unit matrix.  Thus, if the\n             matrix is unity then no elements will be written.\n\n          6. Additional keywords such as ``WCSAXES``, ``CUNITia``,\n             ``LONPOLEa`` and ``LATPOLEa`` may appear.\n\n          7.", "metadata": {"file_name": "astropy/wcs/wcs.py", "File Name": "astropy/wcs/wcs.py", "Classes": "NoConvergence, FITSFixedWarning, WCS, _WcsValidateWcsResult, _WcsValidateHduResult, _WcsValidateResults", "Functions": "_parse_keysel, __WCS_unpickle__, find_all_wcs, validate, write_d2i, write_dist, write_array, _return_list_of_arrays, _return_single_array"}}, {"code": "Parameters\n        ----------\n        wcs : `~astropy.wcs.WCS`\n            The WCS with naxis to be chopped to naxis-1\n        dropax : int\n            The index of the WCS to drop, counting from 0 (i.e., python convention,\n            not FITS convention)\n\n        Returns\n        -------\n        A new `~astropy.wcs.WCS` instance with one axis fewer\n        \"\"\"\n        inds = list(range(self.wcs.naxis))\n        inds.pop(dropax)\n\n        # axis 0 has special meaning to sub\n        # if wcs.wcs.ctype == ['RA','DEC','VLSR'], you want\n        # wcs.sub([1,2]) to get 'RA','DEC' back\n        return self.sub([i+1 for i in inds])\n\n    def swapaxes(self, ax0, ax1):\n        \"\"\"\n        Swap axes in a WCS.\n\n        Parameters\n        ----------\n        wcs : `~astropy.wcs.WCS`\n            The WCS to have its axes swapped\n        ax0 : int\n        ax1 : int\n            The indices of the WCS to be swapped, counting from 0 (i.e., python\n            convention, not FITS convention)\n\n        Returns\n        -------\n        A new `~astropy.wcs.WCS` instance with the same number of axes, but two\n        swapped\n        \"\"\"\n        inds = list(range(self.wcs.naxis))\n        inds[ax0], inds[ax1] = inds[ax1], inds[ax0]\n\n        return self.sub([i+1 for i in inds])\n\n    def reorient_celestial_first(self):\n        \"\"\"\n        Reorient the WCS such that the celestial axes are first, followed by\n        the spectral axis, followed by any others.\n        Assumes at least celestial axes are present.\n        \"\"\"", "metadata": {"file_name": "astropy/wcs/wcs.py", "File Name": "astropy/wcs/wcs.py", "Classes": "NoConvergence, FITSFixedWarning, WCS, _WcsValidateWcsResult, _WcsValidateHduResult, _WcsValidateResults", "Functions": "_parse_keysel, __WCS_unpickle__, find_all_wcs, validate, write_d2i, write_dist, write_array, _return_list_of_arrays, _return_single_array"}}, {"code": "InvalidTransformError\n            Ill-conditioned coordinate transformation parameters.\n        \"\"\".format(__.TWO_OR_MORE_ARGS('naxis', 8),\n                   __.RA_DEC_ORDER(8),\n                   __.RETURNS('pixel coordinates', 8))\n\n    def pix2foc(self, *args):\n        return self._array_converter(self._pix2foc, None, *args)\n    pix2foc.__doc__ = \"\"\"\n        Convert pixel coordinates to focal plane coordinates using the\n        `SIP`_ polynomial distortion convention and `distortion\n        paper`_ table-lookup correction.\n\n        The output is in absolute pixel coordinates, not relative to\n        ``CRPIX``.\n\n        Parameters\n        ----------\n\n        {0}\n\n        Returns\n        -------\n\n        {1}\n\n        Raises\n        ------\n        MemoryError\n            Memory allocation failed.\n\n        ValueError\n            Invalid coordinate transformation parameters.\n        \"\"\".format(__.TWO_OR_MORE_ARGS('2', 8),\n                   __.RETURNS('focal coordinates', 8))\n\n    def p4_pix2foc(self, *args):\n        return self._array_converter(self._p4_pix2foc, None, *args)\n    p4_pix2foc.__doc__ = \"\"\"\n        Convert pixel coordinates to focal plane coordinates using\n        `distortion paper`_ table-lookup correction.\n\n        The output is in absolute pixel coordinates, not relative to\n        ``CRPIX``.\n\n        Parameters\n        ----------\n\n        {0}\n\n        Returns\n        -------\n\n        {1}\n\n        Raises\n        ------\n        MemoryError\n            Memory allocation failed.\n\n        ValueError\n            Invalid coordinate transformation parameters.", "metadata": {"file_name": "astropy/wcs/wcs.py", "File Name": "astropy/wcs/wcs.py", "Classes": "NoConvergence, FITSFixedWarning, WCS, _WcsValidateWcsResult, _WcsValidateHduResult, _WcsValidateResults", "Functions": "_parse_keysel, __WCS_unpickle__, find_all_wcs, validate, write_d2i, write_dist, write_array, _return_list_of_arrays, _return_single_array"}}, {"code": "coordinates here to avoid circular imports\n    from ..coordinates import FK4, FK4NoETerms, FK5, ICRS, ITRS, Galactic\n\n    # Import astropy.time here otherwise setup.py fails before extensions are compiled\n    from ..time import Time\n\n    # Keep only the celestial part of the axes\n    wcs = wcs.sub([WCSSUB_LONGITUDE, WCSSUB_LATITUDE])\n\n    if wcs.wcs.lng == -1 or wcs.wcs.lat == -1:\n        return None\n\n    radesys = wcs.wcs.radesys\n\n    if np.isnan(wcs.wcs.equinox):\n        equinox = None\n    else:\n        equinox = wcs.wcs.equinox\n\n    xcoord = wcs.wcs.ctype[0][:4]\n    ycoord = wcs.wcs.ctype[1][:4]\n\n    # Apply logic from FITS standard to determine the default radesys\n    if radesys == '' and xcoord == 'RA--' and ycoord == 'DEC-':\n        if equinox is None:\n            radesys = \"ICRS\"\n        elif equinox < 1984.:\n            radesys = \"FK4\"\n        else:\n            radesys = \"FK5\"\n\n    if radesys == 'FK4':\n        if equinox is not None:\n            equinox = Time(equinox, format='byear')\n        frame = FK4(equinox=equinox)\n    elif radesys == 'FK4-NO-E':\n        if equinox is not None:\n            equinox = Time(equinox,", "metadata": {"file_name": "astropy/wcs/utils.py", "File Name": "astropy/wcs/utils.py", "Classes": "custom_wcs_to_frame_mappings, custom_frame_to_wcs_mappings", "Functions": "add_stokes_axis_to_wcs, _wcs_to_celestial_frame_builtin, _celestial_frame_to_wcs_builtin, wcs_to_celestial_frame, celestial_frame_to_wcs, proj_plane_pixel_scales, proj_plane_pixel_area, is_proj_plane_distorted, _is_cd_orthogonal, non_celestial_pixel_scales, _has_distortion, skycoord_to_pixel, pixel_to_skycoord"}}, {"code": "if isinstance(header, (str, bytes)):\n            return (None, None)\n\n        if dist == 'CPDIS':\n            d_kw = str('DP')\n            err_kw = str('CPERR')\n        else:\n            d_kw = str('DQ')\n            err_kw = str('CQERR')\n\n        tables = {}\n        for i in range(1, self.naxis + 1):\n            d_error_key = err_kw + str(i)\n            if d_error_key in header:\n                d_error = header[d_error_key]\n                del header[d_error_key]\n            else:\n                d_error = 0.0\n            if d_error < err:\n                tables[i] = None\n                continue\n            distortion = dist + str(i)\n            if distortion in header:\n                dis = header[distortion].lower()\n                del header[distortion]\n                if dis == 'lookup':\n                    if not isinstance(fobj, fits.HDUList):\n                        raise ValueError('an astropy.io.fits.HDUList is '\n                                'required for Lookup table distortion.')\n                    dp = (d_kw + str(i)).strip()\n                    dp_extver_key = dp + str('.EXTVER')\n                    if dp_extver_key in header:\n                        d_extver = header[dp_extver_key]\n                        del header[dp_extver_key]\n                    else:\n                        d_extver = 1\n                    dp_axis_key = dp + str('.AXIS.", "metadata": {"file_name": "astropy/wcs/wcs.py", "File Name": "astropy/wcs/wcs.py", "Classes": "NoConvergence, FITSFixedWarning, WCS, _WcsValidateWcsResult, _WcsValidateHduResult, _WcsValidateResults", "Functions": "_parse_keysel, __WCS_unpickle__, find_all_wcs, validate, write_d2i, write_dist, write_array, _return_list_of_arrays, _return_single_array"}}, {"code": "#\n        #\n        #         ### Outline of the Algorithm ###\n        #\n        #\n        # While the proposed code is relatively long (considering\n        # the simplicity of the algorithm), this is due to: 1)\n        # checking if iterative solution is necessary at all; 2)\n        # checking for divergence; 3) re-implementation of the\n        # completely vectorized algorithm as an \"adaptive\" vectorized\n        # algorithm (for cases when some points diverge for which we\n        # want to stop iterations). In my tests, the adaptive version\n        # of the algorithm is about 50% slower than non-adaptive\n        # version for all HST images.\n        #\n        # The essential part of the vectorized non-adaptive algorithm\n        # (without divergence and other checks) can be described\n        # as follows:\n        #\n        #     pix0 = self.wcs_world2pix(world, origin)\n        #     pix  = pix0.copy() # 0-order solution\n        #\n        #     for k in range(maxiter):\n        #         # find correction to the previous solution:\n        #         dpix = self.pix2foc(pix, origin) - pix0\n        #\n        #         # compute norm (L2) of the correction:\n        #         dn = np.linalg.norm(dpix, axis=1)\n        #\n        #         # apply correction:\n        #         pix -= dpix\n        #\n        #         # check convergence:\n        #         if np.max(dn) < tolerance:\n        #             break\n        #\n        #    return pix\n        #\n        # Here, the input parameter `world` can be a `MxN` array\n        # where `M` is the number of coordinate axes in WCS and `N`\n        # is the number of points to be converted simultaneously to\n        # image coordinates.", "metadata": {"file_name": "astropy/wcs/wcs.py", "File Name": "astropy/wcs/wcs.py", "Classes": "NoConvergence, FITSFixedWarning, WCS, _WcsValidateWcsResult, _WcsValidateHduResult, _WcsValidateResults", "Functions": "_parse_keysel, __WCS_unpickle__, find_all_wcs, validate, write_d2i, write_dist, write_array, _return_list_of_arrays, _return_single_array"}}, {"code": "nitems = len(builtins.range(self._naxis[wcs_index])[iview])\n            except TypeError as exc:\n                if 'indices must be integers' not in str(exc):\n                    raise\n                warnings.warn(\"NAXIS{0} attribute is not updated because at \"\n                              \"least one indix ('{1}') is no integer.\"\n                              \"\".format(wcs_index, iview), AstropyUserWarning)\n            else:\n                wcs_new._naxis[wcs_index] = nitems\n\n        if wcs_new.sip is not None:\n            wcs_new.sip = Sip(self.sip.a, self.sip.b, self.sip.ap, self.sip.bp,\n                              sip_crpix)\n\n        return wcs_new\n\n    def __getitem__(self, item):\n        # \"getitem\" is a shortcut for self.slice; it is very limited\n        # there is no obvious and unambiguous interpretation of wcs[1,2,3]\n        # We COULD allow wcs[1] to link to wcs.sub([2])\n        # (wcs[i] -> wcs.sub([i+1])\n        return self.slice(item)\n\n    def __iter__(self):\n        # Having __getitem__ makes Python think WCS is iterable. However,\n        # Python first checks whether __iter__ is present, so we can raise an\n        # exception here.", "metadata": {"file_name": "astropy/wcs/wcs.py", "File Name": "astropy/wcs/wcs.py", "Classes": "NoConvergence, FITSFixedWarning, WCS, _WcsValidateWcsResult, _WcsValidateHduResult, _WcsValidateResults", "Functions": "_parse_keysel, __WCS_unpickle__, find_all_wcs, validate, write_d2i, write_dist, write_array, _return_list_of_arrays, _return_single_array"}}, {"code": "description = [\"WCS Keywords\\n\",\n                       \"Number of WCS axes: {0!r}\".format(self.naxis)]\n        sfmt = ' : ' + \"\".join([\"{\"+\"{0}\".format(i)+\"!r}  \" for i in range(self.naxis)])\n\n        keywords = ['CTYPE', 'CRVAL', 'CRPIX']\n        values = [self.wcs.ctype, self.wcs.crval, self.wcs.crpix]\n        for keyword, value in zip(keywords, values):\n            description.append(keyword+sfmt.format(*value))\n\n        if hasattr(self.wcs, 'pc'):\n            for i in range(self.naxis):\n                s = ''\n                for j in range(self.naxis):\n                    s += ''.join(['PC', str(i+1), '_', str(j+1), ' '])\n                s += sfmt\n                description.append(s.format(*self.wcs.pc[i]))\n            s = 'CDELT' + sfmt\n            description.append(s.format(*self.wcs.cdelt))\n        elif hasattr(self.wcs, 'cd'):\n            for i in range(self.naxis):\n                s = ''\n                for j in range(self.naxis):\n                    s += \"\".join(['CD', str(i+1), '_', str(j+1), ' '])\n                s += sfmt\n                description.append(s.format(*self.wcs.cd[i]))\n\n        description.append('NAXIS : {}'.format('  '.join(map(str, self._naxis))))\n        return '\\n'.join(description)\n\n    def get_axis_types(self):\n        \"\"\"\n        Similar to `self.wcsprm.axis_types <astropy.wcs.Wcsprm.axis_types>`\n        but provides the information in a more Python-friendly format.\n\n        Returns\n        -------\n        result : list of dicts\n\n            Returns a list of dictionaries, one for each axis, each\n            containing attributes about the type of that axis.\n\n            Each dictionary has the following keys:\n\n            - 'coordinate_type':\n\n              - None: Non-specific coordinate type.", "metadata": {"file_name": "astropy/wcs/wcs.py", "File Name": "astropy/wcs/wcs.py", "Classes": "NoConvergence, FITSFixedWarning, WCS, _WcsValidateWcsResult, _WcsValidateHduResult, _WcsValidateResults", "Functions": "_parse_keysel, __WCS_unpickle__, find_all_wcs, validate, write_d2i, write_dist, write_array, _return_list_of_arrays, _return_single_array"}}, {"code": "0 / [deg] Native latitude of celestial pole\n        RADESYS = 'FK5'                / Equatorial coordinate system\n        EQUINOX =               2010.0 / [yr] Equinox of equatorial coordinates\n\n\n    Notes\n    -----\n\n    To extend this function to frames not defined in astropy.coordinates, you\n    can write your own function which should take a\n    :class:`~astropy.coordinates.baseframe.BaseCoordinateFrame` subclass\n    instance and a projection (given as a string) and should return either a WCS\n    instance, or `None` if the WCS could not be determined. You can register\n    this function temporarily with::\n\n        >>> from astropy.wcs.utils import celestial_frame_to_wcs, custom_frame_to_wcs_mappings\n        >>> with custom_frame_to_wcs_mappings(my_function):\n        ...     celestial_frame_to_wcs(...)\n\n    \"\"\"\n    for mapping_set in FRAME_WCS_MAPPINGS:\n        for func in mapping_set:\n            wcs = func(frame, projection=projection)\n            if wcs is not None:\n                return wcs\n    raise ValueError(\"Could not determine WCS corresponding to the specified \"\n                     \"coordinate frame.\")\n\n\ndef proj_plane_pixel_scales(wcs):\n    \"\"\"\n    For a WCS returns pixel scales along each axis of the image pixel at\n    the ``CRPIX`` location once it is projected onto the\n    \"plane of intermediate world coordinates\" as defined in\n    `Greisen & Calabretta 2002, A&A, 395, 1061 <http://adsabs.harvard.edu/abs/2002A%26A...395.1061G>`_.", "metadata": {"file_name": "astropy/wcs/utils.py", "File Name": "astropy/wcs/utils.py", "Classes": "custom_wcs_to_frame_mappings, custom_frame_to_wcs_mappings", "Functions": "add_stokes_axis_to_wcs, _wcs_to_celestial_frame_builtin, _celestial_frame_to_wcs_builtin, wcs_to_celestial_frame, celestial_frame_to_wcs, proj_plane_pixel_scales, proj_plane_pixel_area, is_proj_plane_distorted, _is_cd_orthogonal, non_celestial_pixel_scales, _has_distortion, skycoord_to_pixel, pixel_to_skycoord"}}, {"code": "if fobj is None:\n            return (None, None)\n\n        if not isinstance(fobj, fits.HDUList):\n            return (None, None)\n\n        try:\n            axiscorr = header[str('AXISCORR')]\n            d2imdis = self._read_d2im_old_format(header, fobj, axiscorr)\n            return d2imdis\n        except KeyError:\n            pass\n\n        dist = 'D2IMDIS'\n        d_kw = 'D2IM'\n        err_kw = 'D2IMERR'\n        tables = {}\n        for i in range(1, self.naxis + 1):\n            d_error = header.get(err_kw + str(i), 0.0)\n            if d_error < err:\n                tables[i] = None\n                continue\n            distortion = dist + str(i)\n            if distortion in header:\n                dis = header[distortion].lower()\n                if dis == 'lookup':\n                    del header[distortion]\n                    assert isinstance(fobj, fits.HDUList), ('An astropy.io.fits.HDUList'\n                                'is required for Lookup table distortion.')\n                    dp = (d_kw + str(i)).strip()\n                    dp_extver_key = dp + str('.EXTVER')\n                    if dp_extver_key in header:\n                        d_extver = header[dp_extver_key]\n                        del header[dp_extver_key]\n                    else:\n                        d_extver = 1\n                    dp_axis_key = dp + str('.AXIS.", "metadata": {"file_name": "astropy/wcs/wcs.py", "File Name": "astropy/wcs/wcs.py", "Classes": "NoConvergence, FITSFixedWarning, WCS, _WcsValidateWcsResult, _WcsValidateHduResult, _WcsValidateResults", "Functions": "_parse_keysel, __WCS_unpickle__, find_all_wcs, validate, write_d2i, write_dist, write_array, _return_list_of_arrays, _return_single_array"}}, {"code": "Order is\n      significant; ``axes[0]`` is the axis number of the input image\n      that corresponds to the first axis in the subimage, etc.  Use an\n      axis number of 0 to create a new axis using the defaults.\n\n    - If ``0``, ``[]`` or ``None``, do a deep copy.\n\n    Coordinate axes types may be specified using either strings or\n    special integer constants.  The available types are:\n\n    - ``'longitude'`` / ``WCSSUB_LONGITUDE``: Celestial longitude\n\n    - ``'latitude'`` / ``WCSSUB_LATITUDE``: Celestial latitude\n\n    - ``'cubeface'`` / ``WCSSUB_CUBEFACE``: Quadcube ``CUBEFACE`` axis\n\n    - ``'spectral'`` / ``WCSSUB_SPECTRAL``: Spectral axis\n\n    - ``'stokes'`` / ``WCSSUB_STOKES``: Stokes axis\n\n    - ``'celestial'`` / ``WCSSUB_CELESTIAL``: An alias for the\n      combination of ``'longitude'``, ``'latitude'`` and ``'cubeface'``.\n\nReturns\n-------\nnew_wcs : `~astropy.wcs.WCS` object\n\nRaises\n------\nMemoryError\n    Memory allocation failed.\n\nInvalidSubimageSpecificationError\n    Invalid subimage specification (no spectral axis).\n\nNonseparableSubimageCoordinateSystem\n    Non-separable subimage coordinate system.\n\nNotes\n-----\nCombinations of subimage axes of particular types may be extracted in\nthe same order as they occur in the input image by combining the\ninteger constants with the 'binary or' (``|``) operator.  For\nexample::\n\n    wcs.sub([WCSSUB_LONGITUDE | WCSSUB_LATITUDE | WCSSUB_SPECTRAL])\n\nwould extract the longitude, latitude, and spectral axes in the same\norder as the input image.  If one of each were present, the resulting\nobject would have three dimensions.\n\nFor convenience, ``WCSSUB_CELESTIAL`` is defined as the combination\n``WCSSUB_LONGITUDE | WCSSUB_LATITUDE | WCSSUB_CUBEFACE``.", "metadata": {"file_name": "astropy/wcs/docstrings.py", "File Name": "astropy/wcs/docstrings.py"}}, {"code": "If you do not want to apply the SIP distortion coefficients,\n                please remove the SIP coefficients from the FITS header or the\n                WCS object.  As an example, if the image is already distortion-corrected\n                (e.g., drizzled) then distortion components should not apply and the SIP\n                coefficients should be removed.\n\n                While the SIP distortion coefficients are being applied here, if that was indeed the intent,\n                for consistency please append \"-SIP\" to the CTYPE in the FITS header or the WCS object.\n\n                \"\"\"\n                log.info(message)\n        elif str(\"B_ORDER\") in header and header[str('B_ORDER')] > 1:\n            raise ValueError(\n                \"B_ORDER provided without corresponding A_ORDER \" +\n                \"keyword for SIP distortion\")\n        else:\n            a = None\n            b = None\n\n        if str(\"AP_ORDER\") in header and header[str('AP_ORDER')] > 1:\n            if str(\"BP_ORDER\") not in header:\n                raise ValueError(\n                    \"AP_ORDER provided without corresponding BP_ORDER \"\n                    \"keyword for SIP distortion\")\n\n            m = int(header[str(\"AP_ORDER\")])\n            ap = np.zeros((m + 1, m + 1), np.double)\n            for i in range(m + 1):\n                for j in range(m - i + 1):\n                    key = str(\"AP_{0}_{1}\").format(i, j)\n                    if key in header:\n                        ap[i, j] = header[key]\n                        del header[key]\n\n            m = int(header[str(\"BP_ORDER\")])\n            if m > 1:\n                bp = np.zeros((m + 1, m + 1), np.double)\n                for i in range(m + 1):\n                    for j in range(m - i + 1):\n                        key = str(\"BP_{0}_{1}\").format(i, j)\n                        if key in header:\n                            bp[i,", "metadata": {"file_name": "astropy/wcs/wcs.py", "File Name": "astropy/wcs/wcs.py", "Classes": "NoConvergence, FITSFixedWarning, WCS, _WcsValidateWcsResult, _WcsValidateHduResult, _WcsValidateResults", "Functions": "_parse_keysel, __WCS_unpickle__, find_all_wcs, validate, write_d2i, write_dist, write_array, _return_list_of_arrays, _return_single_array"}}, {"code": "Longitude ranges may be\n    specified with any convenient normalization, for example\n    ``(-120,+120)`` is the same as ``(240,480)``, except that the\n    solution will be returned with the same normalization, i.e. lie\n    within the interval specified.\n\nvstep : float\n    Step size for solution search, in degrees.  If ``0``, a sensible,\n    although perhaps non-optimal default will be used.\n\nviter : int\n    If a solution is not found then the step size will be halved and\n    the search recommenced.  *viter* controls how many times the step\n    size is halved.  The allowed range is 5 - 10.\n\nworld : double array[naxis]\n    World coordinate elements.  ``world[self.lng]`` and\n    ``world[self.lat]`` are the celestial longitude and latitude, in\n    degrees.  Which is given and which returned depends on the value\n    of *mixcel*.  All other elements are given.  The results will be\n    written to this array in-place.\n\npixcrd : double array[naxis].\n    Pixel coordinates.  The element indicated by *mixpix* is given and\n    the remaining elements will be written in-place.\n\n{0}\n\nReturns\n-------\nresult : dict\n\n    Returns a dictionary with the following keys:\n\n    - *phi* (double array[naxis])\n\n    - *theta* (double array[naxis])\n\n        - Longitude and latitude in the native coordinate system of\n          the projection, in degrees.\n\n    - *imgcrd* (double array[naxis])\n\n        - Image coordinate elements.  ``imgcrd[self.lng]`` and\n          ``imgcrd[self.lat]`` are the projected *x*- and\n          *y*-coordinates, in decimal degrees.\n\n    - *world* (double array[naxis])\n\n        - Another reference to the *world* argument passed in.\n\nRaises\n------\nMemoryError\n    Memory allocation failed.\n\nSingularMatrixError\n    Linear transformation matrix is singular.\n\nInconsistentAxisTypesError\n    Inconsistent or unrecognized coordinate axis types.\n\nValueError\n    Invalid parameter value.\n\nInvalidTransformError\n    Invalid coordinate transformation parameters.\n\nInvalidTransformError\n    Ill-conditioned coordinate transformation parameters.\n\nInvalidCoordinateError\n    Invalid world coordinate.\n\nNoSolutionError\n    No solution found in the specified interval.", "metadata": {"file_name": "astropy/wcs/docstrings.py", "File Name": "astropy/wcs/docstrings.py"}}, {"code": "The original keycomments will be lost, although\n             `to_header` tries hard to write meaningful comments.\n\n          8. Keyword order may be changed.\n\n        \"\"\"\n        # default precision for numerical WCS keywords\n        precision = WCSHDO_P14\n        display_warning = False\n        if relax is None:\n            display_warning = True\n            relax = False\n\n        if relax not in (True, False):\n            do_sip = relax & WCSHDO_SIP\n            relax &= ~WCSHDO_SIP\n        else:\n            do_sip = relax\n            relax = WCSHDO_all if relax is True else WCSHDO_safe\n\n        relax = precision | relax\n\n        if self.wcs is not None:\n            if key is not None:\n                orig_key = self.wcs.alt\n                self.wcs.alt = key\n            header_string = self.wcs.to_header(relax)\n            header = fits.Header.fromstring(header_string)\n            keys_to_remove = [\"\", \" \", \"COMMENT\"]\n            for kw in keys_to_remove:\n                if kw in header:\n                    del header[kw]\n        else:\n            header = fits.Header()\n\n        if do_sip and self.sip is not None:\n            if self.wcs is not None and any(not ctyp.endswith('-SIP') for ctyp in self.wcs.ctype):\n                self._fix_ctype(header, add_sip=True)\n\n            for kw, val in self._write_sip_kw().items():\n                header[kw] = val\n\n        if not do_sip and self.wcs is not None and any(self.wcs.ctype) and self.sip is not None:\n            # This is called when relax is not False or WCSHDO_SIP\n            # The default case of ``relax=None`` is handled further in the code.", "metadata": {"file_name": "astropy/wcs/wcs.py", "File Name": "astropy/wcs/wcs.py", "Classes": "NoConvergence, FITSFixedWarning, WCS, _WcsValidateWcsResult, _WcsValidateHduResult, _WcsValidateResults", "Functions": "_parse_keysel, __WCS_unpickle__, find_all_wcs, validate, write_d2i, write_dist, write_array, _return_list_of_arrays, _return_single_array"}}, {"code": "class _WcsValidateWcsResult(list):\n        def __init__(self, key):\n            self._key = key\n\n        def __repr__(self):\n            result = [\"  WCS key '{0}':\".format(self._key or ' ')]\n            if len(self):\n                for entry in self:\n                    for i, line in enumerate(entry.splitlines()):\n                        if i == 0:\n                            initial_indent = '    - '\n                        else:\n                            initial_indent = '      '\n                        result.extend(\n                            textwrap.wrap(\n                                line,\n                                initial_indent=initial_indent,\n                                subsequent_indent='      '))\n            else:\n                result.append(\"    No issues.\")\n            return '\\n'.join(result)\n\n    class _WcsValidateHduResult(list):\n        def __init__(self, hdu_index, hdu_name):\n            self._hdu_index = hdu_index\n            self._hdu_name = hdu_name\n            list.__init__(self)\n\n        def __repr__(self):\n            if len(self):\n                if self._hdu_name:\n                    hdu_name = ' ({0})'.format(self._hdu_name)\n                else:\n                    hdu_name = ''\n                result = ['HDU {0}{1}:'.format(self._hdu_index, hdu_name)]\n                for wcs in self:\n                    result.append(repr(wcs))\n                return '\\n'.join(result)\n            return ''\n\n    class _WcsValidateResults(list):\n        def __repr__(self):\n            result = []\n            for hdu in self:\n                content = repr(hdu)\n                if len(content):\n                    result.append(content)\n            return '\\n\\n'.join(result)\n\n    global __warningregistry__\n\n    if isinstance(source, fits.HDUList):\n        hdulist = source\n    else:\n        hdulist = fits.open(source)\n\n    results = _WcsValidateResults()\n\n    for i, hdu in enumerate(hdulist):\n        hdu_results = _WcsValidateHduResult(i, hdu.", "metadata": {"file_name": "astropy/wcs/wcs.py", "File Name": "astropy/wcs/wcs.py", "Classes": "NoConvergence, FITSFixedWarning, WCS, _WcsValidateWcsResult, _WcsValidateHduResult, _WcsValidateResults", "Functions": "_parse_keysel, __WCS_unpickle__, find_all_wcs, validate, write_d2i, write_dist, write_array, _return_list_of_arrays, _return_single_array"}}, {"code": "center : bool, optional\n            If `True` use the center of the pixel, otherwise use the corner.\n\n        Returns\n        -------\n        coord : (4, 2) array of (*x*, *y*) coordinates.\n            The order is clockwise starting with the bottom left corner.\n        \"\"\"\n        if axes is not None:\n            naxis1, naxis2 = axes\n        else:\n            if header is None:\n                try:\n                    # classes that inherit from WCS and define naxis1/2\n                    # do not require a header parameter\n                    naxis1 = self._naxis1\n                    naxis2 = self._naxis2\n                except AttributeError:\n                    warnings.warn(\"Need a valid header in order to calculate footprint\\n\", AstropyUserWarning)\n                    return None\n            else:\n                naxis1 = header.get('NAXIS1', None)\n                naxis2 = header.get('NAXIS2', None)\n\n        if naxis1 is None or naxis2 is None:\n            raise ValueError(\n                    \"Image size could not be determined.\")", "metadata": {"file_name": "astropy/wcs/wcs.py", "File Name": "astropy/wcs/wcs.py", "Classes": "NoConvergence, FITSFixedWarning, WCS, _WcsValidateWcsResult, _WcsValidateHduResult, _WcsValidateResults", "Functions": "_parse_keysel, __WCS_unpickle__, find_all_wcs, validate, write_d2i, write_dist, write_array, _return_list_of_arrays, _return_single_array"}}, {"code": "To use binary table image array or pixel\n        list keywords, *keysel* must be set.\n\n        Each element in the list should be one of the following strings:\n\n            - 'image': Image header keywords\n\n            - 'binary': Binary table image array keywords\n\n            - 'pixel': Pixel list keywords\n\n        Keywords such as ``EQUIna`` or ``RFRQna`` that are common to\n        binary table image arrays and pixel lists (including\n        ``WCSNna`` and ``TWCSna``) are selected by both 'binary' and\n        'pixel'.\n\n    fix : bool, optional\n        When `True` (default), call `~astropy.wcs.Wcsprm.fix` on\n        the resulting objects to fix any non-standard uses in the\n        header.  `FITSFixedWarning` warnings will be emitted if any\n        changes were made.\n\n    translate_units : str, optional\n        Specify which potentially unsafe translations of non-standard\n        unit strings to perform.  By default, performs none.  See\n        `WCS.fix` for more information about this parameter.  Only\n        effective when ``fix`` is `True`.", "metadata": {"file_name": "astropy/wcs/wcs.py", "File Name": "astropy/wcs/wcs.py", "Classes": "NoConvergence, FITSFixedWarning, WCS, _WcsValidateWcsResult, _WcsValidateHduResult, _WcsValidateResults", "Functions": "_parse_keysel, __WCS_unpickle__, find_all_wcs, validate, write_d2i, write_dist, write_array, _return_list_of_arrays, _return_single_array"}}, {"code": "# Certain values of the `j' index are used for storing\n        # radial terms; refer to Equation (1) in\n        # <http://web.ipac.caltech.edu/staff/shupe/reprints/SIP_to_PV_SPIE2012.pdf>.\n        pv = np.asarray(pv)\n        # Loop over distinct values of `i' index\n        for i in set(pv[:, 0]):\n            # Get all values of `j' index for this value of `i' index\n            js = set(pv[:, 1][pv[:, 0] == i])\n            # Find max value of `j' index\n            max_j = max(js)\n            for j in (3, 11, 23, 39):\n                if j < max_j and j in js:\n                    return\n\n        self.wcs.set_pv([])\n        warnings.warn(\"Removed redundant SCAMP distortion parameters \" +\n            \"because SIP parameters are also present\", FITSFixedWarning)\n\n    def fix(self, translate_units='', naxis=None):\n        \"\"\"\n        Perform the fix operations from wcslib, and warn about any\n        changes it has made.\n\n        Parameters\n        ----------\n        translate_units : str, optional\n            Specify which potentially unsafe translations of\n            non-standard unit strings to perform.  By default,\n            performs none.\n\n            Although ``\"S\"`` is commonly used to represent seconds,\n            its translation to ``\"s\"`` is potentially unsafe since the\n            standard recognizes ``\"S\"`` formally as Siemens, however\n            rarely that may be used.  The same applies to ``\"H\"`` for\n            hours (Henry), and ``\"D\"`` for days (Debye).\n\n            This string controls what to do in such cases, and is\n            case-insensitive.\n\n            - If the string contains ``\"s\"``, translate ``\"S\"`` to\n              ``\"s\"``.", "metadata": {"file_name": "astropy/wcs/wcs.py", "File Name": "astropy/wcs/wcs.py", "Classes": "NoConvergence, FITSFixedWarning, WCS, _WcsValidateWcsResult, _WcsValidateHduResult, _WcsValidateResults", "Functions": "_parse_keysel, __WCS_unpickle__, find_all_wcs, validate, write_d2i, write_dist, write_array, _return_list_of_arrays, _return_single_array"}}, {"code": "if self.cpdis1 is None and self.cpdis2 is None:\n            return\n\n        if dist == 'CPDIS':\n            d_kw = str('DP')\n            err_kw = str('CPERR')\n        else:\n            d_kw = str('DQ')\n            err_kw = str('CQERR')\n\n        def write_dist(num, cpdis):\n            if cpdis is None:\n                return\n\n            hdulist[0].header[str('{0}{1:d}').format(dist, num)] = (\n                'LOOKUP', 'Prior distortion function type')\n            hdulist[0].header[str('{0}{1:d}.EXTVER').format(d_kw, num)] = (\n                num, 'Version number of WCSDVARR extension')\n            hdulist[0].header[str('{0}{1:d}.NAXES').format(d_kw, num)] = (\n                len(cpdis.data.shape), 'Number of independent variables in distortion function')\n\n            for i in range(cpdis.data.ndim):\n                hdulist[0].header[str('{0}{1:d}.AXIS.", "metadata": {"file_name": "astropy/wcs/wcs.py", "File Name": "astropy/wcs/wcs.py", "Classes": "NoConvergence, FITSFixedWarning, WCS, _WcsValidateWcsResult, _WcsValidateHduResult, _WcsValidateResults", "Functions": "_parse_keysel, __WCS_unpickle__, find_all_wcs, validate, write_d2i, write_dist, write_array, _return_list_of_arrays, _return_single_array"}}, {"code": "# Licensed under a 3-clause BSD style license - see LICENSE.rst\n\"\"\"\nUnder the hood, there are 3 separate classes that perform different\nparts of the transformation:\n\n   - `~astropy.wcs.Wcsprm`: Is a direct wrapper of the core WCS\n     functionality in `wcslib`_.  (This includes TPV and TPD\n     polynomial distortion, but not SIP distortion).\n\n   - `~astropy.wcs.Sip`: Handles polynomial distortion as defined in the\n     `SIP`_ convention.\n\n   - `~astropy.wcs.DistortionLookupTable`: Handles `distortion paper`_\n     lookup tables.\n\nAdditionally, the class `WCS` aggregates all of these transformations\ntogether in a pipeline:\n\n   - Detector to image plane correction (by a pair of\n     `~astropy.wcs.DistortionLookupTable` objects).\n\n   - `SIP`_ distortion correction (by an underlying `~astropy.wcs.Sip`\n     object)\n\n   - `distortion paper`_ table-lookup correction (by a pair of\n     `~astropy.wcs.DistortionLookupTable` objects).\n\n   - `wcslib`_ WCS transformation (by a `~astropy.wcs.Wcsprm` object)\n\n\"\"\"\n\n# STDLIB\nimport copy\nimport io\nimport itertools\nimport os\nimport re\nimport textwrap\nimport warnings\nimport builtins\n\n# THIRD-PARTY\nimport numpy as np\n\n# LOCAL\nfrom .. import log\nfrom ..io import fits\nfrom . import _docutil as __\ntry:\n    from .", "metadata": {"file_name": "astropy/wcs/wcs.py", "File Name": "astropy/wcs/wcs.py", "Classes": "NoConvergence, FITSFixedWarning, WCS, _WcsValidateWcsResult, _WcsValidateHduResult, _WcsValidateResults", "Functions": "_parse_keysel, __WCS_unpickle__, find_all_wcs, validate, write_d2i, write_dist, write_array, _return_list_of_arrays, _return_single_array"}}, {"code": "{0:d}').format(i)\n                    if i == header[dp_axis_key]:\n                        d_data = fobj[str('D2IMARR'), d_extver].data\n                    else:\n                        d_data = (fobj[str('D2IMARR'), d_extver].data).transpose()\n                    del header[dp_axis_key]\n                    d_header = fobj[str('D2IMARR'), d_extver].header\n                    d_crpix = (d_header.get(str('CRPIX1'), 0.0), d_header.get(str('CRPIX2'), 0.0))\n                    d_crval = (d_header.get(str('CRVAL1'), 0.0), d_header.get(str('CRVAL2'), 0.0))\n                    d_cdelt = (d_header.get(str('CDELT1'), 1.0), d_header.get(str('CDELT2'), 1.0))\n                    d_lookup = DistortionLookupTable(d_data, d_crpix,\n                                                     d_crval, d_cdelt)\n                    tables[i] = d_lookup\n                else:\n                    warnings.warn('Polynomial distortion is not implemented.\\n', AstropyUserWarning)\n                for key in list(header):\n                    if key.startswith(dp + str('.')):\n                        del header[key]\n            else:\n                tables[i] = None\n        if not tables:\n            return (None, None)\n        else:\n            return (tables.get(1), tables.get(2))\n\n    def _read_d2im_old_format(self, header, fobj, axiscorr):\n        warnings.warn(\"The use of ``AXISCORR`` for D2IM correction has been deprecated.\"\n                      \"`~astropy.wcs` will read in files with ``AXISCORR`` but ``to_fits()`` will write \"\n                      \"out files without it.\",\n                      AstropyDeprecationWarning)\n        cpdis = [None, None]\n        crpix = [0., 0.]", "metadata": {"file_name": "astropy/wcs/wcs.py", "File Name": "astropy/wcs/wcs.py", "Classes": "NoConvergence, FITSFixedWarning, WCS, _WcsValidateWcsResult, _WcsValidateHduResult, _WcsValidateResults", "Functions": "_parse_keysel, __WCS_unpickle__, find_all_wcs, validate, write_d2i, write_dist, write_array, _return_list_of_arrays, _return_single_array"}}, {"code": "If ``i < 0`` (or not\n    provided), it will be set to the first spectral axis identified\n    from the ``CTYPE`` keyvalues in the FITS header.\n\nRaises\n------\nMemoryError\n    Memory allocation failed.\n\nSingularMatrixError\n    Linear transformation matrix is singular.\n\nInconsistentAxisTypesError\n    Inconsistent or unrecognized coordinate axis types.\n\nValueError\n    Invalid parameter value.\n\nInvalidTransformError\n    Invalid coordinate transformation parameters.\n\nInvalidTransformError\n    Ill-conditioned coordinate transformation parameters.\n\nInvalidSubimageSpecificationError\n    Invalid subimage specification (no spectral axis).\n\"\"\"\n\nssysobs = \"\"\"\n``string`` Spectral reference frame.\n\nThe spectral reference frame in which there is no differential\nvariation in the spectral coordinate across the field-of-view,\n``SSYSOBSa``.\n\nSee also\n--------\nastropy.wcs.Wcsprm.specsys, astropy.wcs.Wcsprm.velosys\n\"\"\"\n\nssyssrc = \"\"\"\n``string`` Spectral reference frame for redshift.\n\nThe spectral reference frame (standard of rest) in which the redshift\nwas measured, ``SSYSSRCa``.\n\"\"\"\n\nsub = \"\"\"\nsub(axes)\n\nExtracts the coordinate description for a subimage from a\n`~astropy.wcs.WCS` object.\n\nThe world coordinate system of the subimage must be separable in the\nsense that the world coordinates at any point in the subimage must\ndepend only on the pixel coordinates of the axes extracted.  In\npractice, this means that the ``PCi_ja`` matrix of the original image\nmust not contain non-zero off-diagonal terms that associate any of the\nsubimage axes with any of the non-subimage axes.\n\n`sub` can also add axes to a wcsprm object.  The new axes will be\ncreated using the defaults set by the Wcsprm constructor which produce\na simple, unnamed, linear axis with world coordinates equal to the\npixel coordinate.  These default values can be changed before\ninvoking `set`.\n\nParameters\n----------\naxes : int or a sequence.\n\n    - If an int, include the first *N* axes in their original order.\n\n    - If a sequence, may contain a combination of image axis numbers\n      (1-relative) or special axis identifiers (see below).", "metadata": {"file_name": "astropy/wcs/docstrings.py", "File Name": "astropy/wcs/docstrings.py"}}, {"code": "where((dnnew >= tol2) & conv)\n\n                else:\n                    # Apply correction:\n                    pix[ind] -= dpixnew\n                    dpix[ind] = dpixnew\n\n                    # Find indices of solutions that have not yet\n                    # converged to the requested accuracy:\n                    subind, = np.where(dnnew >= tol2)\n\n                # Choose solutions that need more iterations:\n                ind = ind[subind]\n                pix0 = pix0[subind]\n\n                k += 1\n\n        # ############################################################\n        # #         FINAL DETECTION OF INVALID, DIVERGING,          ##\n        # #         AND FAILED-TO-CONVERGE POINTS                   ##\n        # ############################################################\n        # Identify diverging and/or invalid points:\n        invalid = ((~np.all(np.isfinite(pix), axis=1)) &\n                   (np.all(np.isfinite(world), axis=1)))\n\n        # When detect_divergence==False, dnprev is outdated\n        # (it is the norm of the very first correction).", "metadata": {"file_name": "astropy/wcs/wcs.py", "File Name": "astropy/wcs/wcs.py", "Classes": "NoConvergence, FITSFixedWarning, WCS, _WcsValidateWcsResult, _WcsValidateHduResult, _WcsValidateResults", "Functions": "_parse_keysel, __WCS_unpickle__, find_all_wcs, validate, write_d2i, write_dist, write_array, _return_list_of_arrays, _return_single_array"}}, {"code": "Header):\n                try:\n                    # Accept any dict-like object\n                    orig_header = header\n                    header = fits.Header()\n                    for dict_key in orig_header.keys():\n                        header[dict_key] = orig_header[dict_key]\n                except TypeError:\n                    raise TypeError(\n                        \"header must be a string, an astropy.io.fits.Header \"\n                        \"object, or a dict-like object\")\n\n            if isinstance(header, fits.Header):\n                header_string = header.tostring().rstrip()\n            else:\n                header_string = header\n\n            # Importantly, header is a *copy* of the passed-in header\n            # because we will be modifying it\n            if isinstance(header_string, str):\n                header_bytes = header_string.encode('ascii')\n                header_string = header_string\n            else:\n                header_bytes = header_string\n                header_string = header_string.decode('ascii')\n\n            try:\n                tmp_header = fits.Header.fromstring(header_string)\n                self._remove_sip_kw(tmp_header)\n                tmp_header_bytes = tmp_header.tostring().rstrip()\n                if isinstance(tmp_header_bytes, str):\n                    tmp_header_bytes = tmp_header_bytes.encode('ascii')\n                tmp_wcsprm = _wcs.Wcsprm(header=tmp_header_bytes, key=key,\n                                         relax=relax, keysel=keysel_flags,\n                                         colsel=colsel, warnings=False)\n            except _wcs.NoWcsKeywordsFoundError:\n                est_naxis = 0\n            else:\n                if naxis is not None:\n                    try:\n                        tmp_wcsprm.sub(naxis)\n                    except ValueError:\n                        pass\n                    est_naxis = tmp_wcsprm.naxis\n                else:\n                    est_naxis = 2\n\n            header = fits.Header.fromstring(header_string)\n\n            if est_naxis == 0:\n                est_naxis = 2\n            self.naxis = est_naxis\n\n            det2im = self._read_det2im_kw(header,", "metadata": {"file_name": "astropy/wcs/wcs.py", "File Name": "astropy/wcs/wcs.py", "Classes": "NoConvergence, FITSFixedWarning, WCS, _WcsValidateWcsResult, _WcsValidateHduResult, _WcsValidateResults", "Functions": "_parse_keysel, __WCS_unpickle__, find_all_wcs, validate, write_d2i, write_dist, write_array, _return_list_of_arrays, _return_single_array"}}, {"code": "The units of the returned result are the same as the units of\n        the `~astropy.wcs.Wcsprm.cdelt`, `~astropy.wcs.Wcsprm.crval`,\n        and `~astropy.wcs.Wcsprm.cd` for the celestial WCS and can be\n        obtained by inquiring the value of `~astropy.wcs.Wcsprm.cunit`\n        property of the `~astropy.wcs.WCS.celestial` WCS object.\n\n    Raises\n    ------\n    ValueError\n        Pixel area is defined only for 2D pixels. Most likely the\n        `~astropy.wcs.Wcsprm.cd` matrix of the `~astropy.wcs.WCS.celestial`\n        WCS is not a square matrix of second order.\n\n    Notes\n    -----\n\n    Depending on the application, square root of the pixel area can be used to\n    represent a single pixel scale of an equivalent square pixel\n    whose area is equal to the area of a generally non-square pixel.\n\n    See Also\n    --------\n    astropy.wcs.utils.proj_plane_pixel_scales\n\n    \"\"\"\n    psm = wcs.celestial.pixel_scale_matrix\n    if psm.shape != (2, 2):\n        raise ValueError(\"Pixel area is defined only for 2D pixels.\")\n    return np.abs(np.linalg.det(psm))\n\n\ndef is_proj_plane_distorted(wcs, maxerr=1.0e-5):\n    r\"\"\"\n    For a WCS returns `False` if square image (detector) pixels stay square\n    when projected onto the \"plane of intermediate world coordinates\"\n    as defined in\n    `Greisen & Calabretta 2002, A&A, 395, 1061 <http://adsabs.harvard.edu/abs/2002A%26A...395.1061G>`_.", "metadata": {"file_name": "astropy/wcs/utils.py", "File Name": "astropy/wcs/utils.py", "Classes": "custom_wcs_to_frame_mappings, custom_frame_to_wcs_mappings", "Functions": "add_stokes_axis_to_wcs, _wcs_to_celestial_frame_builtin, _celestial_frame_to_wcs_builtin, wcs_to_celestial_frame, celestial_frame_to_wcs, proj_plane_pixel_scales, proj_plane_pixel_area, is_proj_plane_distorted, _is_cd_orthogonal, non_celestial_pixel_scales, _has_distortion, skycoord_to_pixel, pixel_to_skycoord"}}, {"code": "name)\n        results.append(hdu_results)\n\n        with warnings.catch_warnings(record=True) as warning_lines:\n            wcses = find_all_wcs(\n                hdu.header, relax=_wcs.WCSHDR_reject,\n                fix=False, _do_set=False)\n\n        for wcs in wcses:\n            wcs_results = _WcsValidateWcsResult(wcs.wcs.alt)\n            hdu_results.append(wcs_results)\n\n            try:\n                del __warningregistry__\n            except NameError:\n                pass\n\n            with warnings.catch_warnings(record=True) as warning_lines:\n                warnings.resetwarnings()\n                warnings.simplefilter(\n                    \"always\", FITSFixedWarning, append=True)\n\n                try:\n                    WCS(hdu.header,\n                        key=wcs.wcs.alt or ' ',\n                        relax=_wcs.WCSHDR_reject,\n                        fix=True, _do_set=False)\n                except WcsError as e:\n                    wcs_results.append(str(e))\n\n                wcs_results.extend([str(x.message) for x in warning_lines])\n\n    return results", "metadata": {"file_name": "astropy/wcs/wcs.py", "File Name": "astropy/wcs/wcs.py", "Classes": "NoConvergence, FITSFixedWarning, WCS, _WcsValidateWcsResult, _WcsValidateHduResult, _WcsValidateResults", "Functions": "_parse_keysel, __WCS_unpickle__, find_all_wcs, validate, write_d2i, write_dist, write_array, _return_list_of_arrays, _return_single_array"}}, {"code": "15976932 -70.8140779 ]\n         [  5.52653698 -72.05170795]]\n\n        >>> # First, turn detect_divergence on:\n        >>> try:  # doctest: +FLOAT_CMP\n        ...   xy = w.all_world2pix(divradec, 1, maxiter=20,\n        ...                        tolerance=1.0e-4, adaptive=False,\n        ...                        detect_divergence=True,\n        ...                        quiet=False)\n        ... except wcs.wcs.NoConvergence as e:\n        ...   print(\"Indices of diverging points: {{0}}\"\n        ...         .format(e.divergent))\n        ...   print(\"Indices of poorly converging points: {{0}}\"\n        ...         .format(e.slow_conv))\n        ...   print(\"Best solution:\\\\n{{0}}\".format(e.best_solution))\n        ...   print(\"Achieved accuracy:\\\\n{{0}}\".format(e.accuracy))\n        Indices of diverging points: [1]\n        Indices of poorly converging points: None\n        Best solution:\n        [[  1.00000238e+00   9.99999965e-01]\n         [ -1.99441636e+06   1.44309097e+06]\n         [  3.00000236e+00   9.99999966e-01]]\n        Achieved accuracy:\n        [[  6.13968380e-05   8.59638593e-07]\n         [  8.59526812e+11   6.61713548e+11]\n         [  6.09398446e-05   8.38759724e-07]]\n        >>> raise e\n        Traceback (most recent call last):\n        ...", "metadata": {"file_name": "astropy/wcs/wcs.py", "File Name": "astropy/wcs/wcs.py", "Classes": "NoConvergence, FITSFixedWarning, WCS, _WcsValidateWcsResult, _WcsValidateHduResult, _WcsValidateResults", "Functions": "_parse_keysel, __WCS_unpickle__, find_all_wcs, validate, write_d2i, write_dist, write_array, _return_list_of_arrays, _return_single_array"}}, {"code": "Returns\n    -------\n    xp, yp : `numpy.ndarray`\n        The pixel coordinates\n\n    See Also\n    --------\n    astropy.coordinates.SkyCoord.from_pixel\n    \"\"\"\n\n    if _has_distortion(wcs) and wcs.naxis != 2:\n        raise ValueError(\"Can only handle WCS with distortions for 2-dimensional WCS\")\n\n    # Keep only the celestial part of the axes, also re-orders lon/lat\n    wcs = wcs.sub([WCSSUB_LONGITUDE, WCSSUB_LATITUDE])\n\n    if wcs.naxis != 2:\n        raise ValueError(\"WCS should contain celestial component\")\n\n    # Check which frame the WCS uses\n    frame = wcs_to_celestial_frame(wcs)\n\n    # Check what unit the WCS needs\n    xw_unit = u.Unit(wcs.wcs.cunit[0])\n    yw_unit = u.Unit(wcs.wcs.cunit[1])\n\n    # Convert positions to frame\n    coords = coords.transform_to(frame)\n\n    # Extract longitude and latitude. We first try and use lon/lat directly,\n    # but if the representation is not spherical or unit spherical this will\n    # fail. We should then force the use of the unit spherical\n    # representation. We don't do that directly to make sure that we preserve\n    # custom lon/lat representations if available.", "metadata": {"file_name": "astropy/wcs/utils.py", "File Name": "astropy/wcs/utils.py", "Classes": "custom_wcs_to_frame_mappings, custom_frame_to_wcs_mappings", "Functions": "add_stokes_axis_to_wcs, _wcs_to_celestial_frame_builtin, _celestial_frame_to_wcs_builtin, wcs_to_celestial_frame, celestial_frame_to_wcs, proj_plane_pixel_scales, proj_plane_pixel_area, is_proj_plane_distorted, _is_cd_orthogonal, non_celestial_pixel_scales, _has_distortion, skycoord_to_pixel, pixel_to_skycoord"}}, {"code": "quiet : bool, optional (Default = False)\n            Do not throw :py:class:`NoConvergence` exceptions when\n            the method does not converge to a solution with the\n            required accuracy within a specified number of maximum\n            iterations set by ``maxiter`` parameter. Instead,\n            simply return the found solution.\n\n        Other Parameters\n        ----------------\n        adaptive : bool, optional (Default = False)\n            Specifies whether to adaptively select only points that\n            did not converge to a solution within the required\n            accuracy for the next iteration. Default is recommended\n            for HST as well as most other instruments.\n\n            .. note::\n               The :py:meth:`all_world2pix` uses a vectorized\n               implementation of the method of consecutive\n               approximations (see ``Notes`` section below) in which it\n               iterates over *all* input points *regardless* until\n               the required accuracy has been reached for *all* input\n               points. In some cases it may be possible that\n               *almost all* points have reached the required accuracy\n               but there are only a few of input data points for\n               which additional iterations may be needed (this\n               depends mostly on the characteristics of the geometric\n               distortions for a given instrument). In this situation\n               it may be advantageous to set ``adaptive`` = `True` in\n               which case :py:meth:`all_world2pix` will continue\n               iterating *only* over the points that have not yet\n               converged to the required accuracy. However, for the\n               HST's ACS/WFC detector, which has the strongest\n               distortions of all HST instruments, testing has\n               shown that enabling this option would lead to a about\n               50-100% penalty in computational time (depending on\n               specifics of the image, geometric distortions, and\n               number of input points to be converted).", "metadata": {"file_name": "astropy/wcs/wcs.py", "File Name": "astropy/wcs/wcs.py", "Classes": "NoConvergence, FITSFixedWarning, WCS, _WcsValidateWcsResult, _WcsValidateHduResult, _WcsValidateResults", "Functions": "_parse_keysel, __WCS_unpickle__, find_all_wcs, validate, write_d2i, write_dist, write_array, _return_list_of_arrays, _return_single_array"}}, {"code": "def _is_cd_orthogonal(cd, maxerr):\n    shape = cd.shape\n    if not (len(shape) == 2 and shape[0] == shape[1]):\n        raise ValueError(\"CD (or PC) matrix must be a 2D square matrix.\")\n\n    pixarea = np.abs(np.linalg.det(cd))\n    if (pixarea == 0.0):\n        raise ValueError(\"CD (or PC) matrix is singular.\")\n\n    # NOTE: Technically, below we should use np.dot(cd, np.conjugate(cd.T))\n    # However, I am not aware of complex CD/PC matrices...\n    I = np.dot(cd, cd.T) / pixarea\n    cd_unitary_err = np.amax(np.abs(I - np.eye(shape[0])))\n\n    return (cd_unitary_err < maxerr)\n\n\ndef non_celestial_pixel_scales(inwcs):\n    \"\"\"\n    Calculate the pixel scale along each axis of a non-celestial WCS,\n    for example one with mixed spectral and spatial axes.\n\n    Parameters\n    ----------\n    inwcs : `~astropy.wcs.WCS`\n        The world coordinate system object.\n\n    Returns\n    -------\n    scale : `numpy.ndarray`\n        The pixel scale along each axis.\n    \"\"\"\n\n    if inwcs.is_celestial:\n        raise ValueError(\"WCS is celestial, use celestial_pixel_scales instead\")\n\n    pccd = inwcs.pixel_scale_matrix\n\n    if np.allclose(np.extract(1-np.eye(*pccd.shape), pccd), 0):\n        return np.abs(np.diagonal(pccd))*u.deg\n    else:\n        raise ValueError(\"WCS is rotated, cannot determine consistent pixel scales\")", "metadata": {"file_name": "astropy/wcs/utils.py", "File Name": "astropy/wcs/utils.py", "Classes": "custom_wcs_to_frame_mappings, custom_frame_to_wcs_mappings", "Functions": "add_stokes_axis_to_wcs, _wcs_to_celestial_frame_builtin, _celestial_frame_to_wcs_builtin, wcs_to_celestial_frame, celestial_frame_to_wcs, proj_plane_pixel_scales, proj_plane_pixel_area, is_proj_plane_distorted, _is_cd_orthogonal, non_celestial_pixel_scales, _has_distortion, skycoord_to_pixel, pixel_to_skycoord"}}, {"code": "format='byear')\n        frame = FK4NoETerms(equinox=equinox)\n    elif radesys == 'FK5':\n        if equinox is not None:\n            equinox = Time(equinox, format='jyear')\n        frame = FK5(equinox=equinox)\n    elif radesys == 'ICRS':\n        frame = ICRS()\n    else:\n        if xcoord == 'GLON' and ycoord == 'GLAT':\n            frame = Galactic()\n        elif xcoord == 'TLON' and ycoord == 'TLAT':\n            frame = ITRS(obstime=wcs.wcs.dateobs or None)\n        else:\n            frame = None\n\n    return frame", "metadata": {"file_name": "astropy/wcs/utils.py", "File Name": "astropy/wcs/utils.py", "Classes": "custom_wcs_to_frame_mappings, custom_frame_to_wcs_mappings", "Functions": "add_stokes_axis_to_wcs, _wcs_to_celestial_frame_builtin, _celestial_frame_to_wcs_builtin, wcs_to_celestial_frame, celestial_frame_to_wcs, proj_plane_pixel_scales, proj_plane_pixel_area, is_proj_plane_distorted, _is_cd_orthogonal, non_celestial_pixel_scales, _has_distortion, skycoord_to_pixel, pixel_to_skycoord"}}, {"code": "Therefore,\n               for HST and possibly instruments, it is recommended\n               to set ``adaptive`` = `False`. The only danger in\n               getting this setting wrong will be a performance\n               penalty.\n\n            .. note::\n               When ``detect_divergence`` is `True`,\n               :py:meth:`all_world2pix` will automatically switch\n               to the adaptive algorithm once divergence has been\n               detected.\n\n        detect_divergence : bool, optional (Default = True)\n            Specifies whether to perform a more detailed analysis\n            of the convergence to a solution. Normally\n            :py:meth:`all_world2pix` may not achieve the required\n            accuracy if either the ``tolerance`` or ``maxiter`` arguments\n            are too low. However, it may happen that for some\n            geometric distortions the conditions of convergence for\n            the the method of consecutive approximations used by\n            :py:meth:`all_world2pix` may not be satisfied, in which\n            case consecutive approximations to the solution will\n            diverge regardless of the ``tolerance`` or ``maxiter``\n            settings.\n\n            When ``detect_divergence`` is `False`, these divergent\n            points will be detected as not having achieved the\n            required accuracy (without further details). In addition,\n            if ``adaptive`` is `False` then the algorithm will not\n            know that the solution (for specific points) is diverging\n            and will continue iterating and trying to \"improve\"\n            diverging solutions. This may result in ``NaN`` or\n            ``Inf`` values in the return results (in addition to a\n            performance penalties).", "metadata": {"file_name": "astropy/wcs/wcs.py", "File Name": "astropy/wcs/wcs.py", "Classes": "NoConvergence, FITSFixedWarning, WCS, _WcsValidateWcsResult, _WcsValidateHduResult, _WcsValidateResults", "Functions": "_parse_keysel, __WCS_unpickle__, find_all_wcs, validate, write_d2i, write_dist, write_array, _return_list_of_arrays, _return_single_array"}}, {"code": "The number of axes, which is set as the ``naxis`` member, may\n       differ for different coordinate representations of the same\n       image.\n\n    3. When the header includes duplicate keywords, in most cases the\n       last encountered is used.\n\n    4. `~astropy.wcs.Wcsprm.set` is called immediately after\n       construction, so any invalid keywords or transformations will\n       be raised by the constructor, not when subsequently calling a\n       transformation method.\n\n    \"\"\"\n\n    def __init__(self, header=None, fobj=None, key=' ', minerr=0.0,\n                 relax=True, naxis=None, keysel=None, colsel=None,\n                 fix=True, translate_units='', _do_set=True):\n        close_fds = []\n\n        if header is None:\n            if naxis is None:\n                naxis = 2\n            wcsprm = _wcs.Wcsprm(header=None, key=key,\n                                 relax=relax, naxis=naxis)\n            self.naxis = wcsprm.naxis\n            # Set some reasonable defaults.\n            det2im = (None, None)\n            cpdis = (None, None)\n            sip = None\n        else:\n            keysel_flags = _parse_keysel(keysel)\n\n            if isinstance(header, (str, bytes)):\n                try:\n                    is_path = (possible_filename(header) and\n                               os.path.exists(header))\n                except (OSError, ValueError):\n                    is_path = False\n\n                if is_path:\n                    if fobj is not None:\n                        raise ValueError(\n                            \"Can not provide both a FITS filename to \"\n                            \"argument 1 and a FITS file object to argument 2\")\n                    fobj = fits.open(header)\n                    close_fds.append(fobj)\n                    header = fobj[0].header\n            elif isinstance(header, fits.hdu.image._ImageBaseHDU):\n                header = header.header\n            elif not isinstance(header, fits.", "metadata": {"file_name": "astropy/wcs/wcs.py", "File Name": "astropy/wcs/wcs.py", "Classes": "NoConvergence, FITSFixedWarning, WCS, _WcsValidateWcsResult, _WcsValidateHduResult, _WcsValidateResults", "Functions": "_parse_keysel, __WCS_unpickle__, find_all_wcs, validate, write_d2i, write_dist, write_array, _return_list_of_arrays, _return_single_array"}}, {"code": "if self.det2im1 is None and self.det2im2 is None:\n            return\n        dist = 'D2IMDIS'\n        d_kw = 'D2IM'\n        err_kw = 'D2IMERR'\n\n        def write_d2i(num, det2im):\n            if det2im is None:\n                return\n            str('{0}{1:d}').format(dist, num),\n            hdulist[0].header[str('{0}{1:d}').format(dist, num)] = (\n                'LOOKUP', 'Detector to image correction type')\n            hdulist[0].header[str('{0}{1:d}.EXTVER').format(d_kw, num)] = (\n                num, 'Version number of WCSDVARR extension')\n            hdulist[0].header[str('{0}{1:d}.NAXES').format(d_kw, num)] = (\n                len(det2im.data.shape), 'Number of independent variables in d2im function')\n            for i in range(det2im.data.ndim):\n                hdulist[0].header[str('{0}{1:d}.AXIS.", "metadata": {"file_name": "astropy/wcs/wcs.py", "File Name": "astropy/wcs/wcs.py", "Classes": "NoConvergence, FITSFixedWarning, WCS, _WcsValidateWcsResult, _WcsValidateHduResult, _WcsValidateResults", "Functions": "_parse_keysel, __WCS_unpickle__, find_all_wcs, validate, write_d2i, write_dist, write_array, _return_list_of_arrays, _return_single_array"}}, {"code": "seterr(invalid='ignore', over='ignore')\n\n        # ############################################################\n        # #                NON-ADAPTIVE ITERATIONS:                 ##\n        # ############################################################\n        if not adaptive:\n            # Fixed-point iterations:\n            while (np.nanmax(dn) >= tol2 and k < maxiter):\n                # Find correction to the previous solution:\n                dpix = self.pix2foc(pix, origin) - pix0\n\n                # Compute norm (L2) squared of the correction:\n                dn = np.sum(dpix*dpix, axis=1)\n\n                # Check for divergence (we do this in two stages\n                # to optimize performance for the most common\n                # scenario when successive approximations converge):\n                if detect_divergence:\n                    divergent = (dn >= dnprev)\n                    if np.any(divergent):\n                        # Find solutions that have not yet converged:\n                        slowconv = (dn >= tol2)\n                        inddiv, = np.where(divergent & slowconv)\n\n                        if inddiv.shape[0] > 0:\n                            # Update indices of elements that\n                            # still need correction:\n                            conv = (dn < dnprev)\n                            iconv = np.where(conv)\n\n                            # Apply correction:\n                            dpixgood = dpix[iconv]\n                            pix[iconv] -= dpixgood\n                            dpix[iconv] = dpixgood\n\n                            # For the next iteration choose\n                            # non-divergent points that have not yet\n                            # converged to the requested accuracy:\n                            ind, = np.", "metadata": {"file_name": "astropy/wcs/wcs.py", "File Name": "astropy/wcs/wcs.py", "Classes": "NoConvergence, FITSFixedWarning, WCS, _WcsValidateWcsResult, _WcsValidateHduResult, _WcsValidateResults", "Functions": "_parse_keysel, __WCS_unpickle__, find_all_wcs, validate, write_d2i, write_dist, write_array, _return_list_of_arrays, _return_single_array"}}, {"code": "BaseCoordinateFrame`\n        subclass instance for which to find the WCS\n    projection : str\n        Projection code to use in ctype, if applicable\n\n    Returns\n    -------\n    wcs : :class:`~astropy.wcs.WCS` instance\n        The corresponding WCS object\n\n    Examples\n    --------\n\n    ::\n\n        >>> from astropy.wcs.utils import celestial_frame_to_wcs\n        >>> from astropy.coordinates import FK5\n        >>> frame = FK5(equinox='J2010')\n        >>> wcs = celestial_frame_to_wcs(frame)\n        >>> wcs.to_header()\n        WCSAXES =                    2 / Number of coordinate axes\n        CRPIX1  =                  0.0 / Pixel coordinate of reference point\n        CRPIX2  =                  0.0 / Pixel coordinate of reference point\n        CDELT1  =                  1.0 / [deg] Coordinate increment at reference point\n        CDELT2  =                  1.0 / [deg] Coordinate increment at reference point\n        CUNIT1  = 'deg'                / Units of coordinate increment and value\n        CUNIT2  = 'deg'                / Units of coordinate increment and value\n        CTYPE1  = 'RA---TAN'           / Right ascension, gnomonic projection\n        CTYPE2  = 'DEC--TAN'           / Declination, gnomonic projection\n        CRVAL1  =                  0.0 / [deg] Coordinate value at reference point\n        CRVAL2  =                  0.0 / [deg] Coordinate value at reference point\n        LONPOLE =                180.0 / [deg] Native longitude of celestial pole\n        LATPOLE =                  0.", "metadata": {"file_name": "astropy/wcs/utils.py", "File Name": "astropy/wcs/utils.py", "Classes": "custom_wcs_to_frame_mappings, custom_frame_to_wcs_mappings", "Functions": "add_stokes_axis_to_wcs, _wcs_to_celestial_frame_builtin, _celestial_frame_to_wcs_builtin, wcs_to_celestial_frame, celestial_frame_to_wcs, proj_plane_pixel_scales, proj_plane_pixel_area, is_proj_plane_distorted, _is_cd_orthogonal, non_celestial_pixel_scales, _has_distortion, skycoord_to_pixel, pixel_to_skycoord"}}, {"code": "def _celestial_frame_to_wcs_builtin(frame, projection='TAN'):\n\n    # Import astropy.coordinates here to avoid circular imports\n    from ..coordinates import BaseRADecFrame, FK4, FK4NoETerms, FK5, ICRS, ITRS, Galactic\n\n    # Create a 2-dimensional WCS\n    wcs = WCS(naxis=2)\n\n    if isinstance(frame, BaseRADecFrame):\n\n        xcoord = 'RA--'\n        ycoord = 'DEC-'\n        if isinstance(frame, ICRS):\n            wcs.wcs.radesys = 'ICRS'\n        elif isinstance(frame, FK4NoETerms):\n            wcs.wcs.radesys = 'FK4-NO-E'\n            wcs.wcs.equinox = frame.equinox.byear\n        elif isinstance(frame, FK4):\n            wcs.wcs.radesys = 'FK4'\n            wcs.wcs.equinox = frame.equinox.byear\n        elif isinstance(frame, FK5):\n            wcs.wcs.radesys = 'FK5'\n            wcs.wcs.equinox = frame.equinox.jyear\n        else:\n            return None\n    elif isinstance(frame, Galactic):\n        xcoord = 'GLON'\n        ycoord = 'GLAT'\n    elif isinstance(frame, ITRS):\n        xcoord = 'TLON'\n        ycoord = 'TLAT'\n        wcs.wcs.radesys = 'ITRS'\n        wcs.wcs.dateobs = frame.obstime.utc.isot\n    else:\n        return None\n\n    wcs.wcs.ctype = [xcoord + '-' + projection, ycoord + '-' + projection]\n\n    return wcs\n\n\nWCS_FRAME_MAPPINGS = [[_wcs_to_celestial_frame_builtin]]\nFRAME_WCS_MAPPINGS = [[_celestial_frame_to_wcs_builtin]]", "metadata": {"file_name": "astropy/wcs/utils.py", "File Name": "astropy/wcs/utils.py", "Classes": "custom_wcs_to_frame_mappings, custom_frame_to_wcs_mappings", "Functions": "add_stokes_axis_to_wcs, _wcs_to_celestial_frame_builtin, _celestial_frame_to_wcs_builtin, wcs_to_celestial_frame, celestial_frame_to_wcs, proj_plane_pixel_scales, proj_plane_pixel_area, is_proj_plane_distorted, _is_cd_orthogonal, non_celestial_pixel_scales, _has_distortion, skycoord_to_pixel, pixel_to_skycoord"}}, {"code": "If no ``CROTAia`` is\nassociated with the latitude axis, `~astropy.wcs.Wcsprm.set` reverts\nto a unity ``PCi_ja`` matrix.\n\"\"\"\n\ncdelt = \"\"\"\n``double array[naxis]`` Coordinate increments (``CDELTia``) for each\ncoord axis.\n\nIf a ``CDi_ja`` linear transformation matrix is present, a warning is\nraised and `~astropy.wcs.Wcsprm.cdelt` is ignored.  The ``CDi_ja``\nmatrix may be deleted by::\n\n  del wcs.wcs.cd\n\nAn undefined value is represented by NaN.\n\"\"\"\n\ncdfix = \"\"\"\ncdfix()\n\nFix erroneously omitted ``CDi_ja`` keywords.\n\nSets the diagonal element of the ``CDi_ja`` matrix to unity if all\n``CDi_ja`` keywords associated with a given axis were omitted.\nAccording to Paper I, if any ``CDi_ja`` keywords at all are given in a\nFITS header then those not given default to zero.  This results in a\nsingular matrix with an intersecting row and column of zeros.\n\nReturns\n-------\nsuccess : int\n    Returns ``0`` for success; ``-1`` if no change required.\n\"\"\"\n\ncel_offset = \"\"\"\n``boolean`` Is there an offset?\n\nIf `True`, an offset will be applied to ``(x, y)`` to force ``(x, y) =\n(0, 0)`` at the fiducial point, (phi_0, theta_0).  Default is `False`.\n\"\"\"\n\ncelfix = \"\"\"\nTranslates AIPS-convention celestial projection types, ``-NCP`` and\n``-GLS``.\n\nReturns\n-------\nsuccess : int\n    Returns ``0`` for success; ``-1`` if no change required.\n\"\"\"\n\ncname = \"\"\"\n``list of strings`` A list of the coordinate axis names, from\n``CNAMEia``.\n\"\"\"\n\ncolax = \"\"\"\n``int array[naxis]`` An array recording the column numbers for each\naxis in a pixel list.\n\"\"\"\n\ncolnum = \"\"\"\n``int`` Column of FITS binary table associated with this WCS.\n\nWhere the coordinate representation is associated with an image-array\ncolumn in a FITS binary table, this property may be used to record the\nrelevant column number.\n\nIt should be set to zero for an image header or pixel list.\n\"\"\"", "metadata": {"file_name": "astropy/wcs/docstrings.py", "File Name": "astropy/wcs/docstrings.py"}}, {"code": "It will return `True` if transformation from image (detector) coordinates\n    to the focal plane coordinates is non-orthogonal or if WCS contains\n    non-linear (e.g., SIP) distortions.\n\n    .. note::\n        Since this function is concerned **only** about the transformation\n        \"image plane\"->\"focal plane\" and **not** about the transformation\n        \"celestial sphere\"->\"focal plane\"->\"image plane\",\n        this function ignores distortions arising due to non-linear nature\n        of most projections.\n\n    Let's denote by *C* either the original or the reconstructed\n    (from ``PC`` and ``CDELT``) CD matrix. `is_proj_plane_distorted`\n    verifies that the transformation from image (detector) coordinates\n    to the focal plane coordinates is orthogonal using the following\n    check:\n\n    .. math::\n        \\left \\| \\frac{C \\cdot C^{\\mathrm{T}}}\n        {| det(C)|} - I \\right \\|_{\\mathrm{max}} < \\epsilon .\n\n    Parameters\n    ----------\n    wcs : `~astropy.wcs.WCS`\n        World coordinate system object\n\n    maxerr : float, optional\n        Accuracy to which the CD matrix, **normalized** such\n        that :math:`|det(CD)|=1`, should be close to being an\n        orthogonal matrix as described in the above equation\n        (see :math:`\\epsilon`).\n\n    Returns\n    -------\n    distorted : bool\n        Returns `True` if focal (projection) plane is distorted and `False`\n        otherwise.\n\n    \"\"\"\n    cwcs = wcs.celestial\n    return (not _is_cd_orthogonal(cwcs.pixel_scale_matrix, maxerr) or\n            _has_distortion(cwcs))", "metadata": {"file_name": "astropy/wcs/utils.py", "File Name": "astropy/wcs/utils.py", "Classes": "custom_wcs_to_frame_mappings, custom_frame_to_wcs_mappings", "Functions": "add_stokes_axis_to_wcs, _wcs_to_celestial_frame_builtin, _celestial_frame_to_wcs_builtin, wcs_to_celestial_frame, celestial_frame_to_wcs, proj_plane_pixel_scales, proj_plane_pixel_area, is_proj_plane_distorted, _is_cd_orthogonal, non_celestial_pixel_scales, _has_distortion, skycoord_to_pixel, pixel_to_skycoord"}}, {"code": "cpdis1 = \"\"\"\n`~astropy.wcs.DistortionLookupTable`\n\nThe pre-linear transformation distortion lookup table, ``CPDIS1``.\n\"\"\"\n\ncpdis2 = \"\"\"\n`~astropy.wcs.DistortionLookupTable`\n\nThe pre-linear transformation distortion lookup table, ``CPDIS2``.\n\"\"\"\n\ncrder = \"\"\"\n``double array[naxis]`` The random error in each coordinate axis,\n``CRDERia``.\n\nAn undefined value is represented by NaN.\n\"\"\"\n\ncrota = \"\"\"\n``double array[naxis]`` ``CROTAia`` keyvalues for each coordinate\naxis.\n\nFor historical compatibility, three alternate specifications of the\nlinear transformations are available in wcslib.  The canonical\n``PCi_ja`` with ``CDELTia``, ``CDi_ja``, and the deprecated\n``CROTAia`` keywords.  Although the latter may not formally co-exist\nwith ``PCi_ja``, the approach here is simply to ignore them if given\nin conjunction with ``PCi_ja``.\n\n`~astropy.wcs.Wcsprm.has_pc`, `~astropy.wcs.Wcsprm.has_cd` and\n`~astropy.wcs.Wcsprm.has_crota` can be used to determine which of\nthese alternatives are present in the header.\n\nThese alternate specifications of the linear transformation matrix are\ntranslated immediately to ``PCi_ja`` by `~astropy.wcs.Wcsprm.set` and\nare nowhere visible to the lower-level routines.  In particular,\n`~astropy.wcs.Wcsprm.set` resets `~astropy.wcs.Wcsprm.cdelt` to unity\nif ``CDi_ja`` is present (and no ``PCi_ja``).  If no ``CROTAia`` is\nassociated with the latitude axis, `~astropy.wcs.Wcsprm.set` reverts\nto a unity ``PCi_ja`` matrix.\n\"\"\"\n\ncrpix = \"\"\"\n``double array[naxis]`` Coordinate reference pixels (``CRPIXja``) for\neach pixel axis.\n\"\"\"\n\ncrval = \"\"\"\n``double array[naxis]`` Coordinate reference values (``CRVALia``) for\neach coordinate axis.\n\"\"\"\n\ncrval_tabprm = \"\"\"\n``double array[M]`` Index values for the reference pixel for each of\nthe tabular coord axes.\n\"\"\"", "metadata": {"file_name": "astropy/wcs/docstrings.py", "File Name": "astropy/wcs/docstrings.py"}}, {"code": "The codes may also be negated to extract all but the types specified,\nfor example::\n\n    wcs.sub([\n      WCSSUB_LONGITUDE,\n      WCSSUB_LATITUDE,\n      WCSSUB_CUBEFACE,\n      -(WCSSUB_SPECTRAL | WCSSUB_STOKES)])\n\nThe last of these specifies all axis types other than spectral or\nStokes.  Extraction is done in the order specified by ``axes``, i.e. a\nlongitude axis (if present) would be extracted first (via ``axes[0]``)\nand not subsequently (via ``axes[3]``).  Likewise for the latitude and\ncubeface axes in this example.\n\nThe number of dimensions in the returned object may be less than or\ngreater than the length of ``axes``.  However, it will never exceed the\nnumber of axes in the input image.\n\"\"\"\n\ntab = \"\"\"\n``list of Tabprm`` Tabular coordinate objects.\n\nA list of tabular coordinate objects associated with this WCS.\n\"\"\"\n\nTabprm = \"\"\"\nA class to store the information related to tabular coordinates,\ni.e., coordinates that are defined via a lookup table.\n\nThis class can not be constructed directly from Python, but instead is\nreturned from `~astropy.wcs.Wcsprm.tab`.\n\"\"\"\n\ntheta0 = \"\"\"\n``double``  The native longitude of the fiducial point.\n\nThe point whose celestial coordinates are given in ``ref[1:2]``.  If\nundefined (NaN) the initialization routine, `~astropy.wcs.Wcsprm.set`,\nwill set this to a projection-specific default.\n\nSee also\n--------\nastropy.wcs.Wcsprm.phi0\n\"\"\"\n\nto_header = \"\"\"\nto_header(relax=False)\n\n`to_header` translates a WCS object into a FITS header.\n\nThe details of the header depends on context:\n\n    - If the `~astropy.wcs.Wcsprm.colnum` member is non-zero then a\n      binary table image array header will be produced.\n\n    - Otherwise, if the `~astropy.wcs.Wcsprm.colax` member is set\n      non-zero then a pixel list header will be produced.\n\n    - Otherwise, a primary image or image extension header will be\n      produced.\n\nThe output header will almost certainly differ from the input in a\nnumber of respects:\n\n    1. The output header only contains WCS-related keywords.", "metadata": {"file_name": "astropy/wcs/docstrings.py", "File Name": "astropy/wcs/docstrings.py"}}, {"code": ".format(coordsys))\n\n        with open(filename, mode='w') as f:\n            f.write(comments)\n            f.write('{}\\n'.format(coordsys))\n            f.write('polygon(')\n            self.calc_footprint().tofile(f, sep=',')\n            f.write(') # color={0}, width={1:d} \\n'.format(color, width))\n\n    @property\n    def _naxis1(self):\n        return self._naxis[0]\n\n    @_naxis1.setter\n    def _naxis1(self, value):\n        self._naxis[0] = value\n\n    @property\n    def _naxis2(self):\n        return self._naxis[1]\n\n    @_naxis2.setter\n    def _naxis2(self, value):\n        self._naxis[1] = value\n\n    def _get_naxis(self, header=None):\n        _naxis = []\n        if (header is not None and\n                not isinstance(header, (str, bytes))):\n            for naxis in itertools.count(1):\n                try:\n                    _naxis.append(header['NAXIS{}'.format(naxis)])\n                except KeyError:\n                    break\n        if len(_naxis) == 0:\n            _naxis = [0, 0]\n        elif len(_naxis) == 1:\n            _naxis.append(0)\n        self._naxis = _naxis\n\n    def printwcs(self):\n        print(repr(self))\n\n    def __repr__(self):\n        '''\n        Return a short description. Simply porting the behavior from\n        the `printwcs()` method.\n        '''", "metadata": {"file_name": "astropy/wcs/wcs.py", "File Name": "astropy/wcs/wcs.py", "Classes": "NoConvergence, FITSFixedWarning, WCS, _WcsValidateWcsResult, _WcsValidateHduResult, _WcsValidateResults", "Functions": "_parse_keysel, __WCS_unpickle__, find_all_wcs, validate, write_d2i, write_dist, write_array, _return_list_of_arrays, _return_single_array"}}, {"code": "We also\n        # consider the process to be diverging if\n        # `|x_{i+1}-x_i|>|x_i-x_{i-1}|`\n        # **when** `|x_{i+1}-x_i|>=tolerance` (when current\n        # approximation is close to the true solution,\n        # `|x_{i+1}-x_i|>|x_i-x_{i-1}|` may be due to rounding errors\n        # and we ignore such \"divergences\" when\n        # `|x_{i+1}-x_i|<tolerance`). It may appear that checking for\n        # `|x_{i+1}-x_i|<tolerance` in order to ignore divergence is\n        # unnecessary since the iterative process should stop anyway,\n        # however, the proposed implementation of this iterative\n        # process is completely vectorized and, therefore, we may\n        # continue iterating over *some* points even though they have\n        # converged to within a specified tolerance (while iterating\n        # over other points that have not yet converged to\n        # a solution).\n        #\n        # In order to efficiently implement iterative process (6)\n        # using available methods in `astropy.wcs.WCS`, we add and\n        # subtract `x_i` from the right side of equation (6):\n        #\n        # (7)   x_{i+1} = x'-(x_i+f(x_i))+x_i = x'-pix2foc(x_i)+x_i,\n        #\n        # where `x'=wcs_world2pix(w)` and it is computed only *once*\n        # before the beginning of the iterative process (and we also\n        # set `x_0=x'`).", "metadata": {"file_name": "astropy/wcs/wcs.py", "File Name": "astropy/wcs/wcs.py", "Classes": "NoConvergence, FITSFixedWarning, WCS, _WcsValidateWcsResult, _WcsValidateHduResult, _WcsValidateResults", "Functions": "_parse_keysel, __WCS_unpickle__, find_all_wcs, validate, write_d2i, write_dist, write_array, _return_list_of_arrays, _return_single_array"}}, {"code": "Raises\n------\nMemoryError\n    Memory allocation failed.\n\nValueError\n    Invalid coordinate transformation parameters.\n\"\"\".format(__.ORIGIN())\n\npc = \"\"\"\n``double array[naxis][naxis]`` The ``PCi_ja`` (pixel coordinate)\ntransformation matrix.\n\nThe order is::\n\n  [[PC1_1, PC1_2],\n   [PC2_1, PC2_2]]\n\nFor historical compatibility, three alternate specifications of the\nlinear transformations are available in wcslib.  The canonical\n``PCi_ja`` with ``CDELTia``, ``CDi_ja``, and the deprecated\n``CROTAia`` keywords.  Although the latter may not formally co-exist\nwith ``PCi_ja``, the approach here is simply to ignore them if given\nin conjunction with ``PCi_ja``.\n\n`~astropy.wcs.Wcsprm.has_pc`, `~astropy.wcs.Wcsprm.has_cd` and\n`~astropy.wcs.Wcsprm.has_crota` can be used to determine which of\nthese alternatives are present in the header.\n\nThese alternate specifications of the linear transformation matrix are\ntranslated immediately to ``PCi_ja`` by `~astropy.wcs.Wcsprm.set` and\nare nowhere visible to the lower-level routines.  In particular,\n`~astropy.wcs.Wcsprm.set` resets `~astropy.wcs.Wcsprm.cdelt` to unity\nif ``CDi_ja`` is present (and no ``PCi_ja``).  If no ``CROTAia`` is\nassociated with the latitude axis, `~astropy.wcs.Wcsprm.set` reverts\nto a unity ``PCi_ja`` matrix.\n\"\"\"\n\nphi0 = \"\"\"\n``double`` The native latitude of the fiducial point.\n\nThe point whose celestial coordinates are given in ``ref[1:2]``.  If\nundefined (NaN) the initialization routine, `~astropy.wcs.Wcsprm.set`,\nwill set this to a projection-specific default.\n\nSee also\n--------\nastropy.wcs.Wcsprm.theta0\n\"\"\"\n\npix2foc = \"\"\"\npix2foc(*pixcrd, origin*) -> double array[ncoord][nelem]\n\nPerform both `SIP`_ polynomial and `distortion paper`_ lookup-table\ncorrection in parallel.", "metadata": {"file_name": "astropy/wcs/docstrings.py", "File Name": "astropy/wcs/docstrings.py"}}, {"code": "bounds_check = \"\"\"\nbounds_check(pix2world, world2pix)\n\nEnable/disable bounds checking.\n\nParameters\n----------\npix2world : bool, optional\n    When `True`, enable bounds checking for the pixel-to-world (p2x)\n    transformations.  Default is `True`.\n\nworld2pix : bool, optional\n    When `True`, enable bounds checking for the world-to-pixel (s2x)\n    transformations.  Default is `True`.\n\nNotes\n-----\nNote that by default (without calling `bounds_check`) strict bounds\nchecking is enabled.\n\"\"\"\n\nbp = \"\"\"\n``double array[bp_order+1][bp_order+1]`` Focal plane to pixel\ntransformation matrix.\n\nThe `SIP`_ ``BP_i_j`` matrix used for focal plane to pixel\ntransformation.  Its values may be changed in place, but it may not be\nresized, without creating a new `~astropy.wcs.Sip` object.\n\"\"\"\n\nbp_order = \"\"\"\n``int`` (read-only) Order of the polynomial (``BP_ORDER``).\n\"\"\"\n\ncd = \"\"\"\n``double array[naxis][naxis]`` The ``CDi_ja`` linear transformation\nmatrix.\n\nFor historical compatibility, three alternate specifications of the\nlinear transformations are available in wcslib.  The canonical\n``PCi_ja`` with ``CDELTia``, ``CDi_ja``, and the deprecated\n``CROTAia`` keywords.  Although the latter may not formally co-exist\nwith ``PCi_ja``, the approach here is simply to ignore them if given\nin conjunction with ``PCi_ja``.\n\n`~astropy.wcs.Wcsprm.has_pc`, `~astropy.wcs.Wcsprm.has_cd` and\n`~astropy.wcs.Wcsprm.has_crota` can be used to determine which of\nthese alternatives are present in the header.\n\nThese alternate specifications of the linear transformation matrix are\ntranslated immediately to ``PCi_ja`` by `~astropy.wcs.Wcsprm.set` and\nare nowhere visible to the lower-level routines.  In particular,\n`~astropy.wcs.Wcsprm.set` resets `~astropy.wcs.Wcsprm.cdelt` to unity\nif ``CDi_ja`` is present (and no ``PCi_ja``).", "metadata": {"file_name": "astropy/wcs/docstrings.py", "File Name": "astropy/wcs/docstrings.py"}}, {"code": "# Licensed under a 3-clause BSD style license - see LICENSE.rst\n\nimport numpy as np\n\nfrom .. import units as u\n\nfrom .wcs import WCS, WCSSUB_LONGITUDE, WCSSUB_LATITUDE\n\n__doctest_skip__ = ['wcs_to_celestial_frame', 'celestial_frame_to_wcs']\n\n__all__ = ['add_stokes_axis_to_wcs', 'celestial_frame_to_wcs',\n           'wcs_to_celestial_frame', 'proj_plane_pixel_scales',\n           'proj_plane_pixel_area', 'is_proj_plane_distorted',\n           'non_celestial_pixel_scales', 'skycoord_to_pixel',\n           'pixel_to_skycoord', 'custom_wcs_to_frame_mappings',\n           'custom_frame_to_wcs_mappings']\n\n\ndef add_stokes_axis_to_wcs(wcs, add_before_ind):\n    \"\"\"\n    Add a new Stokes axis that is uncorrelated with any other axes.\n\n    Parameters\n    ----------\n    wcs : `~astropy.wcs.WCS`\n        The WCS to add to\n    add_before_ind : int\n        Index of the WCS to insert the new Stokes axis in front of.\n        To add at the end, do add_before_ind = wcs.wcs.naxis\n        The beginning is at position 0.\n\n    Returns\n    -------\n    A new `~astropy.wcs.WCS` instance with an additional axis\n    \"\"\"\n\n    inds = [i + 1 for i in range(wcs.wcs.naxis)]\n    inds.insert(add_before_ind, 0)\n    newwcs = wcs.sub(inds)\n    newwcs.wcs.ctype[add_before_ind] = 'STOKES'\n    newwcs.wcs.cname[add_before_ind] = 'STOKES'\n    return newwcs\n\n\ndef _wcs_to_celestial_frame_builtin(wcs):\n\n    # Import astropy.", "metadata": {"file_name": "astropy/wcs/utils.py", "File Name": "astropy/wcs/utils.py", "Classes": "custom_wcs_to_frame_mappings, custom_frame_to_wcs_mappings", "Functions": "add_stokes_axis_to_wcs, _wcs_to_celestial_frame_builtin, _celestial_frame_to_wcs_builtin, wcs_to_celestial_frame, celestial_frame_to_wcs, proj_plane_pixel_scales, proj_plane_pixel_area, is_proj_plane_distorted, _is_cd_orthogonal, non_celestial_pixel_scales, _has_distortion, skycoord_to_pixel, pixel_to_skycoord"}}, {"code": "See\n      :ref:`relaxread` for details.\n\nnaxis : int, optional\n    The number of world coordinates axes for the object.  (*naxis* may\n    only be provided if *header* is `None`.)\n\nkeysel : sequence of flag bits, optional\n    Vector of flag bits that may be used to restrict the keyword types\n    considered:\n\n        - ``WCSHDR_IMGHEAD``: Image header keywords.\n\n        - ``WCSHDR_BIMGARR``: Binary table image array.\n\n        - ``WCSHDR_PIXLIST``: Pixel list keywords.\n\n    If zero, there is no restriction.  If -1, the underlying wcslib\n    function ``wcspih()`` is called, rather than ``wcstbh()``.\n\ncolsel : sequence of int\n    A sequence of table column numbers used to restrict the keywords\n    considered.  `None` indicates no restriction.\n\nRaises\n------\nMemoryError\n     Memory allocation failed.\n\nValueError\n     Invalid key.\n\nKeyError\n     Key not found in FITS header.\n\"\"\"\n\nWtbarr = \"\"\"\nClasses to construct coordinate lookup tables from a binary table\nextension (BINTABLE).\n\nThis class can not be constructed directly from Python, but instead is\nreturned from `~astropy.wcs.Wcsprm.wtb`.\n\"\"\"\n\nzsource = \"\"\"\n``double`` The redshift, ``ZSOURCEa``, of the source.\n\nAn undefined value is represented by NaN.\n\"\"\"\n\nWcsError = \"\"\"\nBase class of all invalid WCS errors.\n\"\"\"\n\nSingularMatrix = \"\"\"\nSingularMatrixError()\n\nThe linear transformation matrix is singular.\n\"\"\"\n\nInconsistentAxisTypes = \"\"\"\nInconsistentAxisTypesError()\n\nThe WCS header inconsistent or unrecognized coordinate axis type(s).\n\"\"\"\n\nInvalidTransform = \"\"\"\nInvalidTransformError()\n\nThe WCS transformation is invalid, or the transformation parameters\nare invalid.\n\"\"\"\n\nInvalidCoordinate = \"\"\"\nInvalidCoordinateError()\n\nOne or more of the world coordinates is invalid.\n\"\"\"\n\nNoSolution = \"\"\"\nNoSolutionError()\n\nNo solution can be found in the given interval.\n\"\"\"\n\nInvalidSubimageSpecification = \"\"\"\nInvalidSubimageSpecificationError()\n\nThe subimage specification is invalid.\n\"\"\"\n\nNonseparableSubimageCoordinateSystem = \"\"\"\nNonseparableSubimageCoordinateSystemError()\n\nNon-separable subimage coordinate system.\n\"\"\"\n\nNoWcsKeywordsFound = \"\"\"\nNoWcsKeywordsFoundError()\n\nNo WCS keywords were found in the given header.\n\"\"\"", "metadata": {"file_name": "astropy/wcs/docstrings.py", "File Name": "astropy/wcs/docstrings.py"}}, {"code": "If -1, `wcspih` is called,\n    rather than `wcstbh`.\n\nReturns\n-------\nwcs_list : list of `~astropy.wcs.Wcsprm` objects\n\"\"\"\n\nfix = \"\"\"\nfix(translate_units='', naxis=0)\n\nApplies all of the corrections handled separately by\n`~astropy.wcs.Wcsprm.datfix`, `~astropy.wcs.Wcsprm.unitfix`,\n`~astropy.wcs.Wcsprm.celfix`, `~astropy.wcs.Wcsprm.spcfix`,\n`~astropy.wcs.Wcsprm.cylfix` and `~astropy.wcs.Wcsprm.cdfix`.\n\nParameters\n----------\n\ntranslate_units : str, optional\n    Specify which potentially unsafe translations of non-standard unit\n    strings to perform.  By default, performs all.\n\n    Although ``\"S\"`` is commonly used to represent seconds, its\n    translation to ``\"s\"`` is potentially unsafe since the standard\n    recognizes ``\"S\"`` formally as Siemens, however rarely that may be\n    used.  The same applies to ``\"H\"`` for hours (Henry), and ``\"D\"``\n    for days (Debye).\n\n    This string controls what to do in such cases, and is\n    case-insensitive.\n\n    - If the string contains ``\"s\"``, translate ``\"S\"`` to ``\"s\"``.\n\n    - If the string contains ``\"h\"``, translate ``\"H\"`` to ``\"h\"``.\n\n    - If the string contains ``\"d\"``, translate ``\"D\"`` to ``\"d\"``.\n\n    Thus ``''`` doesn't do any unsafe translations, whereas ``'shd'``\n    does all of them.\n\nnaxis : int array[naxis], optional\n    Image axis lengths.  If this array is set to zero or ``None``,\n    then `~astropy.wcs.Wcsprm.cylfix` will not be invoked.", "metadata": {"file_name": "astropy/wcs/docstrings.py", "File Name": "astropy/wcs/docstrings.py"}}, {"code": "Even when ``detect_divergence``\n            is `False`, :py:meth:`all_world2pix`, at the end of the\n            iterative process, will identify invalid results\n            (``NaN`` or ``Inf``) as \"diverging\" solutions and will\n            raise :py:class:`NoConvergence` unless the ``quiet``\n            parameter is set to `True`.\n\n            When ``detect_divergence`` is `True`,\n            :py:meth:`all_world2pix` will detect points for which\n            current correction to the coordinates is larger than\n            the correction applied during the previous iteration\n            **if** the requested accuracy **has not yet been\n            achieved**. In this case, if ``adaptive`` is `True`,\n            these points will be excluded from further iterations and\n            if ``adaptive`` is `False`, :py:meth:`all_world2pix` will\n            automatically switch to the adaptive algorithm. Thus, the\n            reported divergent solution will be the latest converging\n            solution computed immediately *before* divergence\n            has been detected.\n\n            .. note::\n               When accuracy has been achieved, small increases in\n               current corrections may be possible due to rounding\n               errors (when ``adaptive`` is `False`) and such\n               increases will be ignored.\n\n            .. note::\n               Based on our testing using HST ACS/WFC images, setting\n               ``detect_divergence`` to `True` will incur about 5-20%\n               performance penalty with the larger penalty\n               corresponding to ``adaptive`` set to `True`.\n               Because the benefits of enabling this\n               feature outweigh the small performance penalty,\n               especially when ``adaptive`` = `False`, it is\n               recommended to set ``detect_divergence`` to `True`,\n               unless extensive testing of the distortion models for\n               images from specific instruments show a good stability\n               of the numerical method for a wide range of\n               coordinates (even outside the image itself).", "metadata": {"file_name": "astropy/wcs/wcs.py", "File Name": "astropy/wcs/wcs.py", "Classes": "NoConvergence, FITSFixedWarning, WCS, _WcsValidateWcsResult, _WcsValidateHduResult, _WcsValidateResults", "Functions": "_parse_keysel, __WCS_unpickle__, find_all_wcs, validate, write_d2i, write_dist, write_array, _return_list_of_arrays, _return_single_array"}}, {"code": "Returns\n    -------\n    coords : Whatever ``cls`` is (a subclass of `~astropy.coordinates.SkyCoord`)\n        The celestial coordinates\n\n    See Also\n    --------\n    astropy.coordinates.SkyCoord.from_pixel\n    \"\"\"\n\n    # Import astropy.coordinates here to avoid circular imports\n    from ..coordinates import SkyCoord, UnitSphericalRepresentation\n\n    # we have to do this instead of actually setting the default to SkyCoord\n    # because importing SkyCoord at the module-level leads to circular\n    # dependencies.", "metadata": {"file_name": "astropy/wcs/utils.py", "File Name": "astropy/wcs/utils.py", "Classes": "custom_wcs_to_frame_mappings, custom_frame_to_wcs_mappings", "Functions": "add_stokes_axis_to_wcs, _wcs_to_celestial_frame_builtin, _celestial_frame_to_wcs_builtin, wcs_to_celestial_frame, celestial_frame_to_wcs, proj_plane_pixel_scales, proj_plane_pixel_area, is_proj_plane_distorted, _is_cd_orthogonal, non_celestial_pixel_scales, _has_distortion, skycoord_to_pixel, pixel_to_skycoord"}}, {"code": "Parameters\n----------\npixcrd : double array[ncoord][nelem]\n    Array of pixel coordinates.\n\n{0}\n\nReturns\n-------\nfoccrd : double array[ncoord][nelem]\n    Returns an array of focal plane coordinates.\n\nRaises\n------\nMemoryError\n    Memory allocation failed.\n\nValueError\n    Invalid coordinate transformation parameters.\n\"\"\".format(__.ORIGIN())\n\npiximg_matrix = \"\"\"\n``double array[2][2]`` (read-only) Matrix containing the product of\nthe ``CDELTia`` diagonal matrix and the ``PCi_ja`` matrix.\n\"\"\"\n\nprint_contents = \"\"\"\nprint_contents()\n\nPrint the contents of the `~astropy.wcs.Wcsprm` object to stdout.\nProbably only useful for debugging purposes, and may be removed in the\nfuture.\n\nTo get a string of the contents, use `repr`.\n\"\"\"\n\nprint_contents_tabprm = \"\"\"\nprint_contents()\n\nPrint the contents of the `~astropy.wcs.Tabprm` object to\nstdout.  Probably only useful for debugging purposes, and may be\nremoved in the future.\n\nTo get a string of the contents, use `repr`.\n\"\"\"\n\nradesys = \"\"\"\n``string`` The equatorial or ecliptic coordinate system type,\n``RADESYSa``.\n\"\"\"\n\nrestfrq = \"\"\"\n``double`` Rest frequency (Hz) from ``RESTFRQa``.\n\nAn undefined value is represented by NaN.\n\"\"\"\n\nrestwav = \"\"\"\n``double`` Rest wavelength (m) from ``RESTWAVa``.\n\nAn undefined value is represented by NaN.\n\"\"\"\n\nrow = \"\"\"\n``int`` (read-only)\n\nTable row number.\n\"\"\"\n\ns2p = \"\"\"\ns2p(world, origin)\n\nTransforms world coordinates to pixel coordinates.\n\nParameters\n----------\nworld : double array[ncoord][nelem]\n    Array of world coordinates, in decimal degrees.\n\n{0}\n\nReturns\n-------\nresult : dict\n    Returns a dictionary with the following keys:\n\n    - *phi*: double array[ncoord]\n\n    - *theta*: double array[ncoord]\n\n        - Longitude and latitude in the native coordinate system of\n          the projection, in degrees.\n\n    - *imgcrd*: double array[ncoord][nelem]\n\n       - Array of intermediate world coordinates.", "metadata": {"file_name": "astropy/wcs/docstrings.py", "File Name": "astropy/wcs/docstrings.py"}}, {"code": "naxis : int or sequence, optional\n        Extracts specific coordinate axes using\n        :meth:`~astropy.wcs.Wcsprm.sub`.  If a header is provided, and\n        *naxis* is not ``None``, *naxis* will be passed to\n        :meth:`~astropy.wcs.Wcsprm.sub` in order to select specific\n        axes from the header.  See :meth:`~astropy.wcs.Wcsprm.sub` for\n        more details about this parameter.\n\n    keysel : sequence of flags, optional\n        A sequence of flags used to select the keyword types\n        considered by wcslib.  When ``None``, only the standard image\n        header keywords are considered (and the underlying wcspih() C\n        function is called).  To use binary table image array or pixel\n        list keywords, *keysel* must be set.\n\n        Each element in the list should be one of the following\n        strings:\n\n        - 'image': Image header keywords\n\n        - 'binary': Binary table image array keywords\n\n        - 'pixel': Pixel list keywords\n\n        Keywords such as ``EQUIna`` or ``RFRQna`` that are common to\n        binary table image arrays and pixel lists (including\n        ``WCSNna`` and ``TWCSna``) are selected by both 'binary' and\n        'pixel'.\n\n    colsel : sequence of int, optional\n        A sequence of table column numbers used to restrict the WCS\n        transformations considered to only those pertaining to the\n        specified columns.  If `None`, there is no restriction.\n\n    fix : bool, optional\n        When `True` (default), call `~astropy.wcs.Wcsprm.fix` on\n        the resulting object to fix any non-standard uses in the\n        header.  `FITSFixedWarning` Warnings will be emitted if any\n        changes were made.", "metadata": {"file_name": "astropy/wcs/wcs.py", "File Name": "astropy/wcs/wcs.py", "Classes": "NoConvergence, FITSFixedWarning, WCS, _WcsValidateWcsResult, _WcsValidateHduResult, _WcsValidateResults", "Functions": "_parse_keysel, __WCS_unpickle__, find_all_wcs, validate, write_d2i, write_dist, write_array, _return_list_of_arrays, _return_single_array"}}, {"code": "_add_sip_to_ctype = \"\"\"\n        Inconsistent SIP distortion information is present in the current WCS:\n        SIP coefficients were detected, but CTYPE is missing \"-SIP\" suffix,\n        therefore the current WCS is internally inconsistent.\n\n        Because relax has been set to True, the resulting output WCS will have\n        \"-SIP\" appended to CTYPE in order to make the header internally consistent.\n\n        However, this may produce incorrect astrometry in the output WCS, if\n        in fact the current WCS is already distortion-corrected.\n\n        Therefore, if current WCS is already distortion-corrected (eg, drizzled)\n        then SIP distortion components should not apply. In that case, for a WCS\n        that is already distortion-corrected, please remove the SIP coefficients\n        from the header.\n\n        \"\"\"\n        if log_message:\n            if add_sip:\n                log.info(_add_sip_to_ctype)\n        for i in range(1, self.naxis+1):\n            # strip() must be called here to cover the case of alt key= \" \"\n            kw = 'CTYPE{0}{1}'.format(i, self.wcs.alt).strip()\n            if kw in header:\n                if add_sip:\n                    val = header[kw].strip(\"-SIP\") + \"-SIP\"\n                else:\n                    val = header[kw].strip(\"-SIP\")\n                header[kw] = val\n            else:\n                continue\n        return header\n\n    def to_header_string(self, relax=None):\n        \"\"\"\n        Identical to `to_header`, but returns a string containing the\n        header cards.\n        \"\"\"\n        return str(self.to_header(relax))\n\n    def footprint_to_file(self, filename='footprint.reg', color='green',\n                          width=2, coordsys=None):\n        \"\"\"\n        Writes out a `ds9`_ style regions file. It can be loaded\n        directly by `ds9`_.", "metadata": {"file_name": "astropy/wcs/wcs.py", "File Name": "astropy/wcs/wcs.py", "Classes": "NoConvergence, FITSFixedWarning, WCS, _WcsValidateWcsResult, _WcsValidateHduResult, _WcsValidateResults", "Functions": "_parse_keysel, __WCS_unpickle__, find_all_wcs, validate, write_d2i, write_dist, write_array, _return_list_of_arrays, _return_single_array"}}, {"code": "See also\n--------\nastropy.wcs.Wcsprm.mjdavg\n\"\"\"\n\nname = \"\"\"\n``string`` The name given to the coordinate representation\n``WCSNAMEa``.\n\"\"\"\n\nnaxis = \"\"\"\n``int`` (read-only) The number of axes (pixel and coordinate).\n\nGiven by the ``NAXIS`` or ``WCSAXESa`` keyvalues.\n\nThe number of coordinate axes is determined at parsing time, and can\nnot be subsequently changed.\n\nIt is determined from the highest of the following:\n\n  1. ``NAXIS``\n\n  2. ``WCSAXESa``\n\n  3. The highest axis number in any parameterized WCS keyword.  The\n     keyvalue, as well as the keyword, must be syntactically valid\n     otherwise it will not be considered.\n\nIf none of these keyword types is present, i.e. if the header only\ncontains auxiliary WCS keywords for a particular coordinate\nrepresentation, then no coordinate description is constructed for it.\n\nThis value may differ for different coordinate representations of the\nsame image.\n\"\"\"\n\nnc = \"\"\"\n``int`` (read-only) Total number of coord vectors in the coord array.\n\nTotal number of coordinate vectors in the coordinate array being the\nproduct K_1 * K_2 * ... * K_M.\n\"\"\"\n\nndim = \"\"\"\n``int`` (read-only)\n\nExpected dimensionality of the wcstab array.\n\"\"\"\n\nobsgeo = \"\"\"\n``double array[3]`` Location of the observer in a standard terrestrial\nreference frame.\n\n``OBSGEO-X``, ``OBSGEO-Y``, ``OBSGEO-Z`` (in meters).\n\nAn undefined value is represented by NaN.\n\"\"\"\n\np0 = \"\"\"\n``int array[M]`` Interpolated indices into the coordinate array.\n\nVector of length `~astropy.wcs.Tabprm.M` of interpolated\nindices into the coordinate array such that Upsilon_m, as defined in\nPaper III, is equal to ``(p0[m] + 1) + delta[m]``.\n\"\"\"\n\np2s = \"\"\"\np2s(pixcrd, origin)\n\nConverts pixel to world coordinates.\n\nParameters\n----------\n\npixcrd : double array[ncoord][nelem]\n    Array of pixel coordinates.", "metadata": {"file_name": "astropy/wcs/docstrings.py", "File Name": "astropy/wcs/docstrings.py"}}, {"code": "imgpix_matrix = \"\"\"\n``double array[2][2]`` (read-only) Inverse of the ``CDELT`` or ``PC``\nmatrix.\n\nInverse containing the product of the ``CDELTia`` diagonal matrix and\nthe ``PCi_ja`` matrix.\n\"\"\"\n\nis_unity = \"\"\"\nis_unity() -> bool\n\nReturns `True` if the linear transformation matrix\n(`~astropy.wcs.Wcsprm.cd`) is unity.\n\"\"\"\n\nK = \"\"\"\n``int array[M]`` (read-only) The lengths of the axes of the coordinate\narray.\n\nAn array of length `M` whose elements record the lengths of the axes of\nthe coordinate array and of each indexing vector.\n\"\"\"\n\nkind = \"\"\"\n``str`` (read-only)\n\nCharacter identifying the wcstab array type:\n\n    - ``'c'``: coordinate array,\n    - ``'i'``: index vector.\n\"\"\"\n\nlat = \"\"\"\n``int`` (read-only) The index into the world coord array containing\nlatitude values.\n\"\"\"\n\nlatpole = \"\"\"\n``double`` The native latitude of the celestial pole, ``LATPOLEa`` (deg).\n\"\"\"\n\nlattyp = \"\"\"\n``string`` (read-only) Celestial axis type for latitude.\n\nFor example, \"RA\", \"DEC\", \"GLON\", \"GLAT\", etc. extracted from \"RA--\",\n\"DEC-\", \"GLON\", \"GLAT\", etc. in the first four characters of\n``CTYPEia`` but with trailing dashes removed.\n\"\"\"\n\nlng = \"\"\"\n``int`` (read-only) The index into the world coord array containing\nlongitude values.\n\"\"\"\n\nlngtyp = \"\"\"\n``string`` (read-only) Celestial axis type for longitude.\n\nFor example, \"RA\", \"DEC\", \"GLON\", \"GLAT\", etc. extracted from \"RA--\",\n\"DEC-\", \"GLON\", \"GLAT\", etc. in the first four characters of\n``CTYPEia`` but with trailing dashes removed.\n\"\"\"\n\nlonpole = \"\"\"\n``double`` The native longitude of the celestial pole.\n\n``LONPOLEa`` (deg).\n\"\"\"\n\nM = \"\"\"\n``int`` (read-only) Number of tabular coordinate axes.\n\"\"\"\n\nm = \"\"\"\n``int`` (read-only)\n\nArray axis number for index vectors.\n\"\"\"\n\nmap = \"\"\"\n``int array[M]`` Association between axes.", "metadata": {"file_name": "astropy/wcs/docstrings.py", "File Name": "astropy/wcs/docstrings.py"}}, {"code": "ValueError\n    Invalid coordinate transformation parameters.\n\"\"\".format(__.ORIGIN())\n\nsip_pix2foc = \"\"\"\nsip_pix2foc(*pixcrd, origin*) -> double array[ncoord][nelem]\n\nConvert pixel coordinates to focal plane coordinates using the `SIP`_\npolynomial distortion convention.\n\nParameters\n----------\npixcrd : double array[ncoord][nelem]\n    Array of pixel coordinates.\n\n{0}\n\nReturns\n-------\nfoccrd : double array[ncoord][nelem]\n    Returns an array of focal plane coordinates.\n\nRaises\n------\nMemoryError\n    Memory allocation failed.\n\nValueError\n    Invalid coordinate transformation parameters.\n\"\"\".format(__.ORIGIN())\n\nspcfix = \"\"\"\nspcfix() -> int\n\nTranslates AIPS-convention spectral coordinate types.  {``FREQ``,\n``VELO``, ``FELO``}-{``OBS``, ``HEL``, ``LSR``} (e.g. ``FREQ-LSR``,\n``VELO-OBS``, ``FELO-HEL``)\n\nReturns\n-------\nsuccess : int\n    Returns ``0`` for success; ``-1`` if no change required.\n\"\"\"\n\nspec = \"\"\"\n``int`` (read-only) The index containing the spectral axis values.\n\"\"\"\n\nspecsys = \"\"\"\n``string`` Spectral reference frame (standard of rest), ``SPECSYSa``.\n\nSee also\n--------\nastropy.wcs.Wcsprm.ssysobs, astropy.wcs.Wcsprm.velosys\n\"\"\"\n\nsptr = \"\"\"\nsptr(ctype, i=-1)\n\nTranslates the spectral axis in a WCS object.\n\nFor example, a ``FREQ`` axis may be translated into ``ZOPT-F2W`` and\nvice versa.\n\nParameters\n----------\nctype : str\n    Required spectral ``CTYPEia``, maximum of 8 characters.  The first\n    four characters are required to be given and are never modified.\n    The remaining four, the algorithm code, are completely determined\n    by, and must be consistent with, the first four characters.\n    Wildcarding may be used, i.e.  if the final three characters are\n    specified as ``\\\"???\\\"``, or if just the eighth character is\n    specified as ``\\\"?\\\"``, the correct algorithm code will be\n    substituted and returned.\n\ni : int\n    Index of the spectral axis (0-relative).", "metadata": {"file_name": "astropy/wcs/docstrings.py", "File Name": "astropy/wcs/docstrings.py"}}, {"code": "class custom_wcs_to_frame_mappings:\n    def __init__(self, mappings=[]):\n        if hasattr(mappings, '__call__'):\n            mappings = [mappings]\n        WCS_FRAME_MAPPINGS.append(mappings)\n\n    def __enter__(self):\n        pass\n\n    def __exit__(self, type, value, tb):\n        WCS_FRAME_MAPPINGS.pop()\n\n\n# Backward-compatibility\ncustom_frame_mappings = custom_wcs_to_frame_mappings\n\n\nclass custom_frame_to_wcs_mappings:\n    def __init__(self, mappings=[]):\n        if hasattr(mappings, '__call__'):\n            mappings = [mappings]\n        FRAME_WCS_MAPPINGS.append(mappings)\n\n    def __enter__(self):\n        pass\n\n    def __exit__(self, type, value, tb):\n        FRAME_WCS_MAPPINGS.pop()", "metadata": {"file_name": "astropy/wcs/utils.py", "File Name": "astropy/wcs/utils.py", "Classes": "custom_wcs_to_frame_mappings, custom_frame_to_wcs_mappings", "Functions": "add_stokes_axis_to_wcs, _wcs_to_celestial_frame_builtin, _celestial_frame_to_wcs_builtin, wcs_to_celestial_frame, celestial_frame_to_wcs, proj_plane_pixel_scales, proj_plane_pixel_area, is_proj_plane_distorted, _is_cd_orthogonal, non_celestial_pixel_scales, _has_distortion, skycoord_to_pixel, pixel_to_skycoord"}}, {"code": "Parameters\n        ----------\n        filename : str, optional\n            Output file name - default is ``'footprint.reg'``\n\n        color : str, optional\n            Color to use when plotting the line.\n\n        width : int, optional\n            Width of the region line.\n\n        coordsys : str, optional\n            Coordinate system. If not specified (default), the ``radesys``\n            value is used. For all possible values, see\n            http://ds9.si.edu/doc/ref/region.html#RegionFileFormat\n\n        \"\"\"\n        comments = ('# Region file format: DS9 version 4.0 \\n'\n                    '# global color=green font=\"helvetica 12 bold '\n                    'select=1 highlite=1 edit=1 move=1 delete=1 '\n                    'include=1 fixed=0 source\\n')\n\n        coordsys = coordsys or self.wcs.radesys\n\n        if coordsys not in ('PHYSICAL', 'IMAGE', 'FK4', 'B1950', 'FK5',\n                            'J2000', 'GALACTIC', 'ECLIPTIC', 'ICRS', 'LINEAR',\n                            'AMPLIFIER', 'DETECTOR'):\n            raise ValueError(\"Coordinate system '{}' is not supported. A valid\"\n                             \" one can be given with the 'coordsys' argument.\"", "metadata": {"file_name": "astropy/wcs/wcs.py", "File Name": "astropy/wcs/wcs.py", "Classes": "NoConvergence, FITSFixedWarning, WCS, _WcsValidateWcsResult, _WcsValidateHduResult, _WcsValidateResults", "Functions": "_parse_keysel, __WCS_unpickle__, find_all_wcs, validate, write_d2i, write_dist, write_array, _return_list_of_arrays, _return_single_array"}}, {"code": "where(slowconv & conv)\n                            pix0 = pix0[ind]\n                            dnprev[ind] = dn[ind]\n                            k += 1\n\n                            # Switch to adaptive iterations:\n                            adaptive = True\n                            break\n                    # Save current correction magnitudes for later:\n                    dnprev = dn\n\n                # Apply correction:\n                pix -= dpix\n                k += 1\n\n        # ############################################################\n        # #                  ADAPTIVE ITERATIONS:                   ##\n        # ############################################################\n        if adaptive:\n            if ind is None:\n                ind, = np.where(np.isfinite(pix).all(axis=1))\n                pix0 = pix0[ind]\n\n            # \"Adaptive\" fixed-point iterations:\n            while (ind.shape[0] > 0 and k < maxiter):\n                # Find correction to the previous solution:\n                dpixnew = self.pix2foc(pix[ind], origin) - pix0\n\n                # Compute norm (L2) of the correction:\n                dnnew = np.sum(np.square(dpixnew), axis=1)\n\n                # Bookeeping of corrections:\n                dnprev[ind] = dn[ind].copy()\n                dn[ind] = dnnew\n\n                if detect_divergence:\n                    # Find indices of pixels that are converging:\n                    conv = (dnnew < dnprev[ind])\n                    iconv = np.where(conv)\n                    iiconv = ind[iconv]\n\n                    # Apply correction:\n                    dpixgood = dpixnew[iconv]\n                    pix[iiconv] -= dpixgood\n                    dpix[iiconv] = dpixgood\n\n                    # Find indices of solutions that have not yet\n                    # converged to the requested accuracy\n                    # AND that do not diverge:\n                    subind, = np.", "metadata": {"file_name": "astropy/wcs/wcs.py", "File Name": "astropy/wcs/wcs.py", "Classes": "NoConvergence, FITSFixedWarning, WCS, _WcsValidateWcsResult, _WcsValidateHduResult, _WcsValidateResults", "Functions": "_parse_keysel, __WCS_unpickle__, find_all_wcs, validate, write_d2i, write_dist, write_array, _return_list_of_arrays, _return_single_array"}}, {"code": "\"\"\".format(__.TWO_OR_MORE_ARGS('2', 8),\n                   __.RETURNS('focal coordinates', 8))\n\n    def det2im(self, *args):\n        return self._array_converter(self._det2im, None, *args)\n    det2im.__doc__ = \"\"\"\n        Convert detector coordinates to image plane coordinates using\n        `distortion paper`_ table-lookup correction.\n\n        The output is in absolute pixel coordinates, not relative to\n        ``CRPIX``.\n\n        Parameters\n        ----------\n\n        {0}\n\n        Returns\n        -------\n\n        {1}\n\n        Raises\n        ------\n        MemoryError\n            Memory allocation failed.\n\n        ValueError\n            Invalid coordinate transformation parameters.\n        \"\"\".format(__.TWO_OR_MORE_ARGS('2', 8),\n                   __.RETURNS('pixel coordinates', 8))\n\n    def sip_pix2foc(self, *args):\n        if self.sip is None:\n            if len(args) == 2:\n                return args[0]\n            elif len(args) == 3:\n                return args[:2]\n            else:\n                raise TypeError(\"Wrong number of arguments\")\n        return self._array_converter(self.sip.pix2foc, None, *args)\n    sip_pix2foc.__doc__ = \"\"\"\n        Convert pixel coordinates to focal plane coordinates using the\n        `SIP`_ polynomial distortion convention.\n\n        The output is in pixel coordinates, relative to ``CRPIX``.\n\n        FITS WCS `distortion paper`_ table lookup correction is not\n        applied, even if that information existed in the FITS file\n        that initialized this :class:`~astropy.wcs.WCS` object.  To\n        correct for that, use `~astropy.wcs.WCS.pix2foc` or\n        `~astropy.wcs.WCS.p4_pix2foc`.", "metadata": {"file_name": "astropy/wcs/wcs.py", "File Name": "astropy/wcs/wcs.py", "Classes": "NoConvergence, FITSFixedWarning, WCS, _WcsValidateWcsResult, _WcsValidateHduResult, _WcsValidateResults", "Functions": "_parse_keysel, __WCS_unpickle__, find_all_wcs, validate, write_d2i, write_dist, write_array, _return_list_of_arrays, _return_single_array"}}, {"code": ".. note::\n        This function is concerned **only** about the transformation\n        \"image plane\"->\"projection plane\" and **not** about the\n        transformation \"celestial sphere\"->\"projection plane\"->\"image plane\".\n        Therefore, this function ignores distortions arising due to\n        non-linear nature of most projections.\n\n    .. note::\n        In order to compute the scales corresponding to celestial axes only,\n        make sure that the input `~astropy.wcs.WCS` object contains\n        celestial axes only, e.g., by passing in the\n        `~astropy.wcs.WCS.celestial` WCS object.\n\n    Parameters\n    ----------\n    wcs : `~astropy.wcs.WCS`\n        A world coordinate system object.\n\n    Returns\n    -------\n    scale : `~numpy.ndarray`\n        A vector (`~numpy.ndarray`) of projection plane increments\n        corresponding to each pixel side (axis). The units of the returned\n        results are the same as the units of `~astropy.wcs.Wcsprm.cdelt`,\n        `~astropy.wcs.Wcsprm.crval`, and `~astropy.wcs.Wcsprm.cd` for\n        the celestial WCS and can be obtained by inquiring the value\n        of `~astropy.wcs.Wcsprm.cunit` property of the input\n        `~astropy.wcs.WCS` WCS object.\n\n    See Also\n    --------\n    astropy.wcs.utils.proj_plane_pixel_area\n\n    \"\"\"\n    return np.sqrt((wcs.pixel_scale_matrix**2).sum(axis=0, dtype=float))", "metadata": {"file_name": "astropy/wcs/utils.py", "File Name": "astropy/wcs/utils.py", "Classes": "custom_wcs_to_frame_mappings, custom_frame_to_wcs_mappings", "Functions": "add_stokes_axis_to_wcs, _wcs_to_celestial_frame_builtin, _celestial_frame_to_wcs_builtin, wcs_to_celestial_frame, celestial_frame_to_wcs, proj_plane_pixel_scales, proj_plane_pixel_area, is_proj_plane_distorted, _is_cd_orthogonal, non_celestial_pixel_scales, _has_distortion, skycoord_to_pixel, pixel_to_skycoord"}}, {"code": "Parameters\n        ----------\n\n        {0}\n\n        Returns\n        -------\n\n        {1}\n\n        Raises\n        ------\n        MemoryError\n            Memory allocation failed.\n\n        ValueError\n            Invalid coordinate transformation parameters.\n        \"\"\".format(__.TWO_OR_MORE_ARGS('2', 8),\n                   __.RETURNS('focal coordinates', 8))\n\n    def sip_foc2pix(self, *args):\n        if self.sip is None:\n            if len(args) == 2:\n                return args[0]\n            elif len(args) == 3:\n                return args[:2]\n            else:\n                raise TypeError(\"Wrong number of arguments\")\n        return self._array_converter(self.sip.foc2pix, None, *args)\n    sip_foc2pix.__doc__ = \"\"\"\n        Convert focal plane coordinates to pixel coordinates using the\n        `SIP`_ polynomial distortion convention.\n\n        FITS WCS `distortion paper`_ table lookup distortion\n        correction is not applied, even if that information existed in\n        the FITS file that initialized this `~astropy.wcs.WCS` object.\n\n        Parameters\n        ----------\n\n        {0}\n\n        Returns\n        -------\n\n        {1}\n\n        Raises\n        ------\n        MemoryError\n            Memory allocation failed.\n\n        ValueError\n            Invalid coordinate transformation parameters.\n        \"\"\".format(__.TWO_OR_MORE_ARGS('2', 8),\n                   __.RETURNS('pixel coordinates', 8))\n\n    def to_fits(self, relax=False, key=None):\n        \"\"\"\n        Generate an `astropy.io.fits.HDUList` object with all of the\n        information stored in this object.  This should be logically identical\n        to the input FITS file, but it will be normalized in a number of ways.\n\n        See `to_header` for some warnings about the output produced.", "metadata": {"file_name": "astropy/wcs/wcs.py", "File Name": "astropy/wcs/wcs.py", "Classes": "NoConvergence, FITSFixedWarning, WCS, _WcsValidateWcsResult, _WcsValidateHduResult, _WcsValidateResults", "Functions": "_parse_keysel, __WCS_unpickle__, find_all_wcs, validate, write_d2i, write_dist, write_array, _return_list_of_arrays, _return_single_array"}}, {"code": "class FITSFixedWarning(AstropyWarning):\n    \"\"\"\n    The warning raised when the contents of the FITS header have been\n    modified to be standards compliant.\n    \"\"\"\n    pass\n\n\nclass WCS(WCSBase):\n    \"\"\"WCS objects perform standard WCS transformations, and correct for\n    `SIP`_ and `distortion paper`_ table-lookup transformations, based\n    on the WCS keywords and supplementary data read from a FITS file.\n\n    Parameters\n    ----------\n    header : astropy.io.fits header object, Primary HDU, Image HDU, string, dict-like, or None, optional\n        If *header* is not provided or None, the object will be\n        initialized to default values.\n\n    fobj : An astropy.io.fits file (hdulist) object, optional\n        It is needed when header keywords point to a `distortion\n        paper`_ lookup table stored in a different extension.\n\n    key : str, optional\n        The name of a particular WCS transform to use.  This may be\n        either ``' '`` or ``'A'``-``'Z'`` and corresponds to the\n        ``\\\"a\\\"`` part of the ``CTYPEia`` cards.  *key* may only be\n        provided if *header* is also provided.\n\n    minerr : float, optional\n        The minimum value a distortion correction must have in order\n        to be applied. If the value of ``CQERRja`` is smaller than\n        *minerr*, the corresponding distortion is not applied.\n\n    relax : bool or int, optional\n        Degree of permissiveness:\n\n        - `True` (default): Admit all recognized informal extensions\n          of the WCS standard.\n\n        - `False`: Recognize only FITS keywords defined by the\n          published WCS standard.\n\n        - `int`: a bit field selecting specific extensions to accept.\n          See :ref:`relaxread` for details.", "metadata": {"file_name": "astropy/wcs/wcs.py", "File Name": "astropy/wcs/wcs.py", "Classes": "NoConvergence, FITSFixedWarning, WCS, _WcsValidateWcsResult, _WcsValidateHduResult, _WcsValidateResults", "Functions": "_parse_keysel, __WCS_unpickle__, find_all_wcs, validate, write_d2i, write_dist, write_array, _return_list_of_arrays, _return_single_array"}}, {"code": "As ``CUNITia`` is an optional header keyword,\n`~astropy.wcs.Wcsprm.cunit` may be left blank but otherwise is\nexpected to contain a standard units specification as defined by WCS\nPaper I.  `~astropy.wcs.Wcsprm.unitfix` is available to translate\ncommonly used non-standard units specifications but this must be done\nas a separate step before invoking `~astropy.wcs.Wcsprm.set`.\n\nFor celestial axes, if `~astropy.wcs.Wcsprm.cunit` is not blank,\n`~astropy.wcs.Wcsprm.set` uses ``wcsunits`` to parse it and scale\n`~astropy.wcs.Wcsprm.cdelt`, `~astropy.wcs.Wcsprm.crval`, and\n`~astropy.wcs.Wcsprm.cd` to decimal degrees.  It then resets\n`~astropy.wcs.Wcsprm.cunit` to ``\"deg\"``.\n\nFor spectral axes, if `~astropy.wcs.Wcsprm.cunit` is not blank,\n`~astropy.wcs.Wcsprm.set` uses ``wcsunits`` to parse it and scale\n`~astropy.wcs.Wcsprm.cdelt`, `~astropy.wcs.Wcsprm.crval`, and\n`~astropy.wcs.Wcsprm.cd` to SI units.  It then resets\n`~astropy.wcs.Wcsprm.cunit` accordingly.\n\n`~astropy.wcs.Wcsprm.set` ignores `~astropy.wcs.Wcsprm.cunit` for\nother coordinate types; `~astropy.wcs.Wcsprm.cunit` may be used to\nlabel coordinate values.\n\"\"\"\n\ncylfix = \"\"\"\ncylfix()\n\nFixes WCS keyvalues for malformed cylindrical projections.\n\nReturns\n-------\nsuccess : int\n    Returns ``0`` for success; ``-1`` if no change required.\n\"\"\"\n\ndata = \"\"\"\n``float array`` The array data for the\n`~astropy.wcs.DistortionLookupTable`.\n\"\"\"\n\ndata_wtbarr = \"\"\"\n``double array``\n\nThe array data for the BINTABLE.\n\"\"\"\n\ndateavg = \"\"\"\n``string`` Representative mid-point of the date of observation.\n\nIn ISO format, ``yyyy-mm-ddThh:mm:ss``.", "metadata": {"file_name": "astropy/wcs/docstrings.py", "File Name": "astropy/wcs/docstrings.py"}}, {"code": "def _has_distortion(wcs):\n    \"\"\"\n    `True` if contains any SIP or image distortion components.\n    \"\"\"\n    return any(getattr(wcs, dist_attr) is not None\n               for dist_attr in ['cpdis1', 'cpdis2', 'det2im1', 'det2im2', 'sip'])\n\n\n# TODO: in future, we should think about how the following two functions can be\n# integrated better into the WCS class.\n\ndef skycoord_to_pixel(coords, wcs, origin=0, mode='all'):\n    \"\"\"\n    Convert a set of SkyCoord coordinates into pixels.\n\n    Parameters\n    ----------\n    coords : `~astropy.coordinates.SkyCoord`\n        The coordinates to convert.\n    wcs : `~astropy.wcs.WCS`\n        The WCS transformation to use.\n    origin : int\n        Whether to return 0 or 1-based pixel coordinates.\n    mode : 'all' or 'wcs'\n        Whether to do the transformation including distortions (``'all'``) or\n        only including only the core WCS transformation (``'wcs'``).", "metadata": {"file_name": "astropy/wcs/utils.py", "File Name": "astropy/wcs/utils.py", "Classes": "custom_wcs_to_frame_mappings, custom_frame_to_wcs_mappings", "Functions": "add_stokes_axis_to_wcs, _wcs_to_celestial_frame_builtin, _celestial_frame_to_wcs_builtin, wcs_to_celestial_frame, celestial_frame_to_wcs, proj_plane_pixel_scales, proj_plane_pixel_area, is_proj_plane_distorted, _is_cd_orthogonal, non_celestial_pixel_scales, _has_distortion, skycoord_to_pixel, pixel_to_skycoord"}}, {"code": "# Licensed under a 3-clause BSD style license - see LICENSE.rst\n\"\"\"\n.. _wcslib: http://www.atnf.csiro.au/people/mcalabre/WCS/wcslib/index.html\n.. _distortion paper: http://www.atnf.csiro.au/people/mcalabre/WCS/dcs_20040422.pdf\n.. _SIP: http://irsa.ipac.caltech.edu/data/SPITZER/docs/files/spitzer/shupeADASS.pdf\n.. _FITS WCS standard: https://fits.gsfc.nasa.gov/fits_wcs.html\n\n`astropy.wcs` contains utilities for managing World Coordinate System\n(WCS) transformations in FITS files.  These transformations map the\npixel locations in an image to their real-world units, such as their\nposition on the sky sphere.\n\nIt performs three separate classes of WCS transformations:\n\n- Core WCS, as defined in the `FITS WCS standard`_, based on Mark\n  Calabretta's `wcslib`_.  See `~astropy.wcs.Wcsprm`.\n- Simple Imaging Polynomial (`SIP`_) convention.  See\n  `~astropy.wcs.Sip`.\n- table lookup distortions as defined in WCS `distortion paper`_.  See\n  `~astropy.wcs.DistortionLookupTable`.\n\nEach of these transformations can be used independently or together in\na standard pipeline.\n\"\"\"\n\n\ntry:\n    # Not guaranteed available at setup time\n    from .wcs import *\n    from . import utils\nexcept ImportError:\n    if not _ASTROPY_SETUP_:\n        raise\n\n\ndef get_include():\n    \"\"\"\n    Get the path to astropy.wcs's C header files.\n    \"\"\"\n    import os\n    return os.path.join(os.path.dirname(__file__), \"include\")", "metadata": {"file_name": "astropy/wcs/__init__.py", "File Name": "astropy/wcs/__init__.py", "Functions": "get_include"}}, {"code": "{0}\n\nReturns\n-------\nresult : dict\n    Returns a dictionary with the following keys:\n\n    - *imgcrd*: double array[ncoord][nelem]\n\n      - Array of intermediate world coordinates.  For celestial axes,\n        ``imgcrd[][self.lng]`` and ``imgcrd[][self.lat]`` are the\n        projected *x*-, and *y*-coordinates, in pseudo degrees.  For\n        spectral axes, ``imgcrd[][self.spec]`` is the intermediate\n        spectral coordinate, in SI units.\n\n    - *phi*: double array[ncoord]\n\n    - *theta*: double array[ncoord]\n\n      - Longitude and latitude in the native coordinate system of the\n        projection, in degrees.\n\n    - *world*: double array[ncoord][nelem]\n\n      - Array of world coordinates.  For celestial axes,\n        ``world[][self.lng]`` and ``world[][self.lat]`` are the\n        celestial longitude and latitude, in degrees.  For spectral\n        axes, ``world[][self.spec]`` is the intermediate spectral\n        coordinate, in SI units.\n\n    - *stat*: int array[ncoord]\n\n      - Status return value for each coordinate. ``0`` for success,\n        ``1+`` for invalid pixel coordinate.\n\nRaises\n------\n\nMemoryError\n    Memory allocation failed.\n\nSingularMatrixError\n    Linear transformation matrix is singular.\n\nInconsistentAxisTypesError\n    Inconsistent or unrecognized coordinate axis types.\n\nValueError\n    Invalid parameter value.\n\nValueError\n    *x*- and *y*-coordinate arrays are not the same size.\n\nInvalidTransformError\n    Invalid coordinate transformation parameters.\n\nInvalidTransformError\n    Ill-conditioned coordinate transformation parameters.\n\nSee also\n--------\nastropy.wcs.Wcsprm.lat, astropy.wcs.Wcsprm.lng\n    Definition of the latitude and longitude axes\n\"\"\".format(__.ORIGIN())\n\np4_pix2foc = \"\"\"\np4_pix2foc(*pixcrd, origin*) -> double array[ncoord][nelem]\n\nConvert pixel coordinates to focal plane coordinates using `distortion\npaper`_ lookup-table correction.\n\nParameters\n----------\npixcrd : double array[ncoord][nelem].\n    Array of pixel coordinates.\n\n{0}\n\nReturns\n-------\nfoccrd : double array[ncoord][nelem]\n    Returns an array of focal plane coordinates.", "metadata": {"file_name": "astropy/wcs/docstrings.py", "File Name": "astropy/wcs/docstrings.py"}}, {"code": "compare = \"\"\"\ncompare(other, cmp=0, tolerance=0.0)\n\nCompare two Wcsprm objects for equality.\n\nParameters\n----------\n\nother : Wcsprm\n    The other Wcsprm object to compare to.\n\ncmp : int, optional\n    A bit field controlling the strictness of the comparison.  When 0,\n    (the default), all fields must be identical.\n\n    The following constants may be or'ed together to loosen the\n    comparison.\n\n    - ``WCSCOMPARE_ANCILLARY``: Ignores ancillary keywords that don't\n      change the WCS transformation, such as ``DATE-OBS`` or\n      ``EQUINOX``.\n\n    - ``WCSCOMPARE_TILING``: Ignore integral differences in\n      ``CRPIXja``.  This is the 'tiling' condition, where two WCSes\n      cover different regions of the same map projection and align on\n      the same map grid.\n\n    - ``WCSCOMPARE_CRPIX``: Ignore any differences at all in\n      ``CRPIXja``.  The two WCSes cover different regions of the same\n      map projection but may not align on the same grid map.\n      Overrides ``WCSCOMPARE_TILING``.\n\ntolerance : float, optional\n    The amount of tolerance required.  For example, for a value of\n    1e-6, all floating-point values in the objects must be equal to\n    the first 6 decimal places.  The default value of 0.0 implies\n    exact equality.\n\nReturns\n-------\nequal : bool\n\"\"\"\n\nconvert = \"\"\"\nconvert(array)\n\nPerform the unit conversion on the elements of the given *array*,\nreturning an array of the same shape.\n\"\"\"\n\ncoord = \"\"\"\n``double array[K_M]...[K_2][K_1][M]`` The tabular coordinate array.\n\nHas the dimensions::\n\n    (K_M, ... K_2, K_1, M)\n\n(see `~astropy.wcs.Tabprm.K`) i.e. with the `M` dimension\nvarying fastest so that the `M` elements of a coordinate vector are\nstored contiguously in memory.\n\"\"\"\n\ncopy = \"\"\"\nCreates a deep copy of the WCS object.\n\"\"\"", "metadata": {"file_name": "astropy/wcs/docstrings.py", "File Name": "astropy/wcs/docstrings.py"}}], "traj": {"usage": {"embedding_tokens": 0}}}
{"instance_id": "astropy__astropy-14995", "found_files": ["astropy/utils/masked/core.py", "astropy/nddata/mixins/ndarithmetic.py", "astropy/nddata/nddata.py", "astropy/utils/masked/function_helpers.py", "astropy/nddata/bitmask.py", "astropy/nddata/compat.py", "astropy/nddata/nddata_withmixins.py", "astropy/nddata/decorators.py", "astropy/nddata/nduncertainty.py", "astropy/utils/compat/numpycompat.py", "astropy/nddata/__init__.py", "astropy/utils/masked/__init__.py", "astropy/utils/shapes.py", "astropy/nddata/utils.py"], "node_info": [{"code": "is_float16_result = False\n        if dtype is None:\n            if issubclass(self.dtype.type, (np.integer, np.bool_)):\n                dtype = np.dtype(\"f8\")\n            elif issubclass(self.dtype.type, np.float16):\n                dtype = np.dtype(\"f4\")\n                is_float16_result = out is None\n\n        where = ~self.mask & where\n\n        result = self.sum(\n            axis=axis, dtype=dtype, out=out, keepdims=keepdims, where=where\n        )\n        n = np.add.reduce(where, axis=axis, keepdims=keepdims)\n\n        # catch the case when an axis is fully masked to prevent div by zero:\n        n = np.add.reduce(where, axis=axis, keepdims=keepdims)\n        neq0 = n == 0\n        n += neq0\n        result /= n\n\n        # correct fully-masked slice results to what is expected for 0/0 division\n        result.unmasked[neq0] = np.nan\n\n        if is_float16_result:\n            result = result.astype(self.dtype)\n        return result\n\n    def var(\n        self, axis=None, dtype=None, out=None, ddof=0, keepdims=False, *, where=True\n    ):\n        where_final = ~self.mask & where\n\n        # Simplified implementation based on that in numpy/core/_methods.py\n        n = np.add.reduce(where_final, axis=axis, keepdims=keepdims)[...]\n\n        # Cast bool, unsigned int, and int to float64 by default.\n        if dtype is None and issubclass(self.dtype.type, (np.integer, np.bool_)):\n            dtype = np.dtype(\"f8\")\n        mean = self.mean(axis=axis, dtype=dtype, keepdims=True, where=where)\n\n        x = self - mean\n        x *= x.conjugate()  # Conjugate just returns x if not complex.", "metadata": {"file_name": "astropy/utils/masked/core.py", "File Name": "astropy/utils/masked/core.py", "Classes": "Masked, MaskedInfoBase, MaskedNDArrayInfo, MaskedArraySubclassInfo, MaskedIterator, MaskedNDArray, MaskedRecarray", "Functions": "_comparison_method, _compare, argmin, argmax, argmin, argmax"}}, {"code": "mask = np.logical_or.reduce(\n                                mask, axis=axis, keepdims=keepdims\n                            )\n                        in_masks.append(mask)\n\n                mask = self._combine_masks(in_masks)\n                result_masks = []\n                for os in out_sig:\n                    if os:\n                        # Output has core dimensions.  Assume all those\n                        # get the same mask.\n                        result_mask = np.expand_dims(mask, axis)\n                    else:\n                        result_mask = mask\n                    result_masks.append(result_mask)\n\n                mask = result_masks if len(result_masks) > 1 else result_masks[0]\n\n        elif method == \"__call__\":\n            # Regular ufunc call.\n            # Combine the masks from the input, possibly selecting elements.\n            mask = self._combine_masks(masks, out=out_mask, where=where_unmasked)\n            # If relevant, also mask output elements for which where was masked.\n            if where_mask is not None:\n                mask |= where_mask\n\n        elif method == \"outer\":\n            # Must have two arguments; adjust masks as will be done for data.\n            m0, m1 = masks\n            if m0 is not None and m0.ndim > 0:\n                m0 = m0[(...,) + (np.newaxis,) * np.ndim(unmasked[1])]\n            mask = self._combine_masks((m0, m1), out=out_mask)\n\n        elif method in {\"reduce\", \"accumulate\"}:\n            # Reductions like np.add.reduce (sum).\n            # Treat any masked where as if the input element was masked.\n            mask = self._combine_masks((masks[0], where_mask), copy=False)\n            if mask is not False:\n                # By default, we simply propagate masks, since for\n                # things like np.sum, it makes no sense to do otherwise.\n                # Individual methods need to override as needed.", "metadata": {"file_name": "astropy/utils/masked/core.py", "File Name": "astropy/utils/masked/core.py", "Classes": "Masked, MaskedInfoBase, MaskedNDArrayInfo, MaskedArraySubclassInfo, MaskedIterator, MaskedNDArray, MaskedRecarray", "Functions": "_comparison_method, _compare, argmin, argmax, argmin, argmax"}}, {"code": "class MaskedRecarray(np.recarray, MaskedNDArray, data_cls=np.recarray):\n    # Explicit definition since we need to override some methods.\n\n    def __array_finalize__(self, obj):\n        # recarray.__array_finalize__ does not do super, so we do it\n        # explicitly.\n        super().__array_finalize__(obj)\n        super(np.recarray, self).__array_finalize__(obj)\n\n    # __getattribute__, __setattr__, and field use these somewhat\n    # obscrure ndarray methods.  TODO: override in MaskedNDArray?\n    def getfield(self, dtype, offset=0):\n        for field, info in self.dtype.fields.items():\n            if offset == info[1] and dtype == info[0]:\n                return self[field]\n\n        raise NotImplementedError(\"can only get existing field from structured dtype.\")\n\n    def setfield(self, val, dtype, offset=0):\n        for field, info in self.dtype.fields.items():\n            if offset == info[1] and dtype == info[0]:\n                self[field] = val\n                return\n\n        raise NotImplementedError(\"can only set existing field from structured dtype.\")", "metadata": {"file_name": "astropy/utils/masked/core.py", "File Name": "astropy/utils/masked/core.py", "Classes": "Masked, MaskedInfoBase, MaskedNDArrayInfo, MaskedArraySubclassInfo, MaskedIterator, MaskedNDArray, MaskedRecarray", "Functions": "_comparison_method, _compare, argmin, argmax, argmin, argmax"}}, {"code": "# Licensed under a 3-clause BSD style license - see LICENSE.rst\n# This module implements the Arithmetic mixin to the NDData class.\n\nimport warnings\nfrom copy import deepcopy\n\nimport numpy as np\n\nfrom astropy.nddata.nduncertainty import NDUncertainty\nfrom astropy.units import dimensionless_unscaled\nfrom astropy.utils import format_doc, sharedmethod\nfrom astropy.utils.exceptions import AstropyUserWarning\nfrom astropy.utils.masked import Masked\n\n__all__ = [\"NDArithmeticMixin\"]\n\n# Global so it doesn't pollute the class dict unnecessarily:\n\n# Docstring templates for add, subtract, multiply, divide methods.\n_arit_doc = \"\"\"\n    Performs {name} by evaluating ``self`` {op} ``operand``.\n\n    Parameters\n    ----------\n    operand, operand2 : `NDData`-like instance\n        If ``operand2`` is ``None`` or not given it will perform the operation\n        ``self`` {op} ``operand``.\n        If ``operand2`` is given it will perform ``operand`` {op} ``operand2``.\n        If the method was called on a class rather than on the instance\n        ``operand2`` must be given.\n\n    propagate_uncertainties : `bool` or ``None``, optional\n        If ``None`` the result will have no uncertainty. If ``False`` the\n        result will have a copied version of the first operand that has an\n        uncertainty. If ``True`` the result will have a correctly propagated\n        uncertainty from the uncertainties of the operands but this assumes\n        that the uncertainties are `NDUncertainty`-like. Default is ``True``.\n\n        .. versionchanged:: 1.2\n            This parameter must be given as keyword-parameter. Using it as\n            positional parameter is deprecated.\n            ``None`` was added as valid parameter value.\n\n    handle_mask : callable, ``'first_found'`` or ``None``, optional\n        If ``None`` the result will have no mask. If ``'first_found'`` the\n        result will have a copied version of the first operand that has a\n        mask). If it is a callable then the specified callable must\n        create the results ``mask`` and if necessary provide a copy.", "metadata": {"file_name": "astropy/nddata/mixins/ndarithmetic.py", "File Name": "astropy/nddata/mixins/ndarithmetic.py", "Classes": "NDArithmeticMixin"}}, {"code": "if \"where\" not in kwargs:\n            kwargs[\"where\"] = ~self.mask\n        if initial_func is not None and \"initial\" not in kwargs:\n            kwargs[\"initial\"] = initial_func(self.unmasked)\n        return kwargs\n\n    def trace(self, offset=0, axis1=0, axis2=1, dtype=None, out=None):\n        # Unfortunately, cannot override the call to diagonal inside trace, so\n        # duplicate implementation in numpy/core/src/multiarray/calculation.c.\n        diagonal = self.diagonal(offset=offset, axis1=axis1, axis2=axis2)\n        return diagonal.sum(-1, dtype=dtype, out=out)\n\n    def min(self, axis=None, out=None, **kwargs):\n        return super().min(\n            axis=axis, out=out, **self._reduce_defaults(kwargs, np.nanmax)\n        )\n\n    def max(self, axis=None, out=None, **kwargs):\n        return super().max(\n            axis=axis, out=out, **self._reduce_defaults(kwargs, np.nanmin)\n        )\n\n    def nonzero(self):\n        unmasked_nonzero = self.unmasked.nonzero()\n        if self.ndim >= 1:\n            not_masked = ~self.mask[unmasked_nonzero]\n            return tuple(u[not_masked] for u in unmasked_nonzero)\n        else:\n            return unmasked_nonzero if not self.mask else np.nonzero(0)\n\n    def compress(self, condition, axis=None, out=None):\n        if out is not None:\n            raise NotImplementedError(\"cannot yet give output\")\n        return self._apply(\"compress\", condition, axis=axis)\n\n    def repeat(self, repeats, axis=None):\n        return self._apply(\"repeat\", repeats, axis=axis)\n\n    def choose(self, choices, out=None, mode=\"raise\"):\n        # Let __array_function__ take care since choices can be masked too.", "metadata": {"file_name": "astropy/utils/masked/core.py", "File Name": "astropy/utils/masked/core.py", "Classes": "Masked, MaskedInfoBase, MaskedNDArrayInfo, MaskedArraySubclassInfo, MaskedIterator, MaskedNDArray, MaskedRecarray", "Functions": "_comparison_method, _compare, argmin, argmax, argmin, argmax"}}, {"code": ")\n            elif data.uncertainty is not None:\n                uncertainty = data.uncertainty\n\n            if mask is not None and data.mask is not None:\n                log.info(\"overwriting NDData's current mask with specified mask.\")\n            elif data.mask is not None:\n                mask = data.mask\n\n            if wcs is not None and data.wcs is not None:\n                log.info(\"overwriting NDData's current wcs with specified wcs.\")\n            elif data.wcs is not None:\n                wcs = data.wcs\n\n            if psf is not None and data.psf is not None:\n                log.info(\"Overwriting NDData's current psf with specified psf.\")\n            elif data.psf is not None:\n                psf = data.psf\n\n            if meta is not None and data.meta is not None:\n                log.info(\"overwriting NDData's current meta with specified meta.\")\n            elif data.meta is not None:\n                meta = data.meta\n\n            # get the data attribute as it is, and continue to process it:\n            data = data.data\n\n        # if the data is wrapped by astropy.utils.masked.Masked:\n        if isinstance(data, Masked):\n            # first get the mask if one is available:\n            if hasattr(data, \"mask\"):\n                if mask is not None:\n                    log.info(\n                        \"overwriting Masked Quantity's current mask with specified mask.\"\n                    )\n                else:\n                    mask = data.mask\n\n            if isinstance(data, MaskedNDArray):\n                if unit is not None and hasattr(data, \"unit\") and data.unit != unit:\n                    log.info(\n                        \"overwriting MaskedNDArray's current unit with specified unit.\"\n                    )\n                    data = data.to(unit).value\n                elif unit is None and hasattr(data, \"unit\"):\n                    unit = data.unit\n                    data = data.value\n\n                # now get the unmasked ndarray:\n                data = np.asarray(data)\n\n            if isinstance(data, Quantity):\n                # this is a Quantity:\n                if unit is not None and data.unit != unit:\n                    log.info(\"overwriting Quantity's current unit with specified unit.\")", "metadata": {"file_name": "astropy/nddata/nddata.py", "File Name": "astropy/nddata/nddata.py", "Classes": "NDData"}}, {"code": "# Also ignore axes keyword for now...\n            # TODO: in principle, it should be possible to generate the mask\n            # purely based on the signature.\n            if \"axes\" in kwargs:\n                raise NotImplementedError(\n                    \"Masked does not yet support gufunc calls with 'axes'.\"\n                )\n            if ufunc is np.matmul:\n                # np.matmul is tricky and its signature cannot be parsed by\n                # _parse_gufunc_signature.\n                unmasked = np.atleast_1d(*unmasked)\n                mask0, mask1 = masks\n                masks = []\n                is_mat1 = unmasked[1].ndim >= 2\n                if mask0 is not None:\n                    masks.append(np.logical_or.reduce(mask0, axis=-1, keepdims=is_mat1))\n\n                if mask1 is not None:\n                    masks.append(\n                        np.logical_or.reduce(mask1, axis=-2, keepdims=True)\n                        if is_mat1\n                        else np.logical_or.reduce(mask1)\n                    )\n\n                mask = self._combine_masks(masks, out=out_mask, copy=False)\n\n            else:\n                # Parse signature with private numpy function. Note it\n                # cannot handle spaces in tuples, so remove those.\n                in_sig, out_sig = np.lib.function_base._parse_gufunc_signature(\n                    ufunc.signature.replace(\" \", \"\")\n                )\n                axis = kwargs.get(\"axis\", -1)\n                keepdims = kwargs.get(\"keepdims\", False)\n                in_masks = []\n                for sig, mask in zip(in_sig, masks):\n                    if mask is not None:\n                        if sig:\n                            # Input has core dimensions.  Assume that if any\n                            # value in those is masked, the output will be\n                            # masked too (TODO: for multiple core dimensions\n                            # this may be too strong).", "metadata": {"file_name": "astropy/utils/masked/core.py", "File Name": "astropy/utils/masked/core.py", "Classes": "Masked, MaskedInfoBase, MaskedNDArrayInfo, MaskedArraySubclassInfo, MaskedIterator, MaskedNDArray, MaskedRecarray", "Functions": "_comparison_method, _compare, argmin, argmax, argmin, argmax"}}, {"code": "result = x.sum(\n            axis=axis, dtype=dtype, out=out, keepdims=keepdims, where=where_final\n        )\n        n -= ddof\n        n = np.maximum(n, 0, out=n)\n        result /= n\n        result._mask |= n == 0\n        return result\n\n    def std(\n        self, axis=None, dtype=None, out=None, ddof=0, keepdims=False, *, where=True\n    ):\n        result = self.var(\n            axis=axis, dtype=dtype, out=out, ddof=ddof, keepdims=keepdims, where=where\n        )\n        return np.sqrt(result, out=result)\n\n    def __bool__(self):\n        # First get result from array itself; this will error if not a scalar.\n        result = super().__bool__()\n        return result and not self.mask\n\n    def any(self, axis=None, out=None, keepdims=False, *, where=True):\n        return np.logical_or.reduce(\n            self, axis=axis, out=out, keepdims=keepdims, where=~self.mask & where\n        )\n\n    def all(self, axis=None, out=None, keepdims=False, *, where=True):\n        return np.logical_and.reduce(\n            self, axis=axis, out=out, keepdims=keepdims, where=~self.mask & where\n        )\n\n    # Following overrides needed since somehow the ndarray implementation\n    # does not actually call these.\n    def __str__(self):\n        return np.array_str(self)\n\n    def __repr__(self):\n        return np.array_repr(self)\n\n    def __format__(self, format_spec):\n        string = super().__format__(format_spec)\n        if self.shape == () and self.mask:\n            n = min(3, max(1, len(string)))\n            return \" \" * (len(string) - n) + \"\\u2014\" * n\n        else:\n            return string", "metadata": {"file_name": "astropy/utils/masked/core.py", "File Name": "astropy/utils/masked/core.py", "Classes": "Masked, MaskedInfoBase, MaskedNDArrayInfo, MaskedArraySubclassInfo, MaskedIterator, MaskedNDArray, MaskedRecarray", "Functions": "_comparison_method, _compare, argmin, argmax, argmin, argmax"}}, {"code": "if method == \"reduce\":\n                    axis = kwargs.get(\"axis\", None)\n                    keepdims = kwargs.get(\"keepdims\", False)\n                    mask = np.logical_or.reduce(\n                        mask,\n                        where=where_unmasked,\n                        axis=axis,\n                        keepdims=keepdims,\n                        out=out_mask,\n                    )\n                    if where_unmasked is not True:\n                        # Mask also whole rows in which no elements were selected;\n                        # those will have been left as unmasked above.\n                        mask |= ~np.logical_or.reduce(\n                            where_unmasked, axis=axis, keepdims=keepdims\n                        )\n\n                else:\n                    # Accumulate\n                    axis = kwargs.get(\"axis\", 0)\n                    mask = np.logical_or.accumulate(mask, axis=axis, out=out_mask)\n\n            elif out is None:\n                # Can only get here if neither input nor output was masked, but\n                # perhaps where was masked (possible in \"not NUMPY_LT_1_25\" and\n                # in NUMPY_LT_1_21 (latter also allowed axis).\n                # We don't support this.\n                return NotImplemented\n\n        elif method in {\"reduceat\", \"at\"}:  # pragma: no cover\n            raise NotImplementedError(\n                \"masked instances cannot yet deal with 'reduceat' or 'at'.\"\n            )\n\n        if out_unmasked is not None:\n            kwargs[\"out\"] = out_unmasked\n        if where_unmasked is not True:\n            kwargs[\"where\"] = where_unmasked\n        result = getattr(ufunc, method)(*unmasked, **kwargs)\n\n        if result is None:  # pragma: no cover\n            # This happens for the \"at\" method.\n            return result\n\n        if out is not None and len(out) == 1:\n            out = out[0]\n        return self._masked_result(result, mask, out)\n\n    def __array_function__(self, function, types, args, kwargs):\n        # TODO: go through functions systematically to see which ones\n        # work and/or can be supported.", "metadata": {"file_name": "astropy/utils/masked/core.py", "File Name": "astropy/utils/masked/core.py", "Classes": "Masked, MaskedInfoBase, MaskedNDArrayInfo, MaskedArraySubclassInfo, MaskedIterator, MaskedNDArray, MaskedRecarray", "Functions": "_comparison_method, _compare, argmin, argmax, argmin, argmax"}}, {"code": ")\n        if (\n            operand is not None\n            and operand.uncertainty is not None\n            and not isinstance(operand.uncertainty, NDUncertainty)\n        ):\n            raise TypeError(\n                \"Uncertainty propagation is only defined for \"\n                \"subclasses of NDUncertainty.\"\n            )\n\n        # Now do the uncertainty propagation\n        # TODO: There is no enforced requirement that actually forbids the\n        # uncertainty to have negative entries but with correlation the\n        # sign of the uncertainty DOES matter.\n        if self.uncertainty is None and (\n            not hasattr(operand, \"uncertainty\") or operand.uncertainty is None\n        ):\n            # Neither has uncertainties so the result should have none.\n            return None\n        elif self.uncertainty is None:\n            # Create a temporary uncertainty to allow uncertainty propagation\n            # to yield the correct results. (issue #4152)\n            self.uncertainty = operand.uncertainty.__class__(None)\n            result_uncert = self.uncertainty.propagate(\n                operation, operand, result, correlation\n            )\n            # Delete the temporary uncertainty again.\n            self.uncertainty = None\n            return result_uncert\n\n        elif operand is not None and operand.uncertainty is None:\n            # As with self.uncertainty is None but the other way around.\n            operand.uncertainty = self.uncertainty.__class__(None)\n            result_uncert = self.uncertainty.propagate(\n                operation, operand, result, correlation\n            )\n            operand.uncertainty = None\n            return result_uncert\n\n        else:\n            # Both have uncertainties so just propagate.\n\n            # only supply the axis kwarg if one has been specified for a collapsing operation\n            axis_kwarg = dict(axis=kwds[\"axis\"]) if \"axis\" in kwds else dict()\n            return self.uncertainty.propagate(\n                operation, operand, result, correlation, **axis_kwarg\n            )\n\n    def _arithmetic_mask(self, operation, operand, handle_mask, axis=None, **kwds):\n        \"\"\"\n        Calculate the resulting mask.\n\n        This is implemented as the piecewise ``or`` operation if both have a\n        mask.\n\n        Parameters\n        ----------\n        operation : callable\n            see :meth:`NDArithmeticMixin._arithmetic` parameter description.", "metadata": {"file_name": "astropy/nddata/mixins/ndarithmetic.py", "File Name": "astropy/nddata/mixins/ndarithmetic.py", "Classes": "NDArithmeticMixin"}}, {"code": "MASKED_SAFE_FUNCTIONS |= {\n    getattr(np, name)\n    for name in np.core.fromnumeric.__all__\n    if name not in {\"choose\", \"put\", \"resize\", \"searchsorted\", \"where\", \"alen\"}\n}\nMASKED_SAFE_FUNCTIONS |= {\n    # built-in from multiarray\n    np.may_share_memory, np.can_cast, np.min_scalar_type, np.result_type,\n    np.shares_memory,\n    # np.core.arrayprint\n    np.array_repr,\n    # np.core.function_base\n    np.linspace, np.logspace, np.geomspace,\n    # np.core.numeric\n    np.isclose, np.allclose, np.flatnonzero, np.argwhere,\n    # np.core.shape_base\n    np.atleast_1d, np.atleast_2d, np.atleast_3d, np.stack, np.hstack, np.vstack,\n    # np.lib.function_base\n    np.average, np.diff, np.extract, np.meshgrid, np.trapz, np.gradient,\n    # np.lib.index_tricks\n    np.diag_indices_from, np.triu_indices_from, np.tril_indices_from,\n    np.fill_diagonal,\n    # np.lib.shape_base\n    np.column_stack, np.row_stack, np.dstack,", "metadata": {"file_name": "astropy/utils/masked/function_helpers.py", "File Name": "astropy/utils/masked/function_helpers.py", "Classes": "MaskedFormat", "Functions": "_get_data_and_masks, datetime_as_string, sinc, iscomplex, unwrap, nan_to_num, masked_a_helper, masked_m_helper, masked_v_helper, masked_arr_helper, broadcast_to, outer, empty_like, zeros_like, ones_like, full_like, put, putmask, place, copyto, packbits, unpackbits, bincount, msort, sort_complex, concatenate, append, block, broadcast_arrays, insert, count_nonzero, _masked_median_1d, _masked_median, median, _masked_quantile_1d, _masked_quantile, quantile, percentile, array_equal, array_equiv, where, choose, select, piecewise, interp, lexsort, apply_over_axes, _array2string, array2string, array_str, masked_nanfunc, nanfunc"}}, {"code": "np.save, np.savez, np.savetxt, np.savez_compressed,\n    # Polynomials\n    np.poly, np.polyadd, np.polyder, np.polydiv, np.polyfit, np.polyint,\n    np.polymul, np.polysub, np.polyval, np.roots, np.vander,\n}  # fmt: skip\nIGNORED_FUNCTIONS |= {\n    np.pad, np.searchsorted, np.digitize,\n    np.is_busday, np.busday_count, np.busday_offset,\n    # numpy.lib.function_base\n    np.cov, np.corrcoef, np.trim_zeros,\n    # numpy.core.numeric\n    np.correlate, np.convolve,\n    # numpy.lib.histograms\n    np.histogram, np.histogram2d, np.histogramdd, np.histogram_bin_edges,\n    # TODO!!\n    np.dot, np.vdot, np.inner, np.tensordot, np.cross,\n    np.einsum, np.einsum_path,\n}  # fmt: skip\n\n# Really should do these...\nIGNORED_FUNCTIONS |= {\n    getattr(np, setopsname) for setopsname in np.lib.arraysetops.__all__\n}", "metadata": {"file_name": "astropy/utils/masked/function_helpers.py", "File Name": "astropy/utils/masked/function_helpers.py", "Classes": "MaskedFormat", "Functions": "_get_data_and_masks, datetime_as_string, sinc, iscomplex, unwrap, nan_to_num, masked_a_helper, masked_m_helper, masked_v_helper, masked_arr_helper, broadcast_to, outer, empty_like, zeros_like, ones_like, full_like, put, putmask, place, copyto, packbits, unpackbits, bincount, msort, sort_complex, concatenate, append, block, broadcast_arrays, insert, count_nonzero, _masked_median_1d, _masked_median, median, _masked_quantile_1d, _masked_quantile, quantile, percentile, array_equal, array_equiv, where, choose, select, piecewise, interp, lexsort, apply_over_axes, _array2string, array2string, array_str, masked_nanfunc, nanfunc"}}, {"code": "# Find the appropriate keywords for the appropriate method (not sure\n        # if data and uncertainty are ever used ...)\n        kwds2 = {\"mask\": {}, \"meta\": {}, \"wcs\": {}, \"data\": {}, \"uncertainty\": {}}\n        for i in kwds:\n            splitted = i.split(\"_\", 1)\n            try:\n                kwds2[splitted[0]][splitted[1]] = kwds[i]\n            except KeyError:\n                raise KeyError(f\"Unknown prefix {splitted[0]} for parameter {i}\")\n\n        kwargs = {}\n\n        # First check that the WCS allows the arithmetic operation\n        if compare_wcs is None:\n            kwargs[\"wcs\"] = None\n        elif compare_wcs in [\"ff\", \"first_found\"]:\n            if self.wcs is None and hasattr(operand, \"wcs\"):\n                kwargs[\"wcs\"] = deepcopy(operand.wcs)\n            else:\n                kwargs[\"wcs\"] = deepcopy(self.wcs)\n        else:\n            kwargs[\"wcs\"] = self._arithmetic_wcs(\n                operation, operand, compare_wcs, **kwds2[\"wcs\"]\n            )\n\n        # collapse operations on masked quantities/arrays which are supported by\n        # the astropy.utils.masked or np.ma modules should use those modules to\n        # do the arithmetic on the data and propagate masks.\n        use_masked_arith = operand is None and self.mask is not None\n        if use_masked_arith:\n            # if we're *including* masked values in the operation,\n            # use the astropy Masked module:\n            if not operation_ignores_mask:\n                # call the numpy operation on a Masked NDDataArray\n                # representation of the nddata, with units when available:\n                if self.unit is not None and not hasattr(self.data, \"unit\"):\n                    masked_input = Masked(self.data << self.unit, mask=self.mask)\n                else:\n                    masked_input = Masked(self.data, mask=self.mask)\n            # if we're *excluding* masked values in the operation,\n            # we use the numpy.ma module:\n            else:\n                masked_input = np.ma.masked_array(self.data, self.mask)\n            result = operation(masked_input, axis=axis)\n            # since result may be e.g.", "metadata": {"file_name": "astropy/nddata/mixins/ndarithmetic.py", "File Name": "astropy/nddata/mixins/ndarithmetic.py", "Classes": "NDArithmeticMixin"}}, {"code": "\"\"\"\nA module that provides functions for manipulating bit masks and data quality\n(DQ) arrays.\n\n\"\"\"\nimport numbers\nimport warnings\nfrom collections import OrderedDict\n\nimport numpy as np\n\n__all__ = [\n    \"bitfield_to_boolean_mask\",\n    \"interpret_bit_flags\",\n    \"BitFlagNameMap\",\n    \"extend_bit_flag_map\",\n    \"InvalidBitFlag\",\n]\n\n\n_ENABLE_BITFLAG_CACHING = True\n_MAX_UINT_TYPE = np.maximum_sctype(np.uint)\n_SUPPORTED_FLAGS = int(np.bitwise_not(0, dtype=_MAX_UINT_TYPE, casting=\"unsafe\"))\n\n\ndef _is_bit_flag(n):\n    \"\"\"\n    Verifies if the input number is a bit flag (i.e., an integer number that is\n    an integer power of 2).\n\n    Parameters\n    ----------\n    n : int\n        A positive integer number. Non-positive integers are considered not to\n        be \"flags\".\n\n    Returns\n    -------\n    bool\n        ``True`` if input ``n`` is a bit flag and ``False`` if it is not.\n\n    \"\"\"\n    if n < 1:\n        return False\n\n    return bin(n).count(\"1\") == 1\n\n\ndef _is_int(n):\n    return (isinstance(n, numbers.Integral) and not isinstance(n, bool)) or (\n        isinstance(n, np.generic) and np.issubdtype(n, np.integer)\n    )\n\n\nclass InvalidBitFlag(ValueError):\n    \"\"\"Indicates that a value is not an integer that is a power of 2.\"\"\"\n\n    pass", "metadata": {"file_name": "astropy/nddata/bitmask.py", "File Name": "astropy/nddata/bitmask.py", "Classes": "InvalidBitFlag, BitFlag, BitFlagNameMeta, BitFlagNameMap", "Functions": "_is_bit_flag, _is_int, extend_bit_flag_map, interpret_bit_flags, bitfield_to_boolean_mask"}}, {"code": "if (value is not None) and (value is not np.ma.nomask):\n            mask = np.array(value, dtype=np.bool_, copy=False)\n            if mask.shape != self.data.shape:\n                raise ValueError(\n                    f\"dimensions of mask {mask.shape} and data {self.data.shape} do not match\"\n                )\n            else:\n                self._mask = mask\n        else:\n            # internal representation should be one numpy understands\n            self._mask = np.ma.nomask\n\n    @property\n    def shape(self):\n        \"\"\"\n        shape tuple of this object's data.\n        \"\"\"\n        return self.data.shape\n\n    @property\n    def size(self):\n        \"\"\"\n        integer size of this object's data.\n        \"\"\"\n        return self.data.size\n\n    @property\n    def dtype(self):\n        \"\"\"\n        `numpy.dtype` of this object's data.\n        \"\"\"\n        return self.data.dtype\n\n    @property\n    def ndim(self):\n        \"\"\"\n        integer dimensions of this object's data.\n        \"\"\"\n        return self.data.ndim\n\n    @property\n    def flags(self):\n        return self._flags\n\n    @flags.setter\n    def flags(self, value):\n        if value is not None:\n            if isinstance(value, FlagCollection):\n                if value.shape != self.shape:\n                    raise ValueError(\"dimensions of FlagCollection does not match data\")\n                else:\n                    self._flags = value\n            else:\n                flags = np.array(value, copy=False)\n                if flags.shape != self.shape:\n                    raise ValueError(\"dimensions of flags do not match data\")\n                else:\n                    self._flags = flags\n        else:\n            self._flags = value\n\n    def __array__(self):\n        \"\"\"\n        This allows code that requests a Numpy array to use an NDData\n        object as a Numpy array.\n        \"\"\"\n        if self.mask is not None:\n            return np.ma.masked_array(self.data, self.mask)\n        else:\n            return np.array(self.data)\n\n    def __array_prepare__(self, array, context=None):\n        \"\"\"\n        This ensures that a masked array is returned if self is masked.\n        \"\"\"", "metadata": {"file_name": "astropy/nddata/compat.py", "File Name": "astropy/nddata/compat.py", "Classes": "NDDataArray"}}, {"code": "# For the nanfunctions, we just treat any nan as an additional mask.\n_nanfunc_fill_values = {\"nansum\": 0, \"nancumsum\": 0, \"nanprod\": 1, \"nancumprod\": 1}\n\n\ndef masked_nanfunc(nanfuncname):\n    np_func = getattr(np, nanfuncname[3:])\n    fill_value = _nanfunc_fill_values.get(nanfuncname, None)\n\n    def nanfunc(a, *args, **kwargs):\n        from astropy.utils.masked import Masked\n\n        a, mask = Masked._get_data_and_mask(a)\n        if issubclass(a.dtype.type, np.inexact):\n            nans = np.isnan(a)\n            mask = nans if mask is None else (nans | mask)\n\n        if mask is not None:\n            a = Masked(a, mask)\n            if fill_value is not None:\n                a = a.filled(fill_value)\n\n        return np_func(a, *args, **kwargs)\n\n    doc = f\"Like `numpy.{nanfuncname}`, skipping masked values as well.\\n\\n\"\n    if fill_value is not None:\n        # sum, cumsum, prod, cumprod\n        doc += (\n            f\"Masked/NaN values are replaced with {fill_value}. \"\n            \"The output is not masked.\"", "metadata": {"file_name": "astropy/utils/masked/function_helpers.py", "File Name": "astropy/utils/masked/function_helpers.py", "Classes": "MaskedFormat", "Functions": "_get_data_and_masks, datetime_as_string, sinc, iscomplex, unwrap, nan_to_num, masked_a_helper, masked_m_helper, masked_v_helper, masked_arr_helper, broadcast_to, outer, empty_like, zeros_like, ones_like, full_like, put, putmask, place, copyto, packbits, unpackbits, bincount, msort, sort_complex, concatenate, append, block, broadcast_arrays, insert, count_nonzero, _masked_median_1d, _masked_median, median, _masked_quantile_1d, _masked_quantile, quantile, percentile, array_equal, array_equiv, where, choose, select, piecewise, interp, lexsort, apply_over_axes, _array2string, array2string, array_str, masked_nanfunc, nanfunc"}}, {"code": "if isinstance(data, NDDataArray):\n            if flags is None:\n                flags = data.flags\n            else:\n                log.info(\n                    \"Overwriting NDDataArrays's current flags with specified flags\"\n                )\n        self.flags = flags\n\n    # Implement uncertainty as NDUncertainty to support propagation of\n    # uncertainties in arithmetic operations\n    @property\n    def uncertainty(self):\n        return self._uncertainty\n\n    @uncertainty.setter\n    def uncertainty(self, value):\n        if value is not None:\n            if isinstance(value, NDUncertainty):\n                class_name = self.__class__.__name__\n                if not self.unit and value._unit:\n                    # Raise an error if uncertainty has unit and data does not\n                    raise ValueError(\n                        \"Cannot assign an uncertainty with unit \"\n                        \"to {} without \"\n                        \"a unit\".format(class_name)\n                    )\n                self._uncertainty = value\n                self._uncertainty.parent_nddata = self\n            else:\n                raise TypeError(\n                    \"Uncertainty must be an instance of a NDUncertainty object\"\n                )\n        else:\n            self._uncertainty = value\n\n    # Override unit so that we can add a setter.\n    @property\n    def unit(self):\n        return self._unit\n\n    @unit.setter\n    def unit(self, value):\n        from . import conf\n\n        try:\n            if self._unit is not None and conf.warn_setting_unit_directly:\n                log.info(\n                    \"Setting the unit directly changes the unit without \"\n                    \"updating the data or uncertainty. Use the \"\n                    \".convert_unit_to() method to change the unit and \"\n                    \"scale values appropriately.\"\n                )\n        except AttributeError:\n            # raised if self._unit has not been set yet, in which case the\n            # warning is irrelevant\n            pass\n\n        if value is None:\n            self._unit = None\n        else:\n            self._unit = Unit(value)\n\n    # Implement mask in a way that converts nicely to a numpy masked array\n    @property\n    def mask(self):\n        if self._mask is np.ma.nomask:\n            return None\n        else:\n            return self._mask\n\n    @mask.setter\n    def mask(self, value):\n        # Check that value is not either type of null mask.", "metadata": {"file_name": "astropy/nddata/compat.py", "File Name": "astropy/nddata/compat.py", "Classes": "NDDataArray"}}, {"code": "if function in MASKED_SAFE_FUNCTIONS:\n            return super().__array_function__(function, types, args, kwargs)\n\n        elif function in APPLY_TO_BOTH_FUNCTIONS:\n            helper = APPLY_TO_BOTH_FUNCTIONS[function]\n            try:\n                helper_result = helper(*args, **kwargs)\n            except NotImplementedError:\n                return self._not_implemented_or_raise(function, types)\n\n            data_args, mask_args, kwargs, out = helper_result\n            if out is not None:\n                if not isinstance(out, Masked):\n                    return self._not_implemented_or_raise(function, types)\n                function(*mask_args, out=out.mask, **kwargs)\n                function(*data_args, out=out.unmasked, **kwargs)\n                return out\n\n            mask = function(*mask_args, **kwargs)\n            result = function(*data_args, **kwargs)\n\n        elif function in DISPATCHED_FUNCTIONS:\n            dispatched_function = DISPATCHED_FUNCTIONS[function]\n            try:\n                dispatched_result = dispatched_function(*args, **kwargs)\n            except NotImplementedError:\n                return self._not_implemented_or_raise(function, types)\n\n            if not isinstance(dispatched_result, tuple):\n                return dispatched_result\n\n            result, mask, out = dispatched_result\n\n        elif function in UNSUPPORTED_FUNCTIONS:\n            return NotImplemented\n\n        else:  # pragma: no cover\n            # By default, just pass it through for now.\n            return super().__array_function__(function, types, args, kwargs)\n\n        if mask is None:\n            return result\n        else:\n            return self._masked_result(result, mask, out)\n\n    def _not_implemented_or_raise(self, function, types):\n        # Our function helper or dispatcher found that the function does not\n        # work with Masked.  In principle, there may be another class that\n        # knows what to do with us, for which we should return NotImplemented.\n        # But if there is ndarray (or a non-Masked subclass of it) around,\n        # it quite likely coerces, so we should just break.", "metadata": {"file_name": "astropy/utils/masked/core.py", "File Name": "astropy/utils/masked/core.py", "Classes": "Masked, MaskedInfoBase, MaskedNDArrayInfo, MaskedArraySubclassInfo, MaskedIterator, MaskedNDArray, MaskedRecarray", "Functions": "_comparison_method, _compare, argmin, argmax, argmin, argmax"}}, {"code": "bitfield_to_boolean_mask(dqarr, ignore_flags='~(CR,CLOUDY)',\n        ...                                  good_mask_value=0, dtype=int,\n        ...                                  flag_name_map=flag_map)\n        array([[0, 0, 0, 1, 0, 0, 1, 0],\n               [1, 1, 0, 0, 0, 0, 1, 0]])\n        >>> bitmask.bitfield_to_boolean_mask(dqarr, ignore_flags='~(CR+CLOUDY)',\n        ...                                  good_mask_value=0, dtype=int,\n        ...                                  flag_name_map=flag_map)\n        array([[0, 0, 0, 1, 0, 0, 1, 0],\n               [1, 1, 0, 0, 0, 0, 1, 0]])\n\n    \"\"\"\n    bitfield = np.asarray(bitfield)\n    if not np.issubdtype(bitfield.dtype, np.integer):\n        raise TypeError(\"Input bitfield array must be of integer type.\")\n\n    ignore_mask = interpret_bit_flags(\n        ignore_flags, flip_bits=flip_bits, flag_name_map=flag_name_map\n    )\n\n    if ignore_mask is None:\n        if good_mask_value:\n            mask = np.ones_like(bitfield, dtype=dtype)\n        else:\n            mask = np.zeros_like(bitfield, dtype=dtype)\n        return mask\n\n    # filter out bits beyond the maximum supported by the data type:\n    ignore_mask = ignore_mask & _SUPPORTED_FLAGS\n\n    # invert the \"ignore\" mask:\n    ignore_mask = np.bitwise_not(\n        ignore_mask, dtype=bitfield.dtype.type, casting=\"unsafe\"\n    )\n\n    mask = np.empty_like(bitfield, dtype=np.bool_)\n    np.bitwise_and(bitfield, ignore_mask, out=mask, casting=\"unsafe\")\n\n    if good_mask_value:\n        np.logical_not(mask, out=mask)\n\n    return mask.astype(dtype=dtype, subok=False, copy=False)", "metadata": {"file_name": "astropy/nddata/bitmask.py", "File Name": "astropy/nddata/bitmask.py", "Classes": "InvalidBitFlag, BitFlag, BitFlagNameMeta, BitFlagNameMap", "Functions": "_is_bit_flag, _is_int, extend_bit_flag_map, interpret_bit_flags, bitfield_to_boolean_mask"}}, {"code": "np.array_split, np.split, np.hsplit, np.vsplit, np.dsplit,\n    np.expand_dims, np.apply_along_axis, np.kron, np.tile,\n    np.take_along_axis, np.put_along_axis,\n    # np.lib.type_check (all but asfarray, nan_to_num)\n    np.iscomplexobj, np.isrealobj, np.imag, np.isreal, np.real,\n    np.real_if_close, np.common_type,\n    # np.lib.ufunclike\n    np.fix, np.isneginf, np.isposinf,\n    # np.lib.function_base\n    np.angle, np.i0,\n}  # fmt: skip\nIGNORED_FUNCTIONS = {\n    # I/O - useless for Masked, since no way to store the mask.", "metadata": {"file_name": "astropy/utils/masked/function_helpers.py", "File Name": "astropy/utils/masked/function_helpers.py", "Classes": "MaskedFormat", "Functions": "_get_data_and_masks, datetime_as_string, sinc, iscomplex, unwrap, nan_to_num, masked_a_helper, masked_m_helper, masked_v_helper, masked_arr_helper, broadcast_to, outer, empty_like, zeros_like, ones_like, full_like, put, putmask, place, copyto, packbits, unpackbits, bincount, msort, sort_complex, concatenate, append, block, broadcast_arrays, insert, count_nonzero, _masked_median_1d, _masked_median, median, _masked_quantile_1d, _masked_quantile, quantile, percentile, array_equal, array_equiv, where, choose, select, piecewise, interp, lexsort, apply_over_axes, _array2string, array2string, array_str, masked_nanfunc, nanfunc"}}, {"code": "@dispatched_function\ndef array_equal(a1, a2, equal_nan=False):\n    (a1d, a2d), (a1m, a2m) = _get_data_and_masks(a1, a2)\n    if a1d.shape != a2d.shape:\n        return False\n\n    equal = a1d == a2d\n    if equal_nan:\n        equal |= np.isnan(a1d) & np.isnan(a2d)\n    return bool((equal | a1m | a2m).all())\n\n\n@dispatched_function\ndef array_equiv(a1, a2):\n    return bool((a1 == a2).all())\n\n\n@dispatched_function\ndef where(condition, *args):\n    from astropy.utils.masked import Masked\n\n    if not args:\n        return condition.nonzero(), None, None\n\n    condition, c_mask = Masked._get_data_and_mask(condition)\n\n    data, masks = _get_data_and_masks(*args)\n    unmasked = np.where(condition, *data)\n    mask = np.where(condition, *masks)\n    if c_mask is not None:\n        mask |= c_mask\n    return Masked(unmasked, mask=mask)", "metadata": {"file_name": "astropy/utils/masked/function_helpers.py", "File Name": "astropy/utils/masked/function_helpers.py", "Classes": "MaskedFormat", "Functions": "_get_data_and_masks, datetime_as_string, sinc, iscomplex, unwrap, nan_to_num, masked_a_helper, masked_m_helper, masked_v_helper, masked_arr_helper, broadcast_to, outer, empty_like, zeros_like, ones_like, full_like, put, putmask, place, copyto, packbits, unpackbits, bincount, msort, sort_complex, concatenate, append, block, broadcast_arrays, insert, count_nonzero, _masked_median_1d, _masked_median, median, _masked_quantile_1d, _masked_quantile, quantile, percentile, array_equal, array_equiv, where, choose, select, piecewise, interp, lexsort, apply_over_axes, _array2string, array2string, array_str, masked_nanfunc, nanfunc"}}, {"code": ")\n    elif \"arg\" in nanfuncname:\n        doc += (\n            \"No exceptions are raised for fully masked/NaN slices.\\n\"\n            \"Instead, these give index 0.\"\n        )\n    else:\n        doc += (\n            \"No warnings are given for fully masked/NaN slices.\\n\"\n            \"Instead, they are masked in the output.\"\n        )\n\n    nanfunc.__doc__ = doc\n    nanfunc.__name__ = nanfuncname\n\n    return nanfunc\n\n\nfor nanfuncname in np.lib.nanfunctions.__all__:\n    globals()[nanfuncname] = dispatched_function(\n        masked_nanfunc(nanfuncname), helps=getattr(np, nanfuncname)\n    )\n\n\n# Add any dispatched or helper function that has a docstring to\n# __all__, so they will be typeset by sphinx. The logic is that for\n# those presumably the use of the mask is not entirely obvious.\n__all__ += sorted(\n    helper.__name__\n    for helper in (\n        set(APPLY_TO_BOTH_FUNCTIONS.values()) | set(DISPATCHED_FUNCTIONS.values())\n    )\n    if helper.__doc__\n)", "metadata": {"file_name": "astropy/utils/masked/function_helpers.py", "File Name": "astropy/utils/masked/function_helpers.py", "Classes": "MaskedFormat", "Functions": "_get_data_and_masks, datetime_as_string, sinc, iscomplex, unwrap, nan_to_num, masked_a_helper, masked_m_helper, masked_v_helper, masked_arr_helper, broadcast_to, outer, empty_like, zeros_like, ones_like, full_like, put, putmask, place, copyto, packbits, unpackbits, bincount, msort, sort_complex, concatenate, append, block, broadcast_arrays, insert, count_nonzero, _masked_median_1d, _masked_median, median, _masked_quantile_1d, _masked_quantile, quantile, percentile, array_equal, array_equiv, where, choose, select, piecewise, interp, lexsort, apply_over_axes, _array2string, array2string, array_str, masked_nanfunc, nanfunc"}}, {"code": "If the\n          resultant bitfield element is non-zero, that element will be\n          interpreted as a \"bad\" in the output boolean mask and it will be\n          interpreted as \"good\" otherwise. ``flip_bits`` parameter may be used\n          to flip the bits (``bitwise-NOT``) of the bit mask thus effectively\n          changing the meaning of the ``ignore_flags`` parameter from \"ignore\"\n          to \"use only\" these flags.\n\n        .. note::\n\n            Setting ``ignore_flags`` to 0 effectively will assume that all\n            non-zero elements in the input ``bitfield`` array are to be\n            interpreted as \"bad\".\n\n        | When ``ignore_flags`` argument is a Python list of integer bit\n          flags, these flags are added together to create an integer bit mask.\n          Each item in the list must be a flag, i.e., an integer that is an\n          integer power of 2. In order to flip the bits of the resultant\n          bit mask, use ``flip_bits`` parameter.\n\n        | Alternatively, ``ignore_flags`` may be a string of comma- or\n          ``'+'``(or ``'|'``)-separated list of integer bit flags that should\n          be added (bitwise OR) together to create an integer bit mask.\n          For example, both ``'4,8'``, ``'4|8'``, and ``'4+8'`` are equivalent\n          and indicate that bit flags 4 and 8 in the input ``bitfield``\n          array should be ignored when generating boolean mask.\n\n        .. note::\n\n            ``'None'``, ``'INDEF'``, and empty (or all white space) strings\n            are special values of string ``ignore_flags`` that are\n            interpreted as `None`.\n\n        .. note::\n\n            Each item in the list must be a flag, i.e., an integer that is an\n            integer power of 2. In addition, for convenience, an arbitrary\n            **single** integer is allowed and it will be interpreted as an\n            integer bit mask.", "metadata": {"file_name": "astropy/nddata/bitmask.py", "File Name": "astropy/nddata/bitmask.py", "Classes": "InvalidBitFlag, BitFlag, BitFlagNameMeta, BitFlagNameMap", "Functions": "_is_bit_flag, _is_int, extend_bit_flag_map, interpret_bit_flags, bitfield_to_boolean_mask"}}, {"code": "# Licensed under a 3-clause BSD style license - see LICENSE.rst\n\"\"\"Helpers for letting numpy functions interact with Masked arrays.\n\nThe module supplies helper routines for numpy functions that propagate\nmasks appropriately., for use in the ``__array_function__``\nimplementation of `~astropy.utils.masked.MaskedNDArray`.  They are not\nvery useful on their own, but the ones with docstrings are included in\nthe documentation so that there is a place to find out how the mask is\ninterpreted.\n\n\"\"\"\nimport numpy as np\n\nfrom astropy.units.quantity_helper.function_helpers import FunctionAssigner\nfrom astropy.utils.compat import NUMPY_LT_1_23, NUMPY_LT_1_24\n\n# This module should not really be imported, but we define __all__\n# such that sphinx can typeset the functions with docstrings.\n# The latter are added to __all__ at the end.\n__all__ = [\n    \"MASKED_SAFE_FUNCTIONS\",\n    \"APPLY_TO_BOTH_FUNCTIONS\",\n    \"DISPATCHED_FUNCTIONS\",\n    \"UNSUPPORTED_FUNCTIONS\",\n]\n\n\nMASKED_SAFE_FUNCTIONS = set()\n\"\"\"Set of functions that work fine on Masked classes already.\n\nMost of these internally use `numpy.ufunc` or other functions that\nare already covered.\n\"\"\"\n\nAPPLY_TO_BOTH_FUNCTIONS = {}\n\"\"\"Dict of functions that should apply to both data and mask.", "metadata": {"file_name": "astropy/utils/masked/function_helpers.py", "File Name": "astropy/utils/masked/function_helpers.py", "Classes": "MaskedFormat", "Functions": "_get_data_and_masks, datetime_as_string, sinc, iscomplex, unwrap, nan_to_num, masked_a_helper, masked_m_helper, masked_v_helper, masked_arr_helper, broadcast_to, outer, empty_like, zeros_like, ones_like, full_like, put, putmask, place, copyto, packbits, unpackbits, bincount, msort, sort_complex, concatenate, append, block, broadcast_arrays, insert, count_nonzero, _masked_median_1d, _masked_median, median, _masked_quantile_1d, _masked_quantile, quantile, percentile, array_equal, array_equiv, where, choose, select, piecewise, interp, lexsort, apply_over_axes, _array2string, array2string, array_str, masked_nanfunc, nanfunc"}}, {"code": "operand : same type (class) as self\n            see :meth:`NDArithmeticMixin.add`\n\n        propagate_uncertainties : `bool` or ``None``, optional\n            see :meth:`NDArithmeticMixin.add`\n\n        handle_mask : callable, ``'first_found'`` or ``None``, optional\n            see :meth:`NDArithmeticMixin.add`\n\n        handle_meta : callable, ``'first_found'`` or ``None``, optional\n            see :meth:`NDArithmeticMixin.add`\n\n        compare_wcs : callable, ``'first_found'`` or ``None``, optional\n            see :meth:`NDArithmeticMixin.add`\n\n        uncertainty_correlation : ``Number`` or `~numpy.ndarray`, optional\n            see :meth:`NDArithmeticMixin.add`\n\n        operation_ignores_mask : bool, optional\n            When True, masked values will be excluded from operations;\n            otherwise the operation will be performed on all values,\n            including masked ones.\n\n        axis : int or tuple of ints, optional\n            axis or axes over which to perform collapse operations like min, max, sum or mean.\n\n        kwargs :\n            Any other parameter that should be passed to the\n            different :meth:`NDArithmeticMixin._arithmetic_mask` (or wcs, ...)\n            methods.\n\n        Returns\n        -------\n        result : ndarray or `~astropy.units.Quantity`\n            The resulting data as array (in case both operands were without\n            unit) or as quantity if at least one had a unit.\n\n        kwargs : `dict`\n            The kwargs should contain all the other attributes (besides data\n            and unit) needed to create a new instance for the result. Creating\n            the new instance is up to the calling method, for example\n            :meth:`NDArithmeticMixin.add`.\n\n        \"\"\"", "metadata": {"file_name": "astropy/nddata/mixins/ndarithmetic.py", "File Name": "astropy/nddata/mixins/ndarithmetic.py", "Classes": "NDArithmeticMixin"}}, {"code": "if not isinstance(out, np.ndarray):\n            out = out[...]\n            mask = mask[...]\n\n        return self._masked.from_unmasked(out, mask, copy=False)\n\n    def __setitem__(self, index, value):\n        data, mask = self._masked._get_data_and_mask(value, allow_ma_masked=True)\n        if data is not None:\n            self._dataiter[index] = data\n        self._maskiter[index] = mask\n\n    def __next__(self):\n        \"\"\"\n        Return the next value, or raise StopIteration.\n        \"\"\"\n        out = next(self._dataiter)[...]\n        mask = next(self._maskiter)[...]\n        return self._masked.from_unmasked(out, mask, copy=False)\n\n    next = __next__\n\n\nclass MaskedNDArray(Masked, np.ndarray, base_cls=np.ndarray, data_cls=np.ndarray):\n    _mask = None\n\n    info = MaskedNDArrayInfo()\n\n    def __new__(cls, *args, mask=None, **kwargs):\n        \"\"\"Get data class instance from arguments and then set mask.\"\"\"\n        self = super().__new__(cls, *args, **kwargs)\n        if mask is not None:\n            self.mask = mask\n        elif self._mask is None:\n            self.mask = False\n        return self\n\n    def __init_subclass__(cls, **kwargs):\n        super().__init_subclass__(cls, **kwargs)\n        # For all subclasses we should set a default __new__ that passes on\n        # arguments other than mask to the data class, and then sets the mask.\n        if \"__new__\" not in cls.__dict__:\n\n            def __new__(newcls, *args, mask=None, **kwargs):\n                \"\"\"Get data class instance from arguments and then set mask.\"\"\"\n                # Need to explicitly mention classes outside of class definition.", "metadata": {"file_name": "astropy/utils/masked/core.py", "File Name": "astropy/utils/masked/core.py", "Classes": "Masked, MaskedInfoBase, MaskedNDArrayInfo, MaskedArraySubclassInfo, MaskedIterator, MaskedNDArray, MaskedRecarray", "Functions": "_comparison_method, _compare, argmin, argmax, argmin, argmax"}}, {"code": "# Licensed under a 3-clause BSD style license - see LICENSE.rst\n# This module contains a class equivalent to pre-1.0 NDData.\n\n\nimport numpy as np\n\nfrom astropy import log\nfrom astropy.units import Unit, UnitConversionError, UnitsError  # noqa: F401\n\nfrom .flag_collection import FlagCollection\nfrom .mixins.ndarithmetic import NDArithmeticMixin\nfrom .mixins.ndio import NDIOMixin\nfrom .mixins.ndslicing import NDSlicingMixin\nfrom .nddata import NDData\nfrom .nduncertainty import NDUncertainty\n\n__all__ = [\"NDDataArray\"]\n\n\nclass NDDataArray(NDArithmeticMixin, NDSlicingMixin, NDIOMixin, NDData):\n    \"\"\"\n    An ``NDData`` object with arithmetic. This class is functionally equivalent\n    to ``NDData`` in astropy  versions prior to 1.0.\n\n    The key distinction from raw numpy arrays is the presence of\n    additional metadata such as uncertainties, a mask, units, flags,\n    and/or a coordinate system.\n\n    See also: https://docs.astropy.org/en/stable/nddata/\n\n    Parameters\n    ----------\n    data : ndarray or `NDData`\n        The actual data contained in this `NDData` object. Not that this\n        will always be copies by *reference* , so you should make copy\n        the ``data`` before passing it in if that's the  desired behavior.\n\n    uncertainty : `~astropy.nddata.NDUncertainty`, optional\n        Uncertainties on the data.\n\n    mask : array-like, optional\n        Mask for the data, given as a boolean Numpy array or any object that\n        can be converted to a boolean Numpy array with a shape\n        matching that of the data. The values must be ``False`` where\n        the data is *valid* and ``True`` when it is not (like Numpy\n        masked arrays). If ``data`` is a numpy masked array, providing\n        ``mask`` here will causes the mask from the masked array to be\n        ignored.\n\n    flags : array-like or `~astropy.nddata.FlagCollection`, optional\n        Flags giving information about each pixel.", "metadata": {"file_name": "astropy/nddata/compat.py", "File Name": "astropy/nddata/compat.py", "Classes": "NDDataArray"}}, {"code": "operand, operand2, kwargs :\n            See for example ``add``.\n\n        Result\n        ------\n        result : `~astropy.nddata.NDData`-like\n            Depending how this method was called either ``self_or_cls``\n            (called on class) or ``self_or_cls.__class__`` (called on instance)\n            is the NDData-subclass that is used as wrapper for the result.\n        \"\"\"\n        # DO NOT OVERRIDE THIS METHOD IN SUBCLASSES.\n\n        if isinstance(self_or_cls, NDArithmeticMixin):\n            # True means it was called on the instance, so self_or_cls is\n            # a reference to self\n            cls = self_or_cls.__class__\n            if operand2 is None:\n                # Only one operand was given. Set operand2 to operand and\n                # operand to self so that we call the appropriate method of the\n                # operand.\n                operand2 = operand\n                operand = self_or_cls\n            else:\n                # Convert the first operand to the class of this method.\n                # This is important so that always the correct _arithmetics is\n                # called later that method.\n                operand = cls(operand)\n\n        else:\n            # It was used as classmethod so self_or_cls represents the cls\n            cls = self_or_cls\n\n            # It was called on the class so we expect two operands!\n            if operand2 is None:\n                raise TypeError(\n                    \"operand2 must be given when the method isn't \"\n                    \"called on an instance.\"\n                )\n\n            # Convert to this class. See above comment why.\n            operand = cls(operand)\n\n        # At this point operand, operand2, kwargs and cls are determined.\n        if operand2 is not None and not issubclass(\n            operand2.__class__, NDArithmeticMixin\n        ):\n            # Let's try to convert operand2 to the class of operand to allow for\n            # arithmetic operations with numbers, lists, numpy arrays, numpy masked\n            # arrays, astropy quantities, masked quantities and of other subclasses\n            # of NDData.\n            operand2 = cls(operand2)\n\n            # Now call the _arithmetics method to do the arithmetic.", "metadata": {"file_name": "astropy/nddata/mixins/ndarithmetic.py", "File Name": "astropy/nddata/mixins/ndarithmetic.py", "Classes": "NDArithmeticMixin"}}, {"code": ")\n\n        if \",\" in bit_flags:\n            bit_flags = bit_flags.split(\",\")\n\n        elif \"+\" in bit_flags:\n            bit_flags = bit_flags.split(\"+\")\n\n        elif \"|\" in bit_flags:\n            bit_flags = bit_flags.split(\"|\")\n\n        else:\n            if bit_flags == \"\":\n                raise ValueError(\n                    \"Empty bit flag lists not allowed when either bitwise-NOT \"\n                    \"or parenthesis are present.\"\n                )\n            bit_flags = [bit_flags]\n\n        if flag_name_map is not None:\n            try:\n                int(bit_flags[0])\n            except ValueError:\n                bit_flags = [flag_name_map[f] for f in bit_flags]\n\n        allow_non_flags = len(bit_flags) == 1\n\n    elif hasattr(bit_flags, \"__iter__\"):\n        if not all(_is_int(flag) for flag in bit_flags):\n            if flag_name_map is not None and all(\n                isinstance(flag, str) for flag in bit_flags\n            ):\n                bit_flags = [flag_name_map[f] for f in bit_flags]\n            else:\n                raise TypeError(\n                    \"Every bit flag in a list must be either an \"\n                    \"integer flag value or a 'str' flag name.\"\n                )\n\n    else:\n        raise TypeError(\"Unsupported type for argument 'bit_flags'.\")\n\n    bitset = set(map(int, bit_flags))\n    if len(bitset) != len(bit_flags):\n        warnings.warn(\"Duplicate bit flags will be ignored\")\n\n    bitmask = 0\n    for v in bitset:\n        if not _is_bit_flag(v) and not allow_non_flags:\n            raise ValueError(\n                f\"Input list contains invalid (not powers of two) bit flag: {v}\"\n            )\n        bitmask += v\n\n    if flip_bits:\n        bitmask = ~bitmask\n\n    return bitmask", "metadata": {"file_name": "astropy/nddata/bitmask.py", "File Name": "astropy/nddata/bitmask.py", "Classes": "InvalidBitFlag, BitFlag, BitFlagNameMeta, BitFlagNameMap", "Functions": "_is_bit_flag, _is_int, extend_bit_flag_map, interpret_bit_flags, bitfield_to_boolean_mask"}}, {"code": "@dispatched_function\ndef iscomplex(x):\n    return np.iscomplex(x.unmasked), x.mask.copy(), None\n\n\n@dispatched_function\ndef unwrap(p, *args, **kwargs):\n    return np.unwrap(p.unmasked, *args, **kwargs), p.mask.copy(), None\n\n\n@dispatched_function\ndef nan_to_num(x, copy=True, nan=0.0, posinf=None, neginf=None):\n    data = np.nan_to_num(x.unmasked, copy=copy, nan=nan, posinf=posinf, neginf=neginf)\n    return (data, x.mask.copy(), None) if copy else x\n\n\n# Following are simple functions related to shapes, where the same function\n# should be applied to the data and the mask.  They cannot all share the\n# same helper, because the first arguments have different names.\n@apply_to_both(\n    helps={np.copy, np.asfarray, np.resize, np.moveaxis, np.rollaxis, np.roll}\n)\ndef masked_a_helper(a, *args, **kwargs):\n    data, mask = _get_data_and_masks(a)\n    return data + args, mask + args, kwargs, None\n\n\n@apply_to_both(helps={np.flip, np.flipud, np.fliplr, np.rot90, np.triu, np.tril})\ndef masked_m_helper(m, *args, **kwargs):\n    data, mask = _get_data_and_masks(m)\n    return data + args, mask + args, kwargs, None", "metadata": {"file_name": "astropy/utils/masked/function_helpers.py", "File Name": "astropy/utils/masked/function_helpers.py", "Classes": "MaskedFormat", "Functions": "_get_data_and_masks, datetime_as_string, sinc, iscomplex, unwrap, nan_to_num, masked_a_helper, masked_m_helper, masked_v_helper, masked_arr_helper, broadcast_to, outer, empty_like, zeros_like, ones_like, full_like, put, putmask, place, copyto, packbits, unpackbits, bincount, msort, sort_complex, concatenate, append, block, broadcast_arrays, insert, count_nonzero, _masked_median_1d, _masked_median, median, _masked_quantile_1d, _masked_quantile, quantile, percentile, array_equal, array_equiv, where, choose, select, piecewise, interp, lexsort, apply_over_axes, _array2string, array2string, array_str, masked_nanfunc, nanfunc"}}, {"code": ")\n        return None\n\n    elif isinstance(bit_flags, str):\n        if has_flip_bits:\n            raise TypeError(\n                \"Keyword argument 'flip_bits' is not permitted for \"\n                \"comma-separated string lists of bit flags. Prepend '~' to \"\n                \"the string to indicate bit-flipping.\"\n            )\n\n        bit_flags = str(bit_flags).strip()\n\n        if bit_flags.upper() in [\"\", \"NONE\", \"INDEF\"]:\n            return None\n\n        # check whether bitwise-NOT is present and if it is, check that it is\n        # in the first position:\n        bitflip_pos = bit_flags.find(\"~\")\n        if bitflip_pos == 0:\n            flip_bits = True\n            bit_flags = bit_flags[1:].lstrip()\n        else:\n            if bitflip_pos > 0:\n                raise ValueError(\"Bitwise-NOT must precede bit flag list.\")\n            flip_bits = False\n\n        # basic check for correct use of parenthesis:\n        while True:\n            nlpar = bit_flags.count(\"(\")\n            nrpar = bit_flags.count(\")\")\n\n            if nlpar == 0 and nrpar == 0:\n                break\n\n            if nlpar != nrpar:\n                raise ValueError(\"Unbalanced parentheses in bit flag list.\")\n\n            lpar_pos = bit_flags.find(\"(\")\n            rpar_pos = bit_flags.rfind(\")\")\n            if lpar_pos > 0 or rpar_pos < (len(bit_flags) - 1):\n                raise ValueError(\n                    \"Incorrect syntax (incorrect use of parenthesis) in bit flag list.\"\n                )\n\n            bit_flags = bit_flags[1:-1].strip()\n\n        if sum(k in bit_flags for k in \"+,|\") > 1:\n            raise ValueError(\n                \"Only one type of bit flag separator may be used in one \"\n                \"expression. Allowed separators are: '+', '|', or ','.\"", "metadata": {"file_name": "astropy/nddata/bitmask.py", "File Name": "astropy/nddata/bitmask.py", "Classes": "InvalidBitFlag, BitFlag, BitFlagNameMeta, BitFlagNameMap", "Functions": "_is_bit_flag, _is_int, extend_bit_flag_map, interpret_bit_flags, bitfield_to_boolean_mask"}}, {"code": "For example, instead of ``'4,8'`` one could\n            simply provide string ``'12'``.\n\n        .. note::\n            Only one flag separator is supported at a time. ``ignore_flags``\n            string should not mix ``','``, ``'+'``, and ``'|'`` separators.\n\n        .. note::\n\n            When ``ignore_flags`` is a `str` and when it is prepended with\n            '~', then the meaning of ``ignore_flags`` parameters will be\n            reversed: now it will be interpreted as a list of bit flags to be\n            *used* (or *not ignored*) when deciding which elements of the\n            input ``bitfield`` array are \"bad\". Following this convention,\n            an ``ignore_flags`` string value of ``'~0'`` would be equivalent\n            to setting ``ignore_flags=None``.\n\n        .. warning::\n\n            Because prepending '~' to a string ``ignore_flags`` is equivalent\n            to setting ``flip_bits`` to `True`, ``flip_bits`` cannot be used\n            with string ``ignore_flags`` and it must be set to `None`.\n\n    flip_bits : bool, None (default = None)\n        Specifies whether or not to invert the bits of the bit mask either\n        supplied directly through ``ignore_flags`` parameter or built from the\n        bit flags passed through ``ignore_flags`` (only when bit flags are\n        passed as Python lists of integer bit flags). Occasionally, it may be\n        useful to *consider only specific bit flags* in the ``bitfield``\n        array when creating a boolean mask as opposed to *ignoring* specific\n        bit flags as ``ignore_flags`` behaves by default. This can be achieved\n        by inverting/flipping the bits of the bit mask created from\n        ``ignore_flags`` flags which effectively changes the meaning of the\n        ``ignore_flags`` parameter from \"ignore\" to \"use only\" these flags.\n        Setting ``flip_bits`` to `None` means that no bit flipping will be\n        performed.", "metadata": {"file_name": "astropy/nddata/bitmask.py", "File Name": "astropy/nddata/bitmask.py", "Classes": "InvalidBitFlag, BitFlag, BitFlagNameMeta, BitFlagNameMap", "Functions": "_is_bit_flag, _is_int, extend_bit_flag_map, interpret_bit_flags, bitfield_to_boolean_mask"}}, {"code": "Examples\n    --------\n        >>> from astropy.nddata.bitmask import interpret_bit_flags, extend_bit_flag_map\n        >>> ST_DQ = extend_bit_flag_map('ST_DQ', CR=1, CLOUDY=4, RAINY=8, HOT=16, DEAD=32)\n        >>> \"{0:016b}\".format(0xFFFF & interpret_bit_flags(28))\n        '0000000000011100'\n        >>> \"{0:016b}\".format(0xFFFF & interpret_bit_flags('4,8,16'))\n        '0000000000011100'\n        >>> \"{0:016b}\".format(0xFFFF & interpret_bit_flags('CLOUDY,RAINY,HOT', flag_name_map=ST_DQ))\n        '0000000000011100'\n        >>> \"{0:016b}\".format(0xFFFF & interpret_bit_flags('~4,8,16'))\n        '1111111111100011'\n        >>> \"{0:016b}\".format(0xFFFF & interpret_bit_flags('~(4+8+16)'))\n        '1111111111100011'\n        >>> \"{0:016b}\".format(0xFFFF & interpret_bit_flags('~(CLOUDY+RAINY+HOT)',\n        ... flag_name_map=ST_DQ))\n        '1111111111100011'\n        >>> \"{0:016b}\".format(0xFFFF & interpret_bit_flags([4, 8, 16]))\n        '0000000000011100'\n        >>> \"{0:016b}\".format(0xFFFF & interpret_bit_flags([4, 8, 16], flip_bits=True))\n        '1111111111100011'\n\n    \"\"\"\n    has_flip_bits = flip_bits is not None\n    flip_bits = bool(flip_bits)\n    allow_non_flags = False\n\n    if _is_int(bit_flags):\n        return ~int(bit_flags) if flip_bits else int(bit_flags)\n\n    elif bit_flags is None:\n        if has_flip_bits:\n            raise TypeError(\n                \"Keyword argument 'flip_bits' must be set to 'None' when \"\n                \"input 'bit_flags' is None.\"", "metadata": {"file_name": "astropy/nddata/bitmask.py", "File Name": "astropy/nddata/bitmask.py", "Classes": "InvalidBitFlag, BitFlag, BitFlagNameMeta, BitFlagNameMap", "Functions": "_is_bit_flag, _is_int, extend_bit_flag_map, interpret_bit_flags, bitfield_to_boolean_mask"}}, {"code": "Unable to reassign attribute {name}\"\n            if cls._locked:\n                raise AttributeError(err_msg)\n\n        namel = name.lower()\n        if _ENABLE_BITFLAG_CACHING:\n            if not namel.startswith(\"_\") and namel in cls._cache:\n                raise AttributeError(err_msg)\n\n        else:\n            for b in cls.__bases__:\n                if not namel.startswith(\"_\") and namel in list(\n                    map(str.lower, b.__dict__)\n                ):\n                    raise AttributeError(err_msg)\n            if namel in list(map(str.lower, cls.__dict__)):\n                raise AttributeError(err_msg)\n\n        val = BitFlag(val)\n\n        if _ENABLE_BITFLAG_CACHING and not namel.startswith(\"_\"):\n            cls._cache[namel] = val\n\n        return super().__setattr__(name, val)\n\n    def __getattr__(cls, name):\n        if _ENABLE_BITFLAG_CACHING:\n            flagnames = cls._cache\n        else:\n            flagnames = {k.lower(): v for k, v in cls.__dict__.items()}\n            flagnames.update(\n                {k.lower(): v for b in cls.__bases__ for k, v in b.__dict__.items()}\n            )\n        try:\n            return flagnames[name.lower()]\n        except KeyError:\n            raise AttributeError(f\"Flag '{name}' not defined\")\n\n    def __getitem__(cls, key):\n        return cls.__getattr__(key)\n\n    def __add__(cls, items):\n        if not isinstance(items, dict):\n            if not isinstance(items[0], (tuple, list)):\n                items = [items]\n            items = dict(items)\n\n        return extend_bit_flag_map(\n            cls.__name__ + \"_\" + \"_\".join(list(items)), cls, **items\n        )\n\n    def __iadd__(cls, other):\n        raise NotImplementedError(\n            \"Unary '+' is not supported. Use binary operator instead.\"\n        )\n\n    def __delattr__(cls, name):\n        raise AttributeError(\n            f\"{cls.__name__}: cannot delete {cls.mro()[-2].__name__} member.\"", "metadata": {"file_name": "astropy/nddata/bitmask.py", "File Name": "astropy/nddata/bitmask.py", "Classes": "InvalidBitFlag, BitFlag, BitFlagNameMeta, BitFlagNameMap", "Functions": "_is_bit_flag, _is_int, extend_bit_flag_map, interpret_bit_flags, bitfield_to_boolean_mask"}}, {"code": "a float if operation is a sum over all axes,\n            # let's ensure that result is a masked array, since we'll assume this later:\n            if not hasattr(result, \"mask\"):\n                result = np.ma.masked_array(\n                    result, mask=np.zeros_like(result, dtype=bool)\n                )\n        else:\n            # Then calculate the resulting data (which can but needs not be a\n            # quantity)\n            result = self._arithmetic_data(\n                operation, operand, axis=axis, **kwds2[\"data\"]\n            )\n\n        # preserve original units\n        if not hasattr(result, \"unit\") and hasattr(self, \"unit\"):\n            kwargs[\"unit\"] = self.unit\n\n        # Determine the other properties\n        if propagate_uncertainties is None:\n            kwargs[\"uncertainty\"] = None\n        elif not propagate_uncertainties:\n            if self.uncertainty is None:\n                kwargs[\"uncertainty\"] = deepcopy(operand.uncertainty)\n            else:\n                kwargs[\"uncertainty\"] = deepcopy(self.uncertainty)\n        else:\n            kwargs[\"uncertainty\"] = self._arithmetic_uncertainty(\n                operation,\n                operand,\n                result,\n                uncertainty_correlation,\n                axis=axis,\n                **kwds2[\"uncertainty\"],\n            )\n\n        # If both are None, there is nothing to do.\n        if self.psf is not None or (operand is not None and operand.psf is not None):\n            warnings.warn(\n                f\"Not setting psf attribute during {operation.__name__}.", "metadata": {"file_name": "astropy/nddata/mixins/ndarithmetic.py", "File Name": "astropy/nddata/mixins/ndarithmetic.py", "Classes": "NDArithmeticMixin"}}, {"code": "data = data.to(unit)\n                elif unit is None and data.unit is not None:\n                    unit = data.unit\n                data = data.value\n\n        if isinstance(data, np.ma.masked_array):\n            if mask is not None:\n                log.info(\n                    \"overwriting masked ndarray's current mask with specified mask.\"\n                )\n            else:\n                mask = data.mask\n            data = data.data\n\n        if isinstance(data, Quantity):\n            # this is a Quantity:\n            if unit is not None and data.unit != unit:\n                log.info(\"overwriting Quantity's current unit with specified unit.\")\n                data = data.to(unit)\n            elif unit is None and data.unit is not None:\n                unit = data.unit\n            data = data.value\n\n        if isinstance(data, np.ndarray):\n            # check for mask from np.ma.masked_ndarray\n            if hasattr(data, \"mask\"):\n                if mask is not None:\n                    log.info(\n                        \"overwriting masked ndarray's current mask with specified mask.\"\n                    )\n                else:\n                    mask = data.mask\n\n        # Quick check on the parameters if they match the requirements.\n        if (\n            not hasattr(data, \"shape\")\n            or not hasattr(data, \"__getitem__\")\n            or not hasattr(data, \"__array__\")\n        ):\n            # Data doesn't look like a numpy array, try converting it to\n            # one.\n            data = np.array(data, subok=True, copy=False)\n        # Another quick check to see if what we got looks like an array\n        # rather than an object (since numpy will convert a\n        # non-numerical/non-string inputs to an array of objects).\n        if data.dtype == \"O\":\n            raise TypeError(\"could not convert data to numpy array.\")\n\n        if unit is not None:\n            unit = Unit(unit)\n\n        if copy:\n            # Data might have been copied before but no way of validating\n            # without another variable.", "metadata": {"file_name": "astropy/nddata/nddata.py", "File Name": "astropy/nddata/nddata.py", "Classes": "NDData"}}, {"code": "Usually used to get the current shape of an array, but may also be\n        used to reshape the array in-place by assigning a tuple of array\n        dimensions to it.  As with `numpy.reshape`, one of the new shape\n        dimensions can be -1, in which case its value is inferred from the\n        size of the array and the remaining dimensions.\n\n        Raises\n        ------\n        AttributeError\n            If a copy is required, of either the data or the mask.\n\n        \"\"\"\n        # Redefinition to allow defining a setter and add a docstring.\n        return super().shape\n\n    @shape.setter\n    def shape(self, shape):\n        old_shape = self.shape\n        self._mask.shape = shape\n        # Reshape array proper in try/except just in case some broadcasting\n        # or so causes it to fail.\n        try:\n            super(MaskedNDArray, type(self)).shape.__set__(self, shape)\n        except Exception as exc:\n            self._mask.shape = old_shape\n            # Given that the mask reshaping succeeded, the only logical\n            # reason for an exception is something like a broadcast error in\n            # in __array_finalize__, or a different memory ordering between\n            # mask and data.  For those, give a more useful error message;\n            # otherwise just raise the error.\n            if \"could not broadcast\" in exc.args[0]:\n                raise AttributeError(\n                    \"Incompatible shape for in-place modification. \"\n                    \"Use `.reshape()` to make a copy with the desired \"\n                    \"shape.\"", "metadata": {"file_name": "astropy/utils/masked/core.py", "File Name": "astropy/utils/masked/core.py", "Classes": "Masked, MaskedInfoBase, MaskedNDArrayInfo, MaskedArraySubclassInfo, MaskedIterator, MaskedNDArray, MaskedRecarray", "Functions": "_comparison_method, _compare, argmin, argmax, argmin, argmax"}}, {"code": "Only relevant if a single\n            input mask is not `None`, and ``out`` is not given.\n        \"\"\"\n        masks = [m for m in masks if m is not None and m is not False]\n        if not masks:\n            return False\n        if len(masks) == 1:\n            if out is None:\n                return masks[0].copy() if copy else masks[0]\n            else:\n                np.copyto(out, masks[0], where=where)\n                return out\n\n        out = np.logical_or(masks[0], masks[1], out=out, where=where)\n        for mask in masks[2:]:\n            np.logical_or(out, mask, out=out, where=where)\n        return out\n\n    def __array_ufunc__(self, ufunc, method, *inputs, **kwargs):\n        out = kwargs.pop(\"out\", None)\n        out_unmasked = None\n        out_mask = None\n        if out is not None:\n            out_unmasked, out_masks = self._get_data_and_masks(*out)\n            for d, m in zip(out_unmasked, out_masks):\n                if m is None:\n                    # TODO: allow writing to unmasked output if nothing is masked?\n                    if d is not None:\n                        raise TypeError(\"cannot write to unmasked output\")\n                elif out_mask is None:\n                    out_mask = m\n\n        # TODO: where is only needed for __call__ and reduce;\n        # this is very fast, but still worth separating out?\n        where = kwargs.pop(\"where\", True)\n        if where is True:\n            where_unmasked = True\n            where_mask = None\n        else:\n            where_unmasked, where_mask = self._get_data_and_mask(where)\n\n        unmasked, masks = self._get_data_and_masks(*inputs)\n\n        if ufunc.signature:\n            # We're dealing with a gufunc. For now, only deal with\n            # np.matmul and gufuncs for which the mask of any output always\n            # depends on all core dimension values of all inputs.", "metadata": {"file_name": "astropy/utils/masked/core.py", "File Name": "astropy/utils/masked/core.py", "Classes": "Masked, MaskedInfoBase, MaskedNDArrayInfo, MaskedArraySubclassInfo, MaskedIterator, MaskedNDArray, MaskedRecarray", "Functions": "_comparison_method, _compare, argmin, argmax, argmin, argmax"}}, {"code": "# Licensed under a 3-clause BSD style license - see LICENSE.rst\n\n\"\"\"\nThis module implements a class based on NDData with all Mixins.\n\"\"\"\n\n\nfrom .mixins.ndarithmetic import NDArithmeticMixin\nfrom .mixins.ndio import NDIOMixin\nfrom .mixins.ndslicing import NDSlicingMixin\nfrom .nddata import NDData\n\n__all__ = [\"NDDataRef\"]\n\n\nclass NDDataRef(NDArithmeticMixin, NDIOMixin, NDSlicingMixin, NDData):\n    \"\"\"Implements `NDData` with all Mixins.\n\n    This class implements a `NDData`-like container that supports reading and\n    writing as implemented in the ``astropy.io.registry`` and also slicing\n    (indexing) and simple arithmetic (add, subtract, divide and multiply).\n\n    Notes\n    -----\n    A key distinction from `NDDataArray` is that this class does not attempt\n    to provide anything that was not defined in any of the parent classes.\n\n    See Also\n    --------\n    NDData\n    NDArithmeticMixin\n    NDSlicingMixin\n    NDIOMixin\n\n    Examples\n    --------\n    The mixins allow operation that are not possible with `NDData` or\n    `NDDataBase`, i.e. simple arithmetic::\n\n        >>> from astropy.nddata import NDDataRef, StdDevUncertainty\n        >>> import numpy as np\n\n        >>> data = np.ones((3,3), dtype=float)\n        >>> ndd1 = NDDataRef(data, uncertainty=StdDevUncertainty(data))\n        >>> ndd2 = NDDataRef(data, uncertainty=StdDevUncertainty(data))\n\n        >>> ndd3 = ndd1.add(ndd2)\n        >>> ndd3.data  # doctest: +FLOAT_CMP\n        array([[2., 2., 2.],\n               [2., 2., 2.],\n               [2., 2., 2.]])", "metadata": {"file_name": "astropy/nddata/nddata_withmixins.py", "File Name": "astropy/nddata/nddata_withmixins.py", "Classes": "NDDataRef"}}, {"code": "self = super(cls, newcls).__new__(newcls, *args, **kwargs)\n                if mask is not None:\n                    self.mask = mask\n                elif self._mask is None:\n                    self.mask = False\n                return self\n\n            cls.__new__ = __new__\n\n        if \"info\" not in cls.__dict__ and hasattr(cls._data_cls, \"info\"):\n            data_info = cls._data_cls.info\n            attr_names = data_info.attr_names | {\"serialize_method\"}\n            new_info = type(\n                cls.__name__ + \"Info\",\n                (MaskedArraySubclassInfo, data_info.__class__),\n                dict(attr_names=attr_names),\n            )\n            cls.info = new_info()\n\n    # The two pieces typically overridden.\n    @classmethod\n    def from_unmasked(cls, data, mask=None, copy=False):\n        # Note: have to override since __new__ would use ndarray.__new__\n        # which expects the shape as its first argument, not an array.\n        data = np.array(data, subok=True, copy=copy)\n        self = data.view(cls)\n        self._set_mask(mask, copy=copy)\n        return self\n\n    @property\n    def unmasked(self):\n        return super().view(self._data_cls)\n\n    @classmethod\n    def _get_masked_cls(cls, data_cls):\n        # Short-cuts\n        if data_cls is np.ndarray:\n            return MaskedNDArray\n        elif data_cls is None:  # for .view()\n            return cls\n\n        return super()._get_masked_cls(data_cls)\n\n    @property\n    def flat(self):\n        \"\"\"A 1-D iterator over the Masked array.\n\n        This returns a ``MaskedIterator`` instance, which behaves the same\n        as the `~numpy.flatiter` instance returned by `~numpy.ndarray.flat`,\n        and is similar to Python's built-in iterator, except that it also\n        allows assignment.\n        \"\"\"\n        return MaskedIterator(self)\n\n    @property\n    def _baseclass(self):\n        \"\"\"Work-around for MaskedArray initialization.", "metadata": {"file_name": "astropy/utils/masked/core.py", "File Name": "astropy/utils/masked/core.py", "Classes": "Masked, MaskedInfoBase, MaskedNDArrayInfo, MaskedArraySubclassInfo, MaskedIterator, MaskedNDArray, MaskedRecarray", "Functions": "_comparison_method, _compare, argmin, argmax, argmin, argmax"}}, {"code": ")\n        >>> bitmask.bitfield_to_boolean_mask(dqarr, ignore_flags=6,\n        ...                                  good_mask_value=0, dtype=int)\n        array([[0, 0, 1, 0, 0, 1, 1, 0],\n               [1, 0, 0, 0, 0, 1, 0, 0]])\n        >>> bitmask.bitfield_to_boolean_mask(dqarr, ignore_flags=~6,\n        ...                                  good_mask_value=0, dtype=int)\n        array([[0, 0, 0, 1, 0, 0, 1, 0],\n               [1, 1, 0, 0, 0, 0, 1, 0]])\n        >>> bitmask.bitfield_to_boolean_mask(dqarr, ignore_flags=6, dtype=int,\n        ...                                  flip_bits=True, good_mask_value=0)\n        array([[0, 0, 0, 1, 0, 0, 1, 0],\n               [1, 1, 0, 0, 0, 0, 1, 0]])\n        >>> bitmask.bitfield_to_boolean_mask(dqarr, ignore_flags='~(2+4)',\n        ...                                  good_mask_value=0, dtype=int)\n        array([[0, 0, 0, 1, 0, 0, 1, 0],\n               [1, 1, 0, 0, 0, 0, 1, 0]])\n        >>> bitmask.bitfield_to_boolean_mask(dqarr, ignore_flags=[2, 4],\n        ...                                  flip_bits=True, good_mask_value=0,\n        ...                                  dtype=int)\n        array([[0, 0, 0, 1, 0, 0, 1, 0],\n               [1, 1, 0, 0, 0, 0, 1, 0]])\n        >>> bitmask.", "metadata": {"file_name": "astropy/nddata/bitmask.py", "File Name": "astropy/nddata/bitmask.py", "Classes": "InvalidBitFlag, BitFlag, BitFlagNameMeta, BitFlagNameMap", "Functions": "_is_bit_flag, _is_int, extend_bit_flag_map, interpret_bit_flags, bitfield_to_boolean_mask"}}, {"code": "result, init_kwds = operand._arithmetic(operation, operand2, **kwargs)\n        elif issubclass(operand2.__class__, NDArithmeticMixin):\n            # calling as class method:\n            result, init_kwds = cls._arithmetic(\n                operand,\n                operation,\n                operand2,\n                **kwargs,\n            )\n        else:\n            # otherwise call the _arithmetic method on self for a collapse operation:\n            # for collapse operations, use astropy.utils.masked rather than handle_mask\n            result, init_kwds = self_or_cls._arithmetic(\n                operation,\n                operand2,\n                **kwargs,\n            )\n\n        # Return a new class based on the result\n        return cls(result, **init_kwds)", "metadata": {"file_name": "astropy/nddata/mixins/ndarithmetic.py", "File Name": "astropy/nddata/mixins/ndarithmetic.py", "Classes": "NDArithmeticMixin"}}, {"code": "flag_name_map : BitFlagNameMap\n         A `BitFlagNameMap` object that provides mapping from mnemonic\n         bit flag names to integer bit values in order to translate mnemonic\n         flags to numeric values when ``bit_flags`` that are comma- or\n         '+'-separated list of menmonic bit flag names.\n\n    Returns\n    -------\n    mask : ndarray\n        Returns an array of the same dimensionality as the input ``bitfield``\n        array whose elements can have two possible values,\n        e.g., ``numpy.True_`` or ``numpy.False_`` (or 1 or 0 for integer\n        ``dtype``) according to values of to the input ``bitfield`` elements,\n        ``ignore_flags`` parameter, and the ``good_mask_value`` parameter.\n\n    Examples\n    --------\n        >>> from astropy.nddata import bitmask\n        >>> import numpy as np\n        >>> dqarr = np.asarray([[0, 0, 1, 2, 0, 8, 12, 0],\n        ...                     [10, 4, 0, 0, 0, 16, 6, 0]])\n        >>> flag_map = bitmask.extend_bit_flag_map(\n        ...     'ST_DQ', CR=2, CLOUDY=4, RAINY=8, HOT=16, DEAD=32\n        ... )\n        >>> bitmask.bitfield_to_boolean_mask(dqarr, ignore_flags=0,\n        ...                                  dtype=int)\n        array([[0, 0, 1, 1, 0, 1, 1, 0],\n               [1, 1, 0, 0, 0, 1, 1, 0]])\n        >>> bitmask.bitfield_to_boolean_mask(dqarr, ignore_flags=0,\n        ...                                  dtype=bool)\n        array([[False, False,  True,  True, False,  True,  True, False],\n               [ True,  True, False, False, False,  True,  True, False]]...", "metadata": {"file_name": "astropy/nddata/bitmask.py", "File Name": "astropy/nddata/bitmask.py", "Classes": "InvalidBitFlag, BitFlag, BitFlagNameMeta, BitFlagNameMap", "Functions": "_is_bit_flag, _is_int, extend_bit_flag_map, interpret_bit_flags, bitfield_to_boolean_mask"}}, {"code": "from None\n                else:\n                    data = data.data\n\n        return data, mask\n\n    @classmethod\n    def _get_data_and_masks(cls, *args):\n        data_masks = [cls._get_data_and_mask(arg) for arg in args]\n        return (\n            tuple(data for data, _ in data_masks),\n            tuple(mask for _, mask in data_masks),\n        )\n\n    def _get_mask(self):\n        \"\"\"The mask.\n\n        If set, replace the original mask, with whatever it is set with,\n        using a view if no broadcasting or type conversion is required.\n        \"\"\"\n        return self._mask\n\n    def _set_mask(self, mask, copy=False):\n        self_dtype = getattr(self, \"dtype\", None)\n        mask_dtype = (\n            np.ma.make_mask_descr(self_dtype)\n            if self_dtype and self_dtype.names\n            else np.dtype(\"?\")\n        )\n        ma = np.asanyarray(mask, dtype=mask_dtype)\n        if ma.shape != self.shape:\n            # This will fail (correctly) if not broadcastable.\n            self._mask = np.empty(self.shape, dtype=mask_dtype)\n            self._mask[...] = ma\n        elif ma is mask:\n            # Even if not copying use a view so that shape setting\n            # does not propagate.\n            self._mask = mask.copy() if copy else mask.view()\n        else:\n            self._mask = ma\n\n    mask = property(_get_mask, _set_mask)\n\n    # Note: subclass should generally override the unmasked property.\n    # This one assumes the unmasked data is stored in a private attribute.\n    @property\n    def unmasked(self):\n        \"\"\"The unmasked values.\n\n        See Also\n        --------\n        astropy.utils.masked.Masked.filled\n        \"\"\"\n        return self._unmasked\n\n    def filled(self, fill_value):\n        \"\"\"Get a copy of the underlying data, with masked values filled in.\n\n        Parameters\n        ----------\n        fill_value : object\n            Value to replace masked values with.", "metadata": {"file_name": "astropy/utils/masked/core.py", "File Name": "astropy/utils/masked/core.py", "Classes": "Masked, MaskedInfoBase, MaskedNDArrayInfo, MaskedArraySubclassInfo, MaskedIterator, MaskedNDArray, MaskedRecarray", "Functions": "_comparison_method, _compare, argmin, argmax, argmin, argmax"}}, {"code": "# Add `serialize_method` attribute to the attrs that MaskedNDArrayInfo knows\n    # about.  This allows customization of the way that MaskedColumn objects\n    # get written to file depending on format.  The default is to use whatever\n    # the writer would normally do, which in the case of FITS or ECSV is to use\n    # a NULL value within the data itself.  If serialize_method is 'data_mask'\n    # then the mask is explicitly written out as a separate column if there\n    # are any masked values.  This is the same as for MaskedColumn.\n    attr_names = ParentDtypeInfo.attr_names | {\"serialize_method\"}\n\n    # When `serialize_method` is 'data_mask', and data and mask are being written\n    # as separate columns, use column names <name> and <name>.mask (instead\n    # of default encoding as <name>.data and <name>.mask).\n    _represent_as_dict_primary_data = \"data\"\n\n    def _represent_as_dict(self):\n        out = super()._represent_as_dict()\n\n        masked_array = self._parent\n\n        # If the serialize method for this context (e.g. 'fits' or 'ecsv') is\n        # 'data_mask', that means to serialize using an explicit mask column.", "metadata": {"file_name": "astropy/utils/masked/core.py", "File Name": "astropy/utils/masked/core.py", "Classes": "Masked, MaskedInfoBase, MaskedNDArrayInfo, MaskedArraySubclassInfo, MaskedIterator, MaskedNDArray, MaskedRecarray", "Functions": "_comparison_method, _compare, argmin, argmax, argmin, argmax"}}, {"code": "data = deepcopy(data)\n            mask = deepcopy(mask)\n            wcs = deepcopy(wcs)\n            psf = deepcopy(psf)\n            meta = deepcopy(meta)\n            uncertainty = deepcopy(uncertainty)\n            # Actually - copying the unit is unnecessary but better safe\n            # than sorry :-)\n            unit = deepcopy(unit)\n\n        # Store the attributes\n        self._data = data\n        self.mask = mask\n        self._wcs = None\n        if wcs is not None:\n            # Validate the wcs\n            self.wcs = wcs\n        self.meta = meta  # TODO: Make this call the setter sometime\n        self._unit = unit\n        # Call the setter for uncertainty to further check the uncertainty\n        self.uncertainty = uncertainty\n        self.psf = psf\n\n    def __str__(self):\n        data = str(self.data)\n        unit = f\" {self.unit}\" if self.unit is not None else \"\"\n\n        return data + unit\n\n    def __repr__(self):\n        prefix = self.__class__.__name__ + \"(\"\n        data = np.array2string(self.data, separator=\", \", prefix=prefix)\n        unit = f\", unit='{self.unit}'\" if self.unit is not None else \"\"\n\n        return f\"{prefix}{data}{unit})\"\n\n    @property\n    def data(self):\n        \"\"\"\n        `~numpy.ndarray`-like : The stored dataset.\n        \"\"\"\n        return self._data\n\n    @property\n    def mask(self):\n        \"\"\"\n        any type : Mask for the dataset, if any.\n\n        Masks should follow the ``numpy`` convention that valid data points are\n        marked by ``False`` and invalid ones with ``True``.\n        \"\"\"\n        return self._mask\n\n    @mask.setter\n    def mask(self, value):\n        self._mask = value\n\n    @property\n    def unit(self):\n        \"\"\"\n        `~astropy.units.Unit` : Unit for the dataset, if any.\n        \"\"\"\n        return self._unit\n\n    @property\n    def wcs(self):\n        \"\"\"\n        any type : A world coordinate system (WCS) for the dataset, if any.\n        \"\"\"", "metadata": {"file_name": "astropy/nddata/nddata.py", "File Name": "astropy/nddata/nddata.py", "Classes": "NDData"}}, {"code": "kwargs :\n        Any other parameter that should be passed to the callables used.\n\n    Returns\n    -------\n    result : `~astropy.nddata.NDData`-like\n        The resulting dataset\n\n    Notes\n    -----\n    If a ``callable`` is used for ``mask``, ``wcs`` or ``meta`` the\n    callable must accept the corresponding attributes as first two\n    parameters. If the callable also needs additional parameters these can be\n    defined as ``kwargs`` and must start with ``\"wcs_\"`` (for wcs callable) or\n    ``\"meta_\"`` (for meta callable). This startstring is removed before the\n    callable is called.\n\n    ``\"first_found\"`` can also be abbreviated with ``\"ff\"``.\n    \"\"\"\n\n\nclass NDArithmeticMixin:\n    \"\"\"\n    Mixin class to add arithmetic to an NDData object.\n\n    When subclassing, be sure to list the superclasses in the correct order\n    so that the subclass sees NDData as the main superclass. See\n    `~astropy.nddata.NDDataArray` for an example.\n\n    Notes\n    -----\n    This class only aims at covering the most common cases so there are certain\n    restrictions on the saved attributes::\n\n        - ``uncertainty`` : has to be something that has a `NDUncertainty`-like\n          interface for uncertainty propagation\n        - ``mask`` : has to be something that can be used by a bitwise ``or``\n          operation.\n        - ``wcs`` : has to implement a way of comparing with ``=`` to allow\n          the operation.\n\n    But there is a workaround that allows to disable handling a specific\n    attribute and to simply set the results attribute to ``None`` or to\n    copy the existing attribute (and neglecting the other).\n    For example for uncertainties not representing an `NDUncertainty`-like\n    interface you can alter the ``propagate_uncertainties`` parameter in\n    :meth:`NDArithmeticMixin.add`. ``None`` means that the result will have no\n    uncertainty, ``False`` means it takes the uncertainty of the first operand\n    (if this does not exist from the second operand) as the result's\n    uncertainty.", "metadata": {"file_name": "astropy/nddata/mixins/ndarithmetic.py", "File Name": "astropy/nddata/mixins/ndarithmetic.py", "Classes": "NDArithmeticMixin"}}, {"code": "Bit flipping for string lists of bit flags must be\n        specified by prepending '~' to string bit flag lists\n        (see documentation for ``ignore_flags`` for more details).\n\n        .. warning::\n            This parameter can be set to either `True` or `False` **ONLY** when\n            ``ignore_flags`` is either an integer bit mask or a Python\n            list of integer bit flags. When ``ignore_flags`` is either\n            `None` or a string list of flags, ``flip_bits`` **MUST** be set\n            to `None`.\n\n    good_mask_value : int, bool (default = False)\n        This parameter is used to derive the values that will be assigned to\n        the elements in the output boolean mask array that correspond to the\n        \"good\" bit fields (that are 0 after zeroing bits specified by\n        ``ignore_flags``) in the input ``bitfield`` array. When\n        ``good_mask_value`` is non-zero or ``numpy.True_`` then values in the\n        output boolean mask array corresponding to \"good\" bit fields in\n        ``bitfield`` will be ``numpy.True_`` (if ``dtype`` is ``numpy.bool_``)\n        or 1 (if ``dtype`` is of numerical type) and values of corresponding\n        to \"bad\" flags will be ``numpy.False_`` (or 0). When\n        ``good_mask_value`` is zero or ``numpy.False_`` then the values\n        in the output boolean mask array corresponding to \"good\" bit fields\n        in ``bitfield`` will be ``numpy.False_`` (if ``dtype`` is\n        ``numpy.bool_``) or 0 (if ``dtype`` is of numerical type) and values\n        of corresponding to \"bad\" flags will be ``numpy.True_`` (or 1).\n\n    dtype : data-type (default = ``numpy.bool_``)\n        The desired data-type for the output binary mask array.", "metadata": {"file_name": "astropy/nddata/bitmask.py", "File Name": "astropy/nddata/bitmask.py", "Classes": "InvalidBitFlag, BitFlag, BitFlagNameMeta, BitFlagNameMap", "Functions": "_is_bit_flag, _is_int, extend_bit_flag_map, interpret_bit_flags, bitfield_to_boolean_mask"}}, {"code": "n2 = len(funclist)\n    # undocumented: single condition is promoted to a list of one condition\n    if np.isscalar(condlist) or (\n        not isinstance(condlist[0], (list, np.ndarray)) and x.ndim != 0\n    ):  # pragma: no cover\n        condlist = [condlist]\n\n    condlist = np.array(condlist, dtype=bool)\n    n = len(condlist)\n\n    if n == n2 - 1:  # compute the \"otherwise\" condition.\n        condelse = ~np.any(condlist, axis=0, keepdims=True)\n        condlist = np.concatenate([condlist, condelse], axis=0)\n        n += 1\n    elif n != n2:\n        raise ValueError(\n            f\"with {n} condition(s), either {n} or {n + 1} functions are expected\"\n        )\n\n    # The one real change...\n    y = np.zeros_like(x)\n    where = []\n    what = []\n    for k in range(n):\n        item = funclist[k]\n        if not callable(item):\n            where.append(condlist[k])\n            what.append(item)\n        else:\n            vals = x[condlist[k]]\n            if vals.size > 0:\n                where.append(condlist[k])\n                what.append(item(vals, *args, **kw))\n\n    for item, value in zip(where, what):\n        y[item] = value\n\n    return y", "metadata": {"file_name": "astropy/utils/masked/function_helpers.py", "File Name": "astropy/utils/masked/function_helpers.py", "Classes": "MaskedFormat", "Functions": "_get_data_and_masks, datetime_as_string, sinc, iscomplex, unwrap, nan_to_num, masked_a_helper, masked_m_helper, masked_v_helper, masked_arr_helper, broadcast_to, outer, empty_like, zeros_like, ones_like, full_like, put, putmask, place, copyto, packbits, unpackbits, bincount, msort, sort_complex, concatenate, append, block, broadcast_arrays, insert, count_nonzero, _masked_median_1d, _masked_median, median, _masked_quantile_1d, _masked_quantile, quantile, percentile, array_equal, array_equiv, where, choose, select, piecewise, interp, lexsort, apply_over_axes, _array2string, array2string, array_str, masked_nanfunc, nanfunc"}}, {"code": "@dispatched_function\ndef packbits(a, *args, **kwargs):\n    result = np.packbits(a.unmasked, *args, **kwargs)\n    mask = np.packbits(a.mask, *args, **kwargs).astype(bool)\n    return result, mask, None\n\n\n@dispatched_function\ndef unpackbits(a, *args, **kwargs):\n    result = np.unpackbits(a.unmasked, *args, **kwargs)\n    mask = np.zeros(a.shape, dtype=\"u1\")\n    mask[a.mask] = 255\n    mask = np.unpackbits(mask, *args, **kwargs).astype(bool)\n    return result, mask, None", "metadata": {"file_name": "astropy/utils/masked/function_helpers.py", "File Name": "astropy/utils/masked/function_helpers.py", "Classes": "MaskedFormat", "Functions": "_get_data_and_masks, datetime_as_string, sinc, iscomplex, unwrap, nan_to_num, masked_a_helper, masked_m_helper, masked_v_helper, masked_arr_helper, broadcast_to, outer, empty_like, zeros_like, ones_like, full_like, put, putmask, place, copyto, packbits, unpackbits, bincount, msort, sort_complex, concatenate, append, block, broadcast_arrays, insert, count_nonzero, _masked_median_1d, _masked_median, median, _masked_quantile_1d, _masked_quantile, quantile, percentile, array_equal, array_equiv, where, choose, select, piecewise, interp, lexsort, apply_over_axes, _array2string, array2string, array_str, masked_nanfunc, nanfunc"}}, {"code": "if any(issubclass(t, np.ndarray) and not issubclass(t, Masked) for t in types):\n            raise TypeError(\n                \"the MaskedNDArray implementation cannot handle {} \"\n                \"with the given arguments.\".format(function)\n            ) from None\n        else:\n            return NotImplemented\n\n    def _masked_result(self, result, mask, out):\n        if isinstance(result, tuple):\n            if out is None:\n                out = (None,) * len(result)\n            if not isinstance(mask, (list, tuple)):\n                mask = (mask,) * len(result)\n            return tuple(\n                self._masked_result(result_, mask_, out_)\n                for (result_, mask_, out_) in zip(result, mask, out)\n            )\n\n        if out is None:\n            # Note that we cannot count on result being the same class as\n            # 'self' (e.g., comparison of quantity results in an ndarray, most\n            # operations on Longitude and Latitude result in Angle or\n            # Quantity), so use Masked to determine the appropriate class.\n            return Masked(result, mask)\n\n        # TODO: remove this sanity check once test cases are more complete.\n        assert isinstance(out, Masked)\n        # If we have an output, the result was written in-place, so we should\n        # also write the mask in-place (if not done already in the code).\n        if out._mask is not mask:\n            out._mask[...] = mask\n        return out\n\n    # Below are ndarray methods that need to be overridden as masked elements\n    # need to be skipped and/or an initial value needs to be set.\n    def _reduce_defaults(self, kwargs, initial_func=None):\n        \"\"\"Get default where and initial for masked reductions.\n\n        Generally, the default should be to skip all masked elements.  For\n        reductions such as np.minimum.reduce, we also need an initial value,\n        which can be determined using ``initial_func``.\n\n        \"\"\"", "metadata": {"file_name": "astropy/utils/masked/core.py", "File Name": "astropy/utils/masked/core.py", "Classes": "Masked, MaskedInfoBase, MaskedNDArrayInfo, MaskedArraySubclassInfo, MaskedIterator, MaskedNDArray, MaskedRecarray", "Functions": "_comparison_method, _compare, argmin, argmax, argmin, argmax"}}, {"code": ">>> ndd3.uncertainty.array  # doctest: +FLOAT_CMP\n        array([[1.41421356, 1.41421356, 1.41421356],\n               [1.41421356, 1.41421356, 1.41421356],\n               [1.41421356, 1.41421356, 1.41421356]])\n\n    see `NDArithmeticMixin` for a complete list of all supported arithmetic\n    operations.\n\n    But also slicing (indexing) is possible::\n\n        >>> ndd4 = ndd3[1,:]\n        >>> ndd4.data  # doctest: +FLOAT_CMP\n        array([2., 2., 2.])\n        >>> ndd4.uncertainty.array  # doctest: +FLOAT_CMP\n        array([1.41421356, 1.41421356, 1.41421356])\n\n    See `NDSlicingMixin` for a description how slicing works (which attributes)\n    are sliced.\n    \"\"\"\n\n    pass", "metadata": {"file_name": "astropy/nddata/nddata_withmixins.py", "File Name": "astropy/nddata/nddata_withmixins.py", "Classes": "NDDataRef"}}, {"code": "# Licensed under a 3-clause BSD style license - see LICENSE.rst\n\"\"\"\nBuilt-in mask mixin class.\n\nThe design uses `Masked` as a factory class which automatically\ngenerates new subclasses for any data class that is itself a\nsubclass of a predefined masked class, with `MaskedNDArray`\nproviding such a predefined class for `~numpy.ndarray`.\n\nGenerally, any new predefined class should override the\n``from_unmasked(data, mask, copy=False)`` class method that\ncreates an instance from unmasked data and a mask, as well as\nthe ``unmasked`` property that returns just the data.\nThe `Masked` class itself provides a base ``mask`` property,\nwhich can also be overridden if needed.\n\n\"\"\"\nimport builtins\n\nimport numpy as np\n\nfrom astropy.utils.compat import NUMPY_LT_1_22\nfrom astropy.utils.data_info import ParentDtypeInfo\nfrom astropy.utils.shapes import NDArrayShapeMethods\n\nfrom .function_helpers import (\n    APPLY_TO_BOTH_FUNCTIONS,\n    DISPATCHED_FUNCTIONS,\n    MASKED_SAFE_FUNCTIONS,\n    UNSUPPORTED_FUNCTIONS,\n)\n\n__all__ = [\"Masked\", \"MaskedNDArray\"]\n\n\nget__doc__ = \"\"\"Masked version of {0.__name__}.\n\nExcept for the ability to pass in a ``mask``, parameters are\nas for `{0.__module__}.{0.__name__}`.\n\"\"\".format\n\n\nclass Masked(NDArrayShapeMethods):\n    \"\"\"A scalar value or array of values with associated mask.\n\n    The resulting instance will take its exact type from whatever the\n    contents are, with the type generated on the fly as needed.\n\n    Parameters\n    ----------\n    data : array-like\n        The data for which a mask is to be added.  The result will be a\n        a subclass of the type of ``data``.\n    mask : array-like of bool, optional\n        The initial mask to assign.  If not given, taken from the data.\n    copy : bool\n        Whether the data and mask should be copied. Default: `False`.\n\n    \"\"\"", "metadata": {"file_name": "astropy/utils/masked/core.py", "File Name": "astropy/utils/masked/core.py", "Classes": "Masked, MaskedInfoBase, MaskedNDArrayInfo, MaskedArraySubclassInfo, MaskedIterator, MaskedNDArray, MaskedRecarray", "Functions": "_comparison_method, _compare, argmin, argmax, argmin, argmax"}}, {"code": "See Also\n        --------\n        astropy.utils.masked.Masked.unmasked\n        \"\"\"\n        unmasked = self.unmasked.copy()\n        if self.mask.dtype.names:\n            np.ma.core._recursive_filled(unmasked, self.mask, fill_value)\n        else:\n            unmasked[self.mask] = fill_value\n\n        return unmasked\n\n    def _apply(self, method, *args, **kwargs):\n        # Required method for NDArrayShapeMethods, to help provide __getitem__\n        # and shape-changing methods.\n        if callable(method):\n            data = method(self.unmasked, *args, **kwargs)\n            mask = method(self.mask, *args, **kwargs)\n        else:\n            data = getattr(self.unmasked, method)(*args, **kwargs)\n            mask = getattr(self.mask, method)(*args, **kwargs)\n\n        result = self.from_unmasked(data, mask, copy=False)\n        if \"info\" in self.__dict__:\n            result.info = self.info\n\n        return result\n\n    def __setitem__(self, item, value):\n        value, mask = self._get_data_and_mask(value, allow_ma_masked=True)\n        if value is not None:\n            self.unmasked[item] = value\n        self.mask[item] = mask\n\n\nclass MaskedInfoBase:\n    mask_val = np.ma.masked\n\n    def __init__(self, bound=False):\n        super().__init__(bound)\n\n        # If bound to a data object instance then create the dict of attributes\n        # which stores the info attribute values.\n        if bound:\n            # Specify how to serialize this object depending on context.\n            self.serialize_method = {\n                \"fits\": \"null_value\",\n                \"ecsv\": \"null_value\",\n                \"hdf5\": \"data_mask\",\n                \"parquet\": \"data_mask\",\n                None: \"null_value\",\n            }\n\n\nclass MaskedNDArrayInfo(MaskedInfoBase, ParentDtypeInfo):\n    \"\"\"\n    Container for meta information like name, description, format.\n    \"\"\"", "metadata": {"file_name": "astropy/utils/masked/core.py", "File Name": "astropy/utils/masked/core.py", "Classes": "Masked, MaskedInfoBase, MaskedNDArrayInfo, MaskedArraySubclassInfo, MaskedIterator, MaskedNDArray, MaskedRecarray", "Functions": "_comparison_method, _compare, argmin, argmax, argmin, argmax"}}, {"code": "Default is `numpy.logical_or`.\n\n        .. versionadded:: 1.2\n\n    handle_meta : callable, ``'first_found'`` or ``None``, optional\n        If ``None`` the result will have no meta. If ``'first_found'`` the\n        result will have a copied version of the first operand that has a\n        (not empty) meta. If it is a callable then the specified callable must\n        create the results ``meta`` and if necessary provide a copy.\n        Default is ``None``.\n\n        .. versionadded:: 1.2\n\n    compare_wcs : callable, ``'first_found'`` or ``None``, optional\n        If ``None`` the result will have no wcs and no comparison between\n        the wcs of the operands is made. If ``'first_found'`` the\n        result will have a copied version of the first operand that has a\n        wcs. If it is a callable then the specified callable must\n        compare the ``wcs``. The resulting ``wcs`` will be like if ``False``\n        was given otherwise it raises a ``ValueError`` if the comparison was\n        not successful. Default is ``'first_found'``.\n\n        .. versionadded:: 1.2\n\n    uncertainty_correlation : number or `~numpy.ndarray`, optional\n        The correlation between the two operands is used for correct error\n        propagation for correlated data as given in:\n        https://en.wikipedia.org/wiki/Propagation_of_uncertainty#Example_formulas\n        Default is 0.\n\n        .. versionadded:: 1.2", "metadata": {"file_name": "astropy/nddata/mixins/ndarithmetic.py", "File Name": "astropy/nddata/mixins/ndarithmetic.py", "Classes": "NDArithmeticMixin"}}, {"code": "# FIXME: Python interns some values, for example the\n                        # integers from -5 to 255 (any maybe some other types\n                        # as well). In that case the default is\n                        # indistinguishable from an explicitly passed kwarg\n                        # and it won't notice that and use the attribute of the\n                        # NDData.\n                        if propmatch in func_args or (\n                            propmatch in func_kwargs\n                            and (\n                                bound_args.arguments[propmatch]\n                                is not sig[propmatch].default\n                            )\n                        ):\n                            warnings.warn(\n                                \"Property {} has been passed explicitly and \"\n                                \"as an NDData property{}, using explicitly \"\n                                \"specified value\"\n                                \"\".format(\n                                    propmatch, \"\" if prop == propmatch else \" \" + prop\n                                ),\n                                AstropyUserWarning,\n                            )\n                            continue\n                    # Otherwise use the property as input for the function.\n                    kwargs[propmatch] = value\n                # Finally, replace data by the data attribute\n                data = data.data\n\n                if ignored:\n                    warnings.warn(\n                        \"The following attributes were set on the \"\n                        \"data object, but will be ignored by the \"\n                        \"function: \" + \", \".join(ignored),\n                        AstropyUserWarning,\n                    )\n\n            result = func(data, *args, **kwargs)\n\n            if unpack and repack:\n                # If there are multiple required returned arguments make sure\n                # the result is a tuple (because we don't want to unpack\n                # numpy arrays or compare their length, never!) and has the\n                # same length.\n                if len(returns) > 1:\n                    if not isinstance(result, tuple) or len(returns) != len(result):\n                        raise ValueError(\n                            \"Function did not return the expected number of arguments.\"", "metadata": {"file_name": "astropy/nddata/decorators.py", "File Name": "astropy/nddata/decorators.py", "Functions": "support_nddata, support_nddata_decorator, wrapper"}}, {"code": "@dispatched_function\ndef sort_complex(a):\n    # Just a copy of function_base.sort_complex, to avoid the asarray.\n    b = a.copy()\n    b.sort()\n    if not issubclass(b.dtype.type, np.complexfloating):  # pragma: no cover\n        if b.dtype.char in \"bhBH\":\n            return b.astype(\"F\")\n        elif b.dtype.char == \"g\":\n            return b.astype(\"G\")\n        else:\n            return b.astype(\"D\")\n    else:\n        return b\n\n\n@dispatched_function\ndef concatenate(arrays, axis=0, out=None, dtype=None, casting=\"same_kind\"):\n    data, masks = _get_data_and_masks(*arrays)\n    if out is None:\n        return (\n            np.concatenate(data, axis=axis, dtype=dtype, casting=casting),\n            np.concatenate(masks, axis=axis),\n            None,\n        )\n    else:\n        from astropy.utils.masked import Masked\n\n        if not isinstance(out, Masked):\n            raise NotImplementedError\n        np.concatenate(masks, out=out.mask, axis=axis)\n        np.concatenate(data, out=out.unmasked, axis=axis, dtype=dtype, casting=casting)\n        return out\n\n\n@apply_to_both\ndef append(arr, values, axis=None):\n    data, masks = _get_data_and_masks(arr, values)\n    return data, masks, dict(axis=axis), None", "metadata": {"file_name": "astropy/utils/masked/function_helpers.py", "File Name": "astropy/utils/masked/function_helpers.py", "Classes": "MaskedFormat", "Functions": "_get_data_and_masks, datetime_as_string, sinc, iscomplex, unwrap, nan_to_num, masked_a_helper, masked_m_helper, masked_v_helper, masked_arr_helper, broadcast_to, outer, empty_like, zeros_like, ones_like, full_like, put, putmask, place, copyto, packbits, unpackbits, bincount, msort, sort_complex, concatenate, append, block, broadcast_arrays, insert, count_nonzero, _masked_median_1d, _masked_median, median, _masked_quantile_1d, _masked_quantile, quantile, percentile, array_equal, array_equiv, where, choose, select, piecewise, interp, lexsort, apply_over_axes, _array2string, array2string, array_str, masked_nanfunc, nanfunc"}}, {"code": ")\n                if _ENABLE_BITFLAG_CACHING:\n                    cache[kl] = v\n\n        members = {\n            k: v if k.startswith(\"_\") else BitFlag(v) for k, v in members.items()\n        }\n\n        if _ENABLE_BITFLAG_CACHING:\n            cache.update(\n                {k.lower(): v for k, v in members.items() if not k.startswith(\"_\")}\n            )\n            members = {\"_locked\": True, \"__version__\": \"\", **members, \"_cache\": cache}\n        else:\n            members = {\"_locked\": True, \"__version__\": \"\", **members}\n\n        return super().__new__(mcls, name, bases, members)\n\n    def __setattr__(cls, name, val):\n        if name == \"_locked\":\n            return super().__setattr__(name, True)\n\n        else:\n            if name == \"__version__\":\n                if cls._locked:\n                    raise AttributeError(\"Version cannot be modified.\")\n                return super().__setattr__(name, val)\n\n            err_msg = f\"Bit flags are read-only.", "metadata": {"file_name": "astropy/nddata/bitmask.py", "File Name": "astropy/nddata/bitmask.py", "Classes": "InvalidBitFlag, BitFlag, BitFlagNameMeta, BitFlagNameMap", "Functions": "_is_bit_flag, _is_int, extend_bit_flag_map, interpret_bit_flags, bitfield_to_boolean_mask"}}, {"code": "By default, the ``operation`` will be ignored.\n\n        operand : `NDData`-like instance\n            The second operand wrapped in an instance of the same class as\n            self.\n\n        handle_mask : callable\n            see :meth:`NDArithmeticMixin.add`\n\n        kwds :\n            Additional parameters given to ``handle_mask``.\n\n        Returns\n        -------\n        result_mask : any type\n            If only one mask was present this mask is returned.\n            If neither had a mask ``None`` is returned. Otherwise\n            ``handle_mask`` must create (and copy) the returned mask.\n        \"\"\"\n        # If only one mask is present we need not bother about any type checks\n        if (\n            self.mask is None and operand is not None and operand.mask is None\n        ) or handle_mask is None:\n            return None\n        elif self.mask is None and operand is not None:\n            # Make a copy so there is no reference in the result.\n            return deepcopy(operand.mask)\n        elif operand is None:\n            return deepcopy(self.mask)\n        else:\n            # Now lets calculate the resulting mask (operation enforces copy)\n            return handle_mask(self.mask, operand.mask, **kwds)\n\n    def _arithmetic_wcs(self, operation, operand, compare_wcs, **kwds):\n        \"\"\"\n        Calculate the resulting wcs.\n\n        There is actually no calculation involved but it is a good place to\n        compare wcs information of both operands. This is currently not working\n        properly with `~astropy.wcs.WCS` (which is the suggested class for\n        storing as wcs property) but it will not break it neither.\n\n        Parameters\n        ----------\n        operation : callable\n            see :meth:`NDArithmeticMixin._arithmetic` parameter description.\n            By default, the ``operation`` will be ignored.\n\n        operand : `NDData` instance or subclass\n            The second operand wrapped in an instance of the same class as\n            self.\n\n        compare_wcs : callable\n            see :meth:`NDArithmeticMixin.add` parameter description.\n\n        kwds :\n            Additional parameters given to ``compare_wcs``.\n\n        Raises\n        ------\n        ValueError\n            If ``compare_wcs`` returns ``False``.", "metadata": {"file_name": "astropy/nddata/mixins/ndarithmetic.py", "File Name": "astropy/nddata/mixins/ndarithmetic.py", "Classes": "NDArithmeticMixin"}}, {"code": "\",\n                AstropyUserWarning,\n            )\n\n        if handle_mask is None:\n            pass\n        elif hasattr(result, \"mask\"):\n            # if numpy.ma or astropy.utils.masked is being used, the constructor\n            # will pick up the mask from the masked object:\n            kwargs[\"mask\"] = None\n        elif handle_mask in [\"ff\", \"first_found\"]:\n            if self.mask is None:\n                kwargs[\"mask\"] = deepcopy(operand.mask)\n            else:\n                kwargs[\"mask\"] = deepcopy(self.mask)\n        else:\n            kwargs[\"mask\"] = self._arithmetic_mask(\n                operation, operand, handle_mask, axis=axis, **kwds2[\"mask\"]\n            )\n\n        if handle_meta is None:\n            kwargs[\"meta\"] = None\n        elif handle_meta in [\"ff\", \"first_found\"]:\n            if not self.meta:\n                kwargs[\"meta\"] = deepcopy(operand.meta)\n            else:\n                kwargs[\"meta\"] = deepcopy(self.meta)\n        else:\n            kwargs[\"meta\"] = self._arithmetic_meta(\n                operation, operand, handle_meta, **kwds2[\"meta\"]\n            )\n\n        # Wrap the individual results into a new instance of the same class.\n        return result, kwargs\n\n    def _arithmetic_data(self, operation, operand, **kwds):\n        \"\"\"\n        Calculate the resulting data.\n\n        Parameters\n        ----------\n        operation : callable\n            see `NDArithmeticMixin._arithmetic` parameter description.\n\n        operand : `NDData`-like instance\n            The second operand wrapped in an instance of the same class as\n            self.\n\n        kwds :\n            Additional parameters.\n\n        Returns\n        -------\n        result_data : ndarray or `~astropy.units.Quantity`\n            If both operands had no unit the resulting data is a simple numpy\n            array, but if any of the operands had a unit the return is a\n            Quantity.\n        \"\"\"", "metadata": {"file_name": "astropy/nddata/mixins/ndarithmetic.py", "File Name": "astropy/nddata/mixins/ndarithmetic.py", "Classes": "NDArithmeticMixin"}}, {"code": ".. versionadded:: 1.2\n\n        Parameters\n        ----------\n        operation : callable\n            The operation that is performed on the `NDData`. Supported are\n            `numpy.add`, `numpy.subtract`, `numpy.multiply` and\n            `numpy.true_divide` (or `numpy.divide`).\n\n        other_nddata : `NDData` instance\n            The second operand in the arithmetic operation.\n\n        result_data : `~astropy.units.Quantity` or ndarray\n            The result of the arithmetic operations on the data.\n\n        correlation : `numpy.ndarray` or number\n            The correlation (rho) is defined between the uncertainties in\n            sigma_AB = sigma_A * sigma_B * rho. A value of ``0`` means\n            uncorrelated operands.\n\n        axis : int or tuple of ints, optional\n            Axis over which to perform a collapsing operation.\n\n        Returns\n        -------\n        resulting_uncertainty : `NDUncertainty` instance\n            Another instance of the same `NDUncertainty` subclass containing\n            the uncertainty of the result.\n\n        Raises\n        ------\n        ValueError\n            If the ``operation`` is not supported or if correlation is not zero\n            but the subclass does not support correlated uncertainties.\n\n        Notes\n        -----\n        First this method checks if a correlation is given and the subclass\n        implements propagation with correlated uncertainties.\n        Then the second uncertainty is converted (or an Exception is raised)\n        to the same class in order to do the propagation.\n        Then the appropriate propagation method is invoked and the result is\n        returned.\n        \"\"\"\n        # Check if the subclass supports correlation\n        if not self.supports_correlated:\n            if isinstance(correlation, np.ndarray) or correlation != 0:\n                raise ValueError(\n                    \"{} does not support uncertainty propagation\"\n                    \" with correlation.\"", "metadata": {"file_name": "astropy/nddata/nduncertainty.py", "File Name": "astropy/nddata/nduncertainty.py", "Classes": "IncompatibleUncertaintiesException, MissingDataAssociationException, NDUncertainty, UnknownUncertainty, _VariancePropagationMixin, StdDevUncertainty, VarianceUncertainty, InverseVariance", "Functions": "_move_preserved_axes_first, _unravel_preserved_axes, from_variance_for_mean, _inverse"}}, {"code": "@dispatched_function\ndef block(arrays):\n    # We need to override block since the numpy implementation can take two\n    # different paths, one for concatenation, one for creating a large empty\n    # result array in which parts are set.  Each assumes array input and\n    # cannot be used directly.  Since it would be very costly to inspect all\n    # arrays and then turn them back into a nested list, we just copy here the\n    # second implementation, np.core.shape_base._block_slicing, since it is\n    # shortest and easiest.\n    from astropy.utils.masked import Masked\n\n    arrays, list_ndim, result_ndim, final_size = np.core.shape_base._block_setup(arrays)\n    shape, slices, arrays = np.core.shape_base._block_info_recursion(\n        arrays, list_ndim, result_ndim\n    )\n    dtype = np.result_type(*[arr.dtype for arr in arrays])\n    F_order = all(arr.flags[\"F_CONTIGUOUS\"] for arr in arrays)\n    C_order = all(arr.flags[\"C_CONTIGUOUS\"] for arr in arrays)\n    order = \"F\" if F_order and not C_order else \"C\"\n    result = Masked(np.empty(shape=shape, dtype=dtype, order=order))\n    for the_slice, arr in zip(slices, arrays):\n        result[(Ellipsis,) + the_slice] = arr\n    return result", "metadata": {"file_name": "astropy/utils/masked/function_helpers.py", "File Name": "astropy/utils/masked/function_helpers.py", "Classes": "MaskedFormat", "Functions": "_get_data_and_masks, datetime_as_string, sinc, iscomplex, unwrap, nan_to_num, masked_a_helper, masked_m_helper, masked_v_helper, masked_arr_helper, broadcast_to, outer, empty_like, zeros_like, ones_like, full_like, put, putmask, place, copyto, packbits, unpackbits, bincount, msort, sort_complex, concatenate, append, block, broadcast_arrays, insert, count_nonzero, _masked_median_1d, _masked_median, median, _masked_quantile_1d, _masked_quantile, quantile, percentile, array_equal, array_equiv, where, choose, select, piecewise, interp, lexsort, apply_over_axes, _array2string, array2string, array_str, masked_nanfunc, nanfunc"}}, {"code": "When input is a list of flags (either a Python list of integer flags or a\n    string of comma-, ``'|'``-, or ``'+'``-separated list of flags),\n    the returned bit mask is obtained by summing input flags.\n\n    .. note::\n        In order to flip the bits of the returned bit mask,\n        for input of `str` type, prepend '~' to the input string. '~' must\n        be prepended to the *entire string* and not to each bit flag! For\n        input that is already a bit mask or a Python list of bit flags, set\n        ``flip_bits`` for `True` in order to flip the bits of the returned\n        bit mask.\n\n    Parameters\n    ----------\n    bit_flags : int, str, list, None\n        An integer bit mask or flag, `None`, a string of comma-, ``'|'``- or\n        ``'+'``-separated list of integer bit flags or mnemonic flag names,\n        or a Python list of integer bit flags. If ``bit_flags`` is a `str`\n        and if it is prepended with '~', then the output bit mask will have\n        its bits flipped (compared to simple sum of input flags).\n        For input ``bit_flags`` that is already a bit mask or a Python list\n        of bit flags, bit-flipping can be controlled through ``flip_bits``\n        parameter.\n\n        .. note::\n            When ``bit_flags`` is a list of flag names, the ``flag_name_map``\n            parameter must be provided.\n\n        .. note::\n            Only one flag separator is supported at a time. ``bit_flags``\n            string should not mix ``','``, ``'+'``, and ``'|'`` separators.\n\n    flip_bits : bool, None\n        Indicates whether or not to flip the bits of the returned bit mask\n        obtained from input bit flags. This parameter must be set to `None`\n        when input ``bit_flags`` is either `None` or a Python list of flags.", "metadata": {"file_name": "astropy/nddata/bitmask.py", "File Name": "astropy/nddata/bitmask.py", "Classes": "InvalidBitFlag, BitFlag, BitFlagNameMeta, BitFlagNameMap", "Functions": "_is_bit_flag, _is_int, extend_bit_flag_map, interpret_bit_flags, bitfield_to_boolean_mask"}}, {"code": "def _masked_median(a, axis=None, out=None, overwrite_input=False):\n    # As for np.nanmedian, but without a fast option as yet.\n    if axis is None or a.ndim == 1:\n        part = a.ravel()\n        result = _masked_median_1d(part, overwrite_input)\n    else:\n        result = np.apply_along_axis(_masked_median_1d, axis, a, overwrite_input)\n    if out is not None:\n        out[...] = result\n    return result\n\n\n@dispatched_function\ndef median(a, axis=None, out=None, **kwargs):\n    from astropy.utils.masked import Masked\n\n    if out is not None and not isinstance(out, Masked):\n        raise NotImplementedError\n\n    a = Masked(a)\n\n    if NUMPY_LT_1_24:\n        keepdims = kwargs.pop(\"keepdims\", False)\n        r, k = np.lib.function_base._ureduce(\n            a, func=_masked_median, axis=axis, out=out, **kwargs\n        )\n        return (r.reshape(k) if keepdims else r) if out is None else out\n\n    else:\n        return np.lib.function_base._ureduce(\n            a, func=_masked_median, axis=axis, out=out, **kwargs\n        )", "metadata": {"file_name": "astropy/utils/masked/function_helpers.py", "File Name": "astropy/utils/masked/function_helpers.py", "Classes": "MaskedFormat", "Functions": "_get_data_and_masks, datetime_as_string, sinc, iscomplex, unwrap, nan_to_num, masked_a_helper, masked_m_helper, masked_v_helper, masked_arr_helper, broadcast_to, outer, empty_like, zeros_like, ones_like, full_like, put, putmask, place, copyto, packbits, unpackbits, bincount, msort, sort_complex, concatenate, append, block, broadcast_arrays, insert, count_nonzero, _masked_median_1d, _masked_median, median, _masked_quantile_1d, _masked_quantile, quantile, percentile, array_equal, array_equiv, where, choose, select, piecewise, interp, lexsort, apply_over_axes, _array2string, array2string, array_str, masked_nanfunc, nanfunc"}}, {"code": "@apply_to_both(helps={np.diag, np.diagflat})\ndef masked_v_helper(v, *args, **kwargs):\n    data, mask = _get_data_and_masks(v)\n    return data + args, mask + args, kwargs, None\n\n\n@apply_to_both(helps={np.delete})\ndef masked_arr_helper(array, *args, **kwargs):\n    data, mask = _get_data_and_masks(array)\n    return data + args, mask + args, kwargs, None\n\n\n@apply_to_both\ndef broadcast_to(array, shape, subok=False):\n    \"\"\"Broadcast array to the given shape.\n\n    Like `numpy.broadcast_to`, and applied to both unmasked data and mask.\n    Note that ``subok`` is taken to mean whether or not subclasses of\n    the unmasked data and mask are allowed, i.e., for ``subok=False``,\n    a `~astropy.utils.masked.MaskedNDArray` will be returned.\n    \"\"\"\n    data, mask = _get_data_and_masks(array)\n    return data, mask, dict(shape=shape, subok=subok), None\n\n\n@dispatched_function\ndef outer(a, b, out=None):\n    return np.multiply.outer(np.ravel(a), np.ravel(b), out=out)", "metadata": {"file_name": "astropy/utils/masked/function_helpers.py", "File Name": "astropy/utils/masked/function_helpers.py", "Classes": "MaskedFormat", "Functions": "_get_data_and_masks, datetime_as_string, sinc, iscomplex, unwrap, nan_to_num, masked_a_helper, masked_m_helper, masked_v_helper, masked_arr_helper, broadcast_to, outer, empty_like, zeros_like, ones_like, full_like, put, putmask, place, copyto, packbits, unpackbits, bincount, msort, sort_complex, concatenate, append, block, broadcast_arrays, insert, count_nonzero, _masked_median_1d, _masked_median, median, _masked_quantile_1d, _masked_quantile, quantile, percentile, array_equal, array_equiv, where, choose, select, piecewise, interp, lexsort, apply_over_axes, _array2string, array2string, array_str, masked_nanfunc, nanfunc"}}, {"code": "return self.sort(axis=axis, order=None)\n\n    def cumsum(self, axis=None, dtype=None, out=None):\n        if axis is None:\n            self = self.ravel()\n            axis = 0\n        return np.add.accumulate(self, axis=axis, dtype=dtype, out=out)\n\n    def cumprod(self, axis=None, dtype=None, out=None):\n        if axis is None:\n            self = self.ravel()\n            axis = 0\n        return np.multiply.accumulate(self, axis=axis, dtype=dtype, out=out)\n\n    def clip(self, min=None, max=None, out=None, **kwargs):\n        \"\"\"Return an array whose values are limited to ``[min, max]``.\n\n        Like `~numpy.clip`, but any masked values in ``min`` and ``max``\n        are ignored for clipping.  The mask of the input array is propagated.\n        \"\"\"\n        # TODO: implement this at the ufunc level.\n        dmin, mmin = self._get_data_and_mask(min)\n        dmax, mmax = self._get_data_and_mask(max)\n        if mmin is None and mmax is None:\n            # Fast path for unmasked max, min.\n            return super().clip(min, max, out=out, **kwargs)\n\n        masked_out = np.positive(self, out=out)\n        out = masked_out.unmasked\n        if dmin is not None:\n            np.maximum(out, dmin, out=out, where=True if mmin is None else ~mmin)\n        if dmax is not None:\n            np.minimum(out, dmax, out=out, where=True if mmax is None else ~mmax)\n        return masked_out\n\n    def mean(self, axis=None, dtype=None, out=None, keepdims=False, *, where=True):\n        # Implementation based on that in numpy/core/_methods.py\n        # Cast bool, unsigned int, and int to float64 by default,\n        # and do float16 at higher precision.", "metadata": {"file_name": "astropy/utils/masked/core.py", "File Name": "astropy/utils/masked/core.py", "Classes": "Masked, MaskedInfoBase, MaskedNDArrayInfo, MaskedArraySubclassInfo, MaskedIterator, MaskedNDArray, MaskedRecarray", "Functions": "_comparison_method, _compare, argmin, argmax, argmin, argmax"}}, {"code": "This behavior is also explained in the docstring for the\n    different arithmetic operations.\n\n    Decomposing the units is not attempted, mainly due to the internal mechanics\n    of `~astropy.units.Quantity`, so the resulting data might have units like\n    ``km/m`` if you divided for example 100km by 5m. So this Mixin has adopted\n    this behavior.\n\n    Examples\n    --------\n    Using this Mixin with `~astropy.nddata.NDData`:\n\n        >>> from astropy.nddata import NDData, NDArithmeticMixin\n        >>> class NDDataWithMath(NDArithmeticMixin, NDData):\n        ...     pass\n\n    Using it with one operand on an instance::\n\n        >>> ndd = NDDataWithMath(100)\n        >>> ndd.add(20)\n        NDDataWithMath(120)\n\n    Using it with two operand on an instance::\n\n        >>> ndd = NDDataWithMath(-4)\n        >>> ndd.divide(1, ndd)\n        NDDataWithMath(-0.25)\n\n    Using it as classmethod requires two operands::\n\n        >>> NDDataWithMath.subtract(5, 4)\n        NDDataWithMath(1)\n\n    \"\"\"\n\n    def _arithmetic(\n        self,\n        operation,\n        operand,\n        propagate_uncertainties=True,\n        handle_mask=np.logical_or,\n        handle_meta=None,\n        uncertainty_correlation=0,\n        compare_wcs=\"first_found\",\n        operation_ignores_mask=False,\n        axis=None,\n        **kwds,\n    ):\n        \"\"\"\n        Base method which calculates the result of the arithmetic operation.\n\n        This method determines the result of the arithmetic operation on the\n        ``data`` including their units and then forwards to other methods\n        to calculate the other properties for the result (like uncertainty).\n\n        Parameters\n        ----------\n        operation : callable\n            The operation that is performed on the `NDData`. Supported are\n            `numpy.add`, `numpy.subtract`, `numpy.multiply` and\n            `numpy.true_divide`.", "metadata": {"file_name": "astropy/nddata/mixins/ndarithmetic.py", "File Name": "astropy/nddata/mixins/ndarithmetic.py", "Classes": "NDArithmeticMixin"}}, {"code": "The `dict` is keyed by the numpy function and the values are functions\nthat take the input arguments of the numpy function and organize these\nfor passing the data and mask to the numpy function.\n\nReturns\n-------\ndata_args : tuple\n    Arguments to pass on to the numpy function for the unmasked data.\nmask_args : tuple\n    Arguments to pass on to the numpy function for the masked data.\nkwargs : dict\n    Keyword arguments to pass on for both unmasked data and mask.\nout : `~astropy.utils.masked.Masked` instance or None\n    Optional instance in which to store the output.\n\nRaises\n------\nNotImplementedError\n   When an arguments is masked when it should not be or vice versa.\n\"\"\"\n\nDISPATCHED_FUNCTIONS = {}\n\"\"\"Dict of functions that provide the numpy function's functionality.\n\nThese are for more complicated versions where the numpy function itself\ncannot easily be used.  It should return either the result of the\nfunction, or a tuple consisting of the unmasked result, the mask for the\nresult and a possible output instance.\n\nIt should raise `NotImplementedError` if one of the arguments is masked\nwhen it should not be or vice versa.\n\"\"\"\n\nUNSUPPORTED_FUNCTIONS = set()\n\"\"\"Set of numpy functions that are not supported for masked arrays.\n\nFor most, masked input simply makes no sense, but for others it may have\nbeen lack of time.  Issues or PRs for support for functions are welcome.\n\"\"\"\n\n# Almost all from np.core.fromnumeric defer to methods so are OK.", "metadata": {"file_name": "astropy/utils/masked/function_helpers.py", "File Name": "astropy/utils/masked/function_helpers.py", "Classes": "MaskedFormat", "Functions": "_get_data_and_masks, datetime_as_string, sinc, iscomplex, unwrap, nan_to_num, masked_a_helper, masked_m_helper, masked_v_helper, masked_arr_helper, broadcast_to, outer, empty_like, zeros_like, ones_like, full_like, put, putmask, place, copyto, packbits, unpackbits, bincount, msort, sort_complex, concatenate, append, block, broadcast_arrays, insert, count_nonzero, _masked_median_1d, _masked_median, median, _masked_quantile_1d, _masked_quantile, quantile, percentile, array_equal, array_equiv, where, choose, select, piecewise, interp, lexsort, apply_over_axes, _array2string, array2string, array_str, masked_nanfunc, nanfunc"}}, {"code": "if NUMPY_LT_1_23:\n    IGNORED_FUNCTIONS |= {\n        # Deprecated, removed in numpy 1.23\n        np.asscalar,\n        np.alen,\n    }\n\n# Explicitly unsupported functions\nUNSUPPORTED_FUNCTIONS |= {\n    np.unravel_index,\n    np.ravel_multi_index,\n    np.ix_,\n}\n\n# No support for the functions also not supported by Quantity\n# (io, polynomial, etc.).\nUNSUPPORTED_FUNCTIONS |= IGNORED_FUNCTIONS\n\n\napply_to_both = FunctionAssigner(APPLY_TO_BOTH_FUNCTIONS)\ndispatched_function = FunctionAssigner(DISPATCHED_FUNCTIONS)\n\n\ndef _get_data_and_masks(*args):\n    \"\"\"Separate out arguments into tuples of data and masks.\n\n    An all-False mask is created if an argument does not have a mask.\n    \"\"\"\n    from .core import Masked\n\n    data, masks = Masked._get_data_and_masks(*args)\n    masks = tuple(\n        m if m is not None else np.zeros(np.shape(d), bool) for d, m in zip(data, masks)\n    )\n    return data, masks\n\n\n# Following are simple ufunc-like functions which should just copy the mask.\n@dispatched_function\ndef datetime_as_string(arr, *args, **kwargs):\n    return (np.datetime_as_string(arr.unmasked, *args, **kwargs), arr.mask.copy(), None)\n\n\n@dispatched_function\ndef sinc(x):\n    return np.sinc(x.unmasked), x.mask.copy(), None", "metadata": {"file_name": "astropy/utils/masked/function_helpers.py", "File Name": "astropy/utils/masked/function_helpers.py", "Classes": "MaskedFormat", "Functions": "_get_data_and_masks, datetime_as_string, sinc, iscomplex, unwrap, nan_to_num, masked_a_helper, masked_m_helper, masked_v_helper, masked_arr_helper, broadcast_to, outer, empty_like, zeros_like, ones_like, full_like, put, putmask, place, copyto, packbits, unpackbits, bincount, msort, sort_complex, concatenate, append, block, broadcast_arrays, insert, count_nonzero, _masked_median_1d, _masked_median, median, _masked_quantile_1d, _masked_quantile, quantile, percentile, array_equal, array_equiv, where, choose, select, piecewise, interp, lexsort, apply_over_axes, _array2string, array2string, array_str, masked_nanfunc, nanfunc"}}, {"code": "# Licensed under a 3-clause BSD style license - see LICENSE.rst\n\"\"\"\nThis is a collection of monkey patches and workarounds for bugs in\nearlier versions of Numpy.\n\"\"\"\n\nimport numpy as np\n\nfrom astropy.utils import minversion\n\n__all__ = [\n    \"NUMPY_LT_1_21_1\",\n    \"NUMPY_LT_1_22\",\n    \"NUMPY_LT_1_22_1\",\n    \"NUMPY_LT_1_23\",\n    \"NUMPY_LT_1_24\",\n    \"NUMPY_LT_1_25\",\n]\n\n# TODO: It might also be nice to have aliases to these named for specific\n# features/bugs we're checking for (ex:\n# astropy.table.table._BROKEN_UNICODE_TABLE_SORT)\nNUMPY_LT_1_21_1 = not minversion(np, \"1.21.1\")\nNUMPY_LT_1_22 = not minversion(np, \"1.22\")\nNUMPY_LT_1_22_1 = not minversion(np, \"1.22.1\")\nNUMPY_LT_1_23 = not minversion(np, \"1.23\")\nNUMPY_LT_1_24 = not minversion(np, \"1.24\")\nNUMPY_LT_1_25 = not minversion(np, \"1.25.0.dev0+151\")", "metadata": {"file_name": "astropy/utils/compat/numpycompat.py", "File Name": "astropy/utils/compat/numpycompat.py"}}, {"code": "def _unravel_preserved_axes(arr, collapsed_arr, preserve_axes):\n    # After reshaping an array with _move_preserved_axes_first and collapsing\n    # the result, convert the reshaped first axis back into the shape of each\n    # of the original preserved axes.\n    # For example, if arr.shape == (6, 5, 4, 3, 2) and we're preserving axes (1, 2),\n    # then the output of _move_preserved_axes_first should have shape (20, 6, 3, 2).\n    # This method unravels the first axis in the output *after* a collapse, so the\n    # output with shape (20,) becomes shape (5, 4).\n    if collapsed_arr.ndim != len(preserve_axes):\n        arr_shape = np.array(arr.shape)\n        return collapsed_arr.reshape(arr_shape[np.asarray(preserve_axes)])\n    return collapsed_arr\n\n\ndef from_variance_for_mean(x, axis):\n    if axis is None:\n        # do operation on all dimensions:\n        denom = np.ma.count(x)\n    else:\n        denom = np.ma.count(x, axis)\n    return np.sqrt(np.ma.sum(x, axis)) / denom\n\n\n# mapping from collapsing operations to the complementary methods used for `from_variance`\ncollapse_from_variance_mapping = {\n    np.sum: lambda x, axis: np.sqrt(np.ma.sum(x, axis)),\n    np.mean: from_variance_for_mean,\n    np.median: None,\n}\n\n\nclass IncompatibleUncertaintiesException(Exception):\n    \"\"\"This exception should be used to indicate cases in which uncertainties\n    with two different classes can not be propagated.\n    \"\"\"\n\n\nclass MissingDataAssociationException(Exception):\n    \"\"\"This exception should be used to indicate that an uncertainty instance\n    has not been associated with a parent `~astropy.nddata.NDData` object.\n    \"\"\"\n\n\nclass NDUncertainty(metaclass=ABCMeta):\n    \"\"\"This is the metaclass for uncertainty classes used with `NDData`.", "metadata": {"file_name": "astropy/nddata/nduncertainty.py", "File Name": "astropy/nddata/nduncertainty.py", "Classes": "IncompatibleUncertaintiesException, MissingDataAssociationException, NDUncertainty, UnknownUncertainty, _VariancePropagationMixin, StdDevUncertainty, VarianceUncertainty, InverseVariance", "Functions": "_move_preserved_axes_first, _unravel_preserved_axes, from_variance_for_mean, _inverse"}}, {"code": "Defaults to ``None``.\n\n    wcs : any type, optional\n        World coordinate system (WCS) for the dataset.\n        Default is ``None``.\n\n    meta : `dict`-like object, optional\n        Additional meta information about the dataset. If no meta is provided\n        an empty `collections.OrderedDict` is created.\n        Default is ``None``.\n\n    unit : unit-like, optional\n        Unit for the dataset. Strings that can be converted to a\n        `~astropy.units.Unit` are allowed.\n        Default is ``None``.\n\n    copy : `bool`, optional\n        Indicates whether to save the arguments as copy. ``True`` copies\n        every attribute before saving it while ``False`` tries to save every\n        parameter as reference.\n        Note however that it is not always possible to save the input as\n        reference.\n        Default is ``False``.\n\n        .. versionadded:: 1.2\n\n    psf : `numpy.ndarray` or None, optional\n        Image representation of the PSF. In order for convolution to be flux-\n        preserving, this should generally be normalized to sum to unity.\n\n    Raises\n    ------\n    TypeError\n        In case ``data`` or ``meta`` don't meet the restrictions.\n\n    Notes\n    -----\n    Each attribute can be accessed through the homonymous instance attribute:\n    ``data`` in a `NDData` object can be accessed through the `data`\n    attribute::\n\n        >>> from astropy.nddata import NDData\n        >>> nd = NDData([1,2,3])\n        >>> nd.data\n        array([1, 2, 3])\n\n    Given a conflicting implicit and an explicit parameter during\n    initialization, for example the ``data`` is a `~astropy.units.Quantity` and\n    the unit parameter is not ``None``, then the implicit parameter is replaced\n    (without conversion) by the explicit one and a warning is issued::\n\n        >>> import numpy as np\n        >>> import astropy.units as u\n        >>> q = np.array([1,2,3,4]) * u.m\n        >>> nd2 = NDData(q, unit=u.cm)\n        INFO: overwriting Quantity's current unit with specified unit.", "metadata": {"file_name": "astropy/nddata/nddata.py", "File Name": "astropy/nddata/nddata.py", "Classes": "NDData"}}, {"code": "Parameters\n        ----------\n        numpy_op : function\n            Numpy operation like `np.sum` or `np.max` to use in the collapse\n\n        subtract : bool, optional\n            If ``True``, propagate for subtraction, otherwise propagate for\n            addition.\n\n        axis : tuple, optional\n            Axis on which to compute collapsing operations.\n        \"\"\"\n        try:\n            result_unit_sq = self.parent_nddata.unit**2\n        except (AttributeError, TypeError):\n            result_unit_sq = None\n\n        if self.array is not None:\n            # Formula: sigma**2 = dA\n\n            if numpy_op in [np.min, np.max]:\n                # Find the indices of the min/max in parent data along each axis,\n                # return the uncertainty at the corresponding entry:\n                return self._get_err_at_extremum(numpy_op, axis=axis)\n\n            # np.sum and np.mean operations use similar pattern\n            # to `_propagate_add_sub`, for example:\n            else:\n                # lookup the mapping for to_variance and from_variance for this\n                # numpy operation:\n                to_variance = collapse_to_variance_mapping[numpy_op]\n                from_variance = collapse_from_variance_mapping[numpy_op]\n                masked_uncertainty = np.ma.masked_array(\n                    self.array, self.parent_nddata.mask\n                )\n                if (\n                    self.unit is not None\n                    and to_variance(self.unit) != self.parent_nddata.unit**2\n                ):\n                    # If the uncertainty has a different unit than the result we\n                    # need to convert it to the results unit.", "metadata": {"file_name": "astropy/nddata/nduncertainty.py", "File Name": "astropy/nddata/nduncertainty.py", "Classes": "IncompatibleUncertaintiesException, MissingDataAssociationException, NDUncertainty, UnknownUncertainty, _VariancePropagationMixin, StdDevUncertainty, VarianceUncertainty, InverseVariance", "Functions": "_move_preserved_axes_first, _unravel_preserved_axes, from_variance_for_mean, _inverse"}}, {"code": "@dispatched_function\ndef quantile(a, q, axis=None, out=None, **kwargs):\n    from astropy.utils.masked import Masked\n\n    if isinstance(q, Masked) or out is not None and not isinstance(out, Masked):\n        raise NotImplementedError\n\n    a = Masked(a)\n    q = np.asanyarray(q)\n    if not np.lib.function_base._quantile_is_valid(q):\n        raise ValueError(\"Quantiles must be in the range [0, 1]\")\n\n    if NUMPY_LT_1_24:\n        keepdims = kwargs.pop(\"keepdims\", False)\n        r, k = np.lib.function_base._ureduce(\n            a, func=_masked_quantile, q=q, axis=axis, out=out, **kwargs\n        )\n        return (r.reshape(q.shape + k) if keepdims else r) if out is None else out\n    else:\n        return np.lib.function_base._ureduce(\n            a, func=_masked_quantile, q=q, axis=axis, out=out, **kwargs\n        )\n\n\n@dispatched_function\ndef percentile(a, q, *args, **kwargs):\n    q = np.true_divide(q, 100)\n    return quantile(a, q, *args, **kwargs)", "metadata": {"file_name": "astropy/utils/masked/function_helpers.py", "File Name": "astropy/utils/masked/function_helpers.py", "Classes": "MaskedFormat", "Functions": "_get_data_and_masks, datetime_as_string, sinc, iscomplex, unwrap, nan_to_num, masked_a_helper, masked_m_helper, masked_v_helper, masked_arr_helper, broadcast_to, outer, empty_like, zeros_like, ones_like, full_like, put, putmask, place, copyto, packbits, unpackbits, bincount, msort, sort_complex, concatenate, append, block, broadcast_arrays, insert, count_nonzero, _masked_median_1d, _masked_median, median, _masked_quantile_1d, _masked_quantile, quantile, percentile, array_equal, array_equiv, where, choose, select, piecewise, interp, lexsort, apply_over_axes, _array2string, array2string, array_str, masked_nanfunc, nanfunc"}}, {"code": "result_data,\n            correlation,\n            subtract=False,\n            to_variance=np.square,\n            from_variance=np.sqrt,\n        )\n\n    def _propagate_subtract(self, other_uncert, result_data, correlation):\n        return super()._propagate_add_sub(\n            other_uncert,\n            result_data,\n            correlation,\n            subtract=True,\n            to_variance=np.square,\n            from_variance=np.sqrt,\n        )\n\n    def _propagate_multiply(self, other_uncert, result_data, correlation):\n        return super()._propagate_multiply_divide(\n            other_uncert,\n            result_data,\n            correlation,\n            divide=False,\n            to_variance=np.square,\n            from_variance=np.sqrt,\n        )\n\n    def _propagate_divide(self, other_uncert, result_data, correlation):\n        return super()._propagate_multiply_divide(\n            other_uncert,\n            result_data,\n            correlation,\n            divide=True,\n            to_variance=np.square,\n            from_variance=np.sqrt,\n        )\n\n    def _propagate_collapse(self, numpy_operation, axis):\n        # defer to _VariancePropagationMixin\n        return super()._propagate_collapse(numpy_operation, axis=axis)\n\n    def _data_unit_to_uncertainty_unit(self, value):\n        return value\n\n    def _convert_to_variance(self):\n        new_array = None if self.array is None else self.array**2\n        new_unit = None if self.unit is None else self.unit**2\n        return VarianceUncertainty(new_array, unit=new_unit)\n\n    @classmethod\n    def _convert_from_variance(cls, var_uncert):\n        new_array = None if var_uncert.array is None else var_uncert.array ** (1 / 2)\n        new_unit = None if var_uncert.unit is None else var_uncert.unit ** (1 / 2)\n        return cls(new_array, unit=new_unit)", "metadata": {"file_name": "astropy/nddata/nduncertainty.py", "File Name": "astropy/nddata/nduncertainty.py", "Classes": "IncompatibleUncertaintiesException, MissingDataAssociationException, NDUncertainty, UnknownUncertainty, _VariancePropagationMixin, StdDevUncertainty, VarianceUncertainty, InverseVariance", "Functions": "_move_preserved_axes_first, _unravel_preserved_axes, from_variance_for_mean, _inverse"}}, {"code": "flag_name_map : BitFlagNameMap\n         A `BitFlagNameMap` object that provides mapping from mnemonic\n         bit flag names to integer bit values in order to translate mnemonic\n         flags to numeric values when ``bit_flags`` that are comma- or\n         '+'-separated list of menmonic bit flag names.\n\n    Returns\n    -------\n    bitmask : int or None\n        Returns an integer bit mask formed from the input bit value or `None`\n        if input ``bit_flags`` parameter is `None` or an empty string.\n        If input string value was prepended with '~' (or ``flip_bits`` was set\n        to `True`), then returned value will have its bits flipped\n        (inverse mask).", "metadata": {"file_name": "astropy/nddata/bitmask.py", "File Name": "astropy/nddata/bitmask.py", "Classes": "InvalidBitFlag, BitFlag, BitFlagNameMeta, BitFlagNameMap", "Functions": "_is_bit_flag, _is_int, extend_bit_flag_map, interpret_bit_flags, bitfield_to_boolean_mask"}}, {"code": "@dispatched_function\ndef ones_like(a, dtype=None, order=\"K\", subok=True, shape=None):\n    \"\"\"Return an array of ones with the same shape and type as a given array.\n\n    Like `numpy.ones_like`, but will add an all-false mask.\n    \"\"\"\n    unmasked = np.ones_like(\n        a.unmasked, dtype=dtype, order=order, subok=subok, shape=shape\n    )\n    return unmasked, False, None\n\n\n@dispatched_function\ndef full_like(a, fill_value, dtype=None, order=\"K\", subok=True, shape=None):\n    \"\"\"Return a full array with the same shape and type as a given array.\n\n    Like `numpy.full_like`, but with a mask that is also set.\n    If ``fill_value`` is `numpy.ma.masked`, the data will be left unset\n    (i.e., as created by `numpy.empty_like`).\n    \"\"\"\n    result = np.empty_like(a, dtype=dtype, order=order, subok=subok, shape=shape)\n    result[...] = fill_value\n    return result", "metadata": {"file_name": "astropy/utils/masked/function_helpers.py", "File Name": "astropy/utils/masked/function_helpers.py", "Classes": "MaskedFormat", "Functions": "_get_data_and_masks, datetime_as_string, sinc, iscomplex, unwrap, nan_to_num, masked_a_helper, masked_m_helper, masked_v_helper, masked_arr_helper, broadcast_to, outer, empty_like, zeros_like, ones_like, full_like, put, putmask, place, copyto, packbits, unpackbits, bincount, msort, sort_complex, concatenate, append, block, broadcast_arrays, insert, count_nonzero, _masked_median_1d, _masked_median, median, _masked_quantile_1d, _masked_quantile, quantile, percentile, array_equal, array_equiv, where, choose, select, piecewise, interp, lexsort, apply_over_axes, _array2string, array2string, array_str, masked_nanfunc, nanfunc"}}, {"code": "@dispatched_function\ndef empty_like(prototype, dtype=None, order=\"K\", subok=True, shape=None):\n    \"\"\"Return a new array with the same shape and type as a given array.\n\n    Like `numpy.empty_like`, but will add an empty mask.\n    \"\"\"\n    unmasked = np.empty_like(\n        prototype.unmasked, dtype=dtype, order=order, subok=subok, shape=shape\n    )\n    if dtype is not None:\n        dtype = (\n            np.ma.make_mask_descr(unmasked.dtype)\n            if unmasked.dtype.names\n            else np.dtype(\"?\")\n        )\n    mask = np.empty_like(\n        prototype.mask, dtype=dtype, order=order, subok=subok, shape=shape\n    )\n\n    return unmasked, mask, None\n\n\n@dispatched_function\ndef zeros_like(a, dtype=None, order=\"K\", subok=True, shape=None):\n    \"\"\"Return an array of zeros with the same shape and type as a given array.\n\n    Like `numpy.zeros_like`, but will add an all-false mask.\n    \"\"\"\n    unmasked = np.zeros_like(\n        a.unmasked, dtype=dtype, order=order, subok=subok, shape=shape\n    )\n    return unmasked, False, None", "metadata": {"file_name": "astropy/utils/masked/function_helpers.py", "File Name": "astropy/utils/masked/function_helpers.py", "Classes": "MaskedFormat", "Functions": "_get_data_and_masks, datetime_as_string, sinc, iscomplex, unwrap, nan_to_num, masked_a_helper, masked_m_helper, masked_v_helper, masked_arr_helper, broadcast_to, outer, empty_like, zeros_like, ones_like, full_like, put, putmask, place, copyto, packbits, unpackbits, bincount, msort, sort_complex, concatenate, append, block, broadcast_arrays, insert, count_nonzero, _masked_median_1d, _masked_median, median, _masked_quantile_1d, _masked_quantile, quantile, percentile, array_equal, array_equiv, where, choose, select, piecewise, interp, lexsort, apply_over_axes, _array2string, array2string, array_str, masked_nanfunc, nanfunc"}}, {"code": "# Licensed under a 3-clause BSD style license - see LICENSE.rst\n\nimport weakref\nfrom abc import ABCMeta, abstractmethod\nfrom copy import deepcopy\n\nimport numpy as np\n\n# from astropy.utils.compat import ignored\nfrom astropy import log\nfrom astropy.units import Quantity, Unit, UnitConversionError\n\n__all__ = [\n    \"MissingDataAssociationException\",\n    \"IncompatibleUncertaintiesException\",\n    \"NDUncertainty\",\n    \"StdDevUncertainty\",\n    \"UnknownUncertainty\",\n    \"VarianceUncertainty\",\n    \"InverseVariance\",\n]\n\n# mapping from collapsing operations to the complementary methods used for `to_variance`\ncollapse_to_variance_mapping = {\n    np.sum: np.square,\n    np.mean: np.square,\n}\n\n\ndef _move_preserved_axes_first(arr, preserve_axes):\n    # When collapsing an ND array and preserving M axes, move the\n    # preserved axes to the first M axes of the output. For example,\n    # if arr.shape == (6, 5, 4, 3, 2) and we're preserving axes (1, 2),\n    # then the output should have shape (20, 6, 3, 2). Axes 1 and 2 have\n    # shape 5 and 4, so we take their product and put them both in the zeroth\n    # axis.\n    zeroth_axis_after_reshape = np.prod(np.array(arr.shape)[list(preserve_axes)])\n    collapse_axes = [i for i in range(arr.ndim) if i not in preserve_axes]\n    return arr.reshape(\n        [zeroth_axis_after_reshape] + np.array(arr.shape)[collapse_axes].tolist()\n    )", "metadata": {"file_name": "astropy/nddata/nduncertainty.py", "File Name": "astropy/nddata/nduncertainty.py", "Classes": "IncompatibleUncertaintiesException, MissingDataAssociationException, NDUncertainty, UnknownUncertainty, _VariancePropagationMixin, StdDevUncertainty, VarianceUncertainty, InverseVariance", "Functions": "_move_preserved_axes_first, _unravel_preserved_axes, from_variance_for_mean, _inverse"}}, {"code": "These can be specified\n        either as a Numpy array of any type (or an object which can be converted\n        to a Numpy array) with a shape matching that of the\n        data, or as a `~astropy.nddata.FlagCollection` instance which has a\n        shape matching that of the data.\n\n    wcs : None, optional\n        WCS-object containing the world coordinate system for the data.\n\n        .. warning::\n            This is not yet defined because the discussion of how best to\n            represent this class's WCS system generically is still under\n            consideration. For now just leave it as None\n\n    meta : `dict`-like object, optional\n        Metadata for this object.  \"Metadata\" here means all information that\n        is included with this object but not part of any other attribute\n        of this particular object.  e.g., creation date, unique identifier,\n        simulation parameters, exposure time, telescope name, etc.\n\n    unit : `~astropy.units.UnitBase` instance or str, optional\n        The units of the data.\n\n\n    Raises\n    ------\n    ValueError :\n        If the `uncertainty` or `mask` inputs cannot be broadcast (e.g., match\n        shape) onto ``data``.\n    \"\"\"\n\n    def __init__(self, data, *args, flags=None, **kwargs):\n        # Initialize with the parent...\n        super().__init__(data, *args, **kwargs)\n\n        # ...then reset uncertainty to force it to go through the\n        # setter logic below. In base NDData all that is done is to\n        # set self._uncertainty to whatever uncertainty is passed in.\n        self.uncertainty = self._uncertainty\n\n        # Same thing for mask.\n        self.mask = self._mask\n\n        # Initial flags because it is no longer handled in NDData\n        # or NDDataBase.", "metadata": {"file_name": "astropy/nddata/compat.py", "File Name": "astropy/nddata/compat.py", "Classes": "NDDataArray"}}, {"code": "# Licensed under a 3-clause BSD style license - see LICENSE.rst\n\n\"\"\"\nThe `astropy.nddata` subpackage provides the `~astropy.nddata.NDData`\nclass and related tools to manage n-dimensional array-based data (e.g.\nCCD images, IFU Data, grid-based simulation data, ...). This is more than\njust `numpy.ndarray` objects, because it provides metadata that cannot\nbe easily provided by a single array.\n\"\"\"\n\nfrom astropy import config as _config\n\nfrom .bitmask import *\nfrom .blocks import *\nfrom .ccddata import *\nfrom .compat import *\nfrom .decorators import *\nfrom .flag_collection import *\nfrom .mixins.ndarithmetic import *\nfrom .mixins.ndio import *\nfrom .mixins.ndslicing import *\nfrom .nddata import *\nfrom .nddata_base import *\nfrom .nddata_withmixins import *\nfrom .nduncertainty import *\nfrom .utils import *\n\n\nclass Conf(_config.ConfigNamespace):\n    \"\"\"\n    Configuration parameters for `astropy.nddata`.\n    \"\"\"\n\n    warn_unsupported_correlated = _config.ConfigItem(\n        True,\n        \"Whether to issue a warning if `~astropy.nddata.NDData` arithmetic \"\n        \"is performed with uncertainties and the uncertainties do not \"\n        \"support the propagation of correlated uncertainties.\",\n    )\n\n    warn_setting_unit_directly = _config.ConfigItem(\n        True,\n        \"Whether to issue a warning when the `~astropy.nddata.NDData` unit \"\n        \"attribute is changed from a non-``None`` value to another value \"\n        \"that data values/uncertainties are not scaled with the unit change.\",\n    )\n\n\nconf = Conf()", "metadata": {"file_name": "astropy/nddata/__init__.py", "File Name": "astropy/nddata/__init__.py", "Classes": "Conf"}}, {"code": "# Licensed under a 3-clause BSD style license - see LICENSE.rst\n\"\"\"\nBuilt-in mask mixin class.\n\nThe design uses `Masked` as a factory class which automatically\ngenerates new subclasses for any data class that is itself a\nsubclass of a predefined masked class, with `MaskedNDArray`\nproviding such a predefined class for `~numpy.ndarray`.\n\"\"\"\nfrom .core import *", "metadata": {"file_name": "astropy/utils/masked/__init__.py", "File Name": "astropy/utils/masked/__init__.py"}}, {"code": "method = self.serialize_method[self._serialize_context]\n\n        if method == \"data_mask\":\n            out[\"data\"] = masked_array.unmasked\n\n            if np.any(masked_array.mask):\n                # Only if there are actually masked elements do we add the ``mask`` column\n                out[\"mask\"] = masked_array.mask\n\n        elif method == \"null_value\":\n            out[\"data\"] = np.ma.MaskedArray(\n                masked_array.unmasked, mask=masked_array.mask\n            )\n\n        else:\n            raise ValueError(\n                'serialize method must be either \"data_mask\" or \"null_value\"'\n            )\n\n        return out\n\n    def _construct_from_dict(self, map):\n        # Override usual handling, since MaskedNDArray takes shape and buffer\n        # as input, which is less useful here.\n        # The map can contain either a MaskedColumn or a Column and a mask.\n        # Extract the mask for the former case.\n        map.setdefault(\"mask\", getattr(map[\"data\"], \"mask\", False))\n        return self._parent_cls.from_unmasked(**map)\n\n\nclass MaskedArraySubclassInfo(MaskedInfoBase):\n    \"\"\"Mixin class to create a subclasses such as MaskedQuantityInfo.\"\"\"\n\n    # This is used below in __init_subclass__, which also inserts a\n    # 'serialize_method' attribute in attr_names.\n\n    def _represent_as_dict(self):\n        # Use the data_cls as the class name for serialization,\n        # so that we do not have to store all possible masked classes\n        # in astropy.table.serialize.__construct_mixin_classes.\n        out = super()._represent_as_dict()\n        data_cls = self._parent._data_cls\n        out.setdefault(\"__class__\", data_cls.__module__ + \".\" + data_cls.__name__)\n        return out", "metadata": {"file_name": "astropy/utils/masked/core.py", "File Name": "astropy/utils/masked/core.py", "Classes": "Masked, MaskedInfoBase, MaskedNDArrayInfo, MaskedArraySubclassInfo, MaskedIterator, MaskedNDArray, MaskedRecarray", "Functions": "_comparison_method, _compare, argmin, argmax, argmin, argmax"}}, {"code": "def _array2string(a, options, separator=\" \", prefix=\"\"):\n    # Mostly copied from numpy.core.arrayprint, except:\n    # - The format function is wrapped in a mask-aware class;\n    # - Arrays scalars are not cast as arrays.\n    from numpy.core.arrayprint import _formatArray, _leading_trailing\n\n    data = np.asarray(a)\n\n    if a.size > options[\"threshold\"]:\n        summary_insert = \"...\"\n        data = _leading_trailing(data, options[\"edgeitems\"])\n    else:\n        summary_insert = \"\"\n\n    # find the right formatting function for the array\n    format_function = MaskedFormat.from_data(data, **options)\n\n    # skip over \"[\"\n    next_line_prefix = \" \"\n    # skip over array(\n    next_line_prefix += \" \" * len(prefix)\n\n    lst = _formatArray(\n        a,\n        format_function,\n        options[\"linewidth\"],\n        next_line_prefix,\n        separator,\n        options[\"edgeitems\"],\n        summary_insert,\n        options[\"legacy\"],\n    )\n    return lst", "metadata": {"file_name": "astropy/utils/masked/function_helpers.py", "File Name": "astropy/utils/masked/function_helpers.py", "Classes": "MaskedFormat", "Functions": "_get_data_and_masks, datetime_as_string, sinc, iscomplex, unwrap, nan_to_num, masked_a_helper, masked_m_helper, masked_v_helper, masked_arr_helper, broadcast_to, outer, empty_like, zeros_like, ones_like, full_like, put, putmask, place, copyto, packbits, unpackbits, bincount, msort, sort_complex, concatenate, append, block, broadcast_arrays, insert, count_nonzero, _masked_median_1d, _masked_median, median, _masked_quantile_1d, _masked_quantile, quantile, percentile, array_equal, array_equiv, where, choose, select, piecewise, interp, lexsort, apply_over_axes, _array2string, array2string, array_str, masked_nanfunc, nanfunc"}}, {"code": ") from None\n            else:  # pragma: no cover\n                raise\n\n    _eq_simple = _comparison_method(\"__eq__\")\n    _ne_simple = _comparison_method(\"__ne__\")\n    __lt__ = _comparison_method(\"__lt__\")\n    __le__ = _comparison_method(\"__le__\")\n    __gt__ = _comparison_method(\"__gt__\")\n    __ge__ = _comparison_method(\"__ge__\")\n\n    def __eq__(self, other):\n        if not self.dtype.names:\n            return self._eq_simple(other)\n\n        # For structured arrays, we treat this as a reduction over the fields,\n        # where masked fields are skipped and thus do not influence the result.\n        other = np.asanyarray(other, dtype=self.dtype)\n        result = np.stack(\n            [self[field] == other[field] for field in self.dtype.names], axis=-1\n        )\n        return result.all(axis=-1)\n\n    def __ne__(self, other):\n        if not self.dtype.names:\n            return self._ne_simple(other)\n\n        # For structured arrays, we treat this as a reduction over the fields,\n        # where masked fields are skipped and thus do not influence the result.\n        other = np.asanyarray(other, dtype=self.dtype)\n        result = np.stack(\n            [self[field] != other[field] for field in self.dtype.names], axis=-1\n        )\n        return result.any(axis=-1)\n\n    def _combine_masks(self, masks, out=None, where=True, copy=True):\n        \"\"\"Combine masks, possibly storing it in some output.\n\n        Parameters\n        ----------\n        masks : tuple of array of bool or None\n            Input masks.  Any that are `None` or `False` are ignored.\n            Should broadcast to each other.\n        out : output mask array, optional\n            Possible output array to hold the result.\n        where : array of bool, optional\n            Which elements of the output array to fill.\n        copy : bool optional\n            Whether to ensure a copy is made.", "metadata": {"file_name": "astropy/utils/masked/core.py", "File Name": "astropy/utils/masked/core.py", "Classes": "Masked, MaskedInfoBase, MaskedNDArrayInfo, MaskedArraySubclassInfo, MaskedIterator, MaskedNDArray, MaskedRecarray", "Functions": "_comparison_method, _compare, argmin, argmax, argmin, argmax"}}, {"code": "string = self.format_function(x.unmasked[()])\n        if x.mask:\n            # Strikethrough would be neat, but terminal needs a different\n            # formatting than, say, jupyter notebook.\n            # return \"\\x1B[9m\"+string+\"\\x1B[29m\"\n            # return ''.join(s+'\\u0336' for s in string)\n            n = min(3, max(1, len(string)))\n            return \" \" * (len(string) - n) + \"\\u2014\" * n\n        else:\n            return string\n\n    @classmethod\n    def from_data(cls, data, **options):\n        from numpy.core.arrayprint import _get_format_function\n\n        return cls(_get_format_function(data, **options))", "metadata": {"file_name": "astropy/utils/masked/function_helpers.py", "File Name": "astropy/utils/masked/function_helpers.py", "Classes": "MaskedFormat", "Functions": "_get_data_and_masks, datetime_as_string, sinc, iscomplex, unwrap, nan_to_num, masked_a_helper, masked_m_helper, masked_v_helper, masked_arr_helper, broadcast_to, outer, empty_like, zeros_like, ones_like, full_like, put, putmask, place, copyto, packbits, unpackbits, bincount, msort, sort_complex, concatenate, append, block, broadcast_arrays, insert, count_nonzero, _masked_median_1d, _masked_median, median, _masked_quantile_1d, _masked_quantile, quantile, percentile, array_equal, array_equiv, where, choose, select, piecewise, interp, lexsort, apply_over_axes, _array2string, array2string, array_str, masked_nanfunc, nanfunc"}}, {"code": "def bitfield_to_boolean_mask(\n    bitfield,\n    ignore_flags=0,\n    flip_bits=None,\n    good_mask_value=False,\n    dtype=np.bool_,\n    flag_name_map=None,\n):\n    \"\"\"\n    bitfield_to_boolean_mask(bitfield, ignore_flags=None, flip_bits=None, \\\ngood_mask_value=False, dtype=numpy.bool_)\n    Converts an array of bit fields to a boolean (or integer) mask array\n    according to a bit mask constructed from the supplied bit flags (see\n    ``ignore_flags`` parameter).\n\n    This function is particularly useful to convert data quality arrays to\n    boolean masks with selective filtering of DQ flags.\n\n    Parameters\n    ----------\n    bitfield : ndarray\n        An array of bit flags. By default, values different from zero are\n        interpreted as \"bad\" values and values equal to zero are considered\n        as \"good\" values. However, see ``ignore_flags`` parameter on how to\n        selectively ignore some bits in the ``bitfield`` array data.\n\n    ignore_flags : int, str, list, None (default = 0)\n        An integer bit mask, `None`, a Python list of bit flags, a comma-,\n        or ``'|'``-separated, ``'+'``-separated string list of integer\n        bit flags or mnemonic flag names that indicate what bits in the input\n        ``bitfield`` should be *ignored* (i.e., zeroed), or `None`.\n\n        .. note::\n            When ``bit_flags`` is a list of flag names, the ``flag_name_map``\n            parameter must be provided.\n\n        | Setting ``ignore_flags`` to `None` effectively will make\n          `bitfield_to_boolean_mask` interpret all ``bitfield`` elements\n          as \"good\" regardless of their value.\n\n        | When ``ignore_flags`` argument is an integer bit mask, it will be\n          combined using bitwise-NOT and bitwise-AND with each element of the\n          input ``bitfield`` array (``~ignore_flags & bitfield``).", "metadata": {"file_name": "astropy/nddata/bitmask.py", "File Name": "astropy/nddata/bitmask.py", "Classes": "InvalidBitFlag, BitFlag, BitFlagNameMeta, BitFlagNameMap", "Functions": "_is_bit_flag, _is_int, extend_bit_flag_map, interpret_bit_flags, bitfield_to_boolean_mask"}}, {"code": "# For multiplication we don't need the result as quantity\n        if isinstance(result_data, Quantity):\n            result_data = result_data.value\n\n        if divide:\n            correlation_sign = -1\n        else:\n            correlation_sign = 1\n\n        if other_uncert.array is not None:\n            # We want the result to have a unit consistent with the parent, so\n            # we only need to convert the unit of the other uncertainty if it\n            # is different from its data's unit.\n            if (\n                other_uncert.unit\n                and to_variance(1 * other_uncert.unit)\n                != ((1 * other_uncert.parent_nddata.unit) ** 2).unit\n            ):\n                d_b = (\n                    to_variance(other_uncert.array << other_uncert.unit)\n                    .to((1 * other_uncert.parent_nddata.unit) ** 2)\n                    .value\n                )\n            else:\n                d_b = to_variance(other_uncert.array)\n            # Formula: sigma**2 = |A|**2 * d_b\n            right = np.abs(self.parent_nddata.data**2 * d_b)\n        else:\n            right = 0\n\n        if self.array is not None:\n            # Just the reversed case\n            if (\n                self.unit\n                and to_variance(1 * self.unit)\n                != ((1 * self.parent_nddata.unit) ** 2).unit\n            ):\n                d_a = (\n                    to_variance(self.array << self.unit)\n                    .to((1 * self.parent_nddata.unit) ** 2)\n                    .value\n                )\n            else:\n                d_a = to_variance(self.array)\n            # Formula: sigma**2 = |B|**2 * d_a\n            left = np.abs(other_uncert.parent_nddata.", "metadata": {"file_name": "astropy/nddata/nduncertainty.py", "File Name": "astropy/nddata/nduncertainty.py", "Classes": "IncompatibleUncertaintiesException, MissingDataAssociationException, NDUncertainty, UnknownUncertainty, _VariancePropagationMixin, StdDevUncertainty, VarianceUncertainty, InverseVariance", "Functions": "_move_preserved_axes_first, _unravel_preserved_axes, from_variance_for_mean, _inverse"}}, {"code": "# Licensed under a 3-clause BSD style license - see LICENSE.rst\n# This module implements the base NDData class.\n\n\nfrom copy import deepcopy\n\nimport numpy as np\n\nfrom astropy import log\nfrom astropy.units import Quantity, Unit\nfrom astropy.utils.masked import Masked, MaskedNDArray\nfrom astropy.utils.metadata import MetaData\nfrom astropy.wcs.wcsapi import SlicedLowLevelWCS  # noqa: F401\nfrom astropy.wcs.wcsapi import BaseHighLevelWCS, BaseLowLevelWCS, HighLevelWCSWrapper\n\nfrom .nddata_base import NDDataBase\nfrom .nduncertainty import NDUncertainty, UnknownUncertainty\n\n__all__ = [\"NDData\"]\n\n_meta_doc = \"\"\"`dict`-like : Additional meta information about the dataset.\"\"\"\n\n\nclass NDData(NDDataBase):\n    \"\"\"\n    A container for `numpy.ndarray`-based datasets, using the\n    `~astropy.nddata.NDDataBase` interface.\n\n    The key distinction from raw `numpy.ndarray` is the presence of\n    additional metadata such as uncertainty, mask, unit, a coordinate system\n    and/or a dictionary containing further meta information. This class *only*\n    provides a container for *storing* such datasets. For further functionality\n    take a look at the ``See also`` section.\n\n    See also: https://docs.astropy.org/en/stable/nddata/\n\n    Parameters\n    ----------\n    data : `numpy.ndarray`-like or `NDData`-like\n        The dataset.\n\n    uncertainty : any type, optional\n        Uncertainty in the dataset.\n        Should have an attribute ``uncertainty_type`` that defines what kind of\n        uncertainty is stored, for example ``\"std\"`` for standard deviation or\n        ``\"var\"`` for variance. A metaclass defining such an interface is\n        `NDUncertainty` - but isn't mandatory. If the uncertainty has no such\n        attribute the uncertainty is stored as `UnknownUncertainty`.\n        Defaults to ``None``.\n\n    mask : any type, optional\n        Mask for the dataset. Masks should follow the ``numpy`` convention that\n        **valid** data points are marked by ``False`` and **invalid** ones with\n        ``True``.", "metadata": {"file_name": "astropy/nddata/nddata.py", "File Name": "astropy/nddata/nddata.py", "Classes": "NDData"}}, {"code": "Allows the base class to be inferred correctly when a masked instance\n        is used to initialize (or viewed as) a `~numpy.ma.MaskedArray`.\n\n        \"\"\"\n        return self._data_cls\n\n    def view(self, dtype=None, type=None):\n        \"\"\"New view of the masked array.\n\n        Like `numpy.ndarray.view`, but always returning a masked array subclass.\n        \"\"\"\n        if type is None and (\n            isinstance(dtype, builtins.type) and issubclass(dtype, np.ndarray)\n        ):\n            return super().view(self._get_masked_cls(dtype))\n\n        if dtype is None:\n            return super().view(self._get_masked_cls(type))\n\n        dtype = np.dtype(dtype)\n        if not (\n            dtype.itemsize == self.dtype.itemsize\n            and (dtype.names is None or len(dtype.names) == len(self.dtype.names))\n        ):\n            raise NotImplementedError(\n                f\"{self.__class__} cannot be viewed with a dtype with a \"\n                \"with a different number of fields or size.\"\n            )\n\n        return super().view(dtype, self._get_masked_cls(type))\n\n    def __array_finalize__(self, obj):\n        # If we're a new object or viewing an ndarray, nothing has to be done.\n        if obj is None or obj.__class__ is np.ndarray:\n            return\n\n        # Logically, this should come from ndarray and hence be None, but\n        # just in case someone creates a new mixin, we check.\n        super_array_finalize = super().__array_finalize__\n        if super_array_finalize:  # pragma: no cover\n            super_array_finalize(obj)\n\n        if self._mask is None:\n            # Got here after, e.g., a view of another masked class.\n            # Get its mask, or initialize ours.\n            self._set_mask(getattr(obj, \"_mask\", False))\n\n        if \"info\" in obj.__dict__:\n            self.info = obj.info\n\n    @property\n    def shape(self):\n        \"\"\"The shape of the data and the mask.", "metadata": {"file_name": "astropy/utils/masked/core.py", "File Name": "astropy/utils/masked/core.py", "Classes": "Masked, MaskedInfoBase, MaskedNDArrayInfo, MaskedArraySubclassInfo, MaskedIterator, MaskedNDArray, MaskedRecarray", "Functions": "_comparison_method, _compare, argmin, argmax, argmin, argmax"}}, {"code": "if axis is not None and not hasattr(axis, \"__len__\"):\n            # this is a single axis:\n            axis = [axis]\n\n        if extremum is np.min:\n            arg_extremum = np.ma.argmin\n        elif extremum is np.max:\n            arg_extremum = np.ma.argmax\n\n        all_axes = np.arange(self.array.ndim)\n\n        if axis is None:\n            # collapse over all dimensions\n            ind = arg_extremum(np.asanyarray(self.parent_nddata).ravel())\n            return self.array.ravel()[ind]\n\n        # collapse an ND array over arbitrary dimensions:\n        preserve_axes = [ax for ax in all_axes if ax not in axis]\n        meas = np.ma.masked_array(\n            _move_preserved_axes_first(self.parent_nddata.data, preserve_axes),\n            _move_preserved_axes_first(self.parent_nddata.mask, preserve_axes),\n        )\n        err = _move_preserved_axes_first(self.array, preserve_axes)\n\n        result = np.array(\n            [e[np.unravel_index(arg_extremum(m), m.shape)] for m, e in zip(meas, err)]\n        )\n\n        return _unravel_preserved_axes(\n            self.parent_nddata.data,\n            result,\n            preserve_axes,\n        )\n\n    def _propagate_add_sub(\n        self,\n        other_uncert,\n        result_data,\n        correlation,\n        subtract=False,\n        to_variance=lambda x: x,\n        from_variance=lambda x: x,\n    ):\n        \"\"\"\n        Error propagation for addition or subtraction of variance or\n        variance-like uncertainties. Uncertainties are calculated using the\n        formulae for variance but can be used for uncertainty convertible to\n        a variance.\n\n        Parameters\n        ----------\n        other_uncert : `~astropy.nddata.NDUncertainty` instance\n            The uncertainty, if any, of the other operand.\n\n        result_data : `~astropy.nddata.NDData` instance\n            The results of the operation on the data.", "metadata": {"file_name": "astropy/nddata/nduncertainty.py", "File Name": "astropy/nddata/nduncertainty.py", "Classes": "IncompatibleUncertaintiesException, MissingDataAssociationException, NDUncertainty, UnknownUncertainty, _VariancePropagationMixin, StdDevUncertainty, VarianceUncertainty, InverseVariance", "Functions": "_move_preserved_axes_first, _unravel_preserved_axes, from_variance_for_mean, _inverse"}}, {"code": "size = 1\n        for sh in self.shape:\n            size *= sh\n        return size\n\n    @property\n    def isscalar(self):\n        return self.shape == ()\n\n    def __len__(self):\n        if self.isscalar:\n            raise TypeError(f\"Scalar {self.__class__.__name__!r} object has no len()\")\n        return self.shape[0]\n\n    def __bool__(self):\n        \"\"\"Any instance should evaluate to True, except when it is empty.\"\"\"\n        return self.size > 0\n\n    def __getitem__(self, item):\n        try:\n            return self._apply(\"__getitem__\", item)\n        except IndexError:\n            if self.isscalar:\n                raise TypeError(\n                    f\"scalar {self.__class.__name__!r} object is not subscriptable.\"\n                )\n            else:\n                raise\n\n    def __iter__(self):\n        if self.isscalar:\n            raise TypeError(\n                f\"scalar {self.__class__.__name__!r} object is not iterable.\"\n            )\n\n        # We cannot just write a generator here, since then the above error\n        # would only be raised once we try to use the iterator, rather than\n        # upon its definition using iter(self).\n        def self_iter():\n            for idx in range(len(self)):\n                yield self[idx]\n\n        return self_iter()\n\n    # Functions that change shape or essentially do indexing.\n    _APPLICABLE_FUNCTIONS = {\n        np.moveaxis,\n        np.rollaxis,\n        np.atleast_1d,\n        np.atleast_2d,\n        np.atleast_3d,\n        np.expand_dims,\n        np.broadcast_to,\n        np.flip,\n        np.fliplr,\n        np.flipud,\n        np.rot90,\n        np.roll,\n        np.delete,\n    }\n\n    # Functions that themselves defer to a method. Those are all\n    # defined in np.core.fromnumeric, but exclude alen as well as\n    # sort and partition, which make copies before calling the method.", "metadata": {"file_name": "astropy/utils/shapes.py", "File Name": "astropy/utils/shapes.py", "Classes": "NDArrayShapeMethods, ShapedLikeNDArray, IncompatibleShapeError", "Functions": "check_broadcast, unbroadcast, simplify_basic_index, self_iter"}}, {"code": "return True\n\n    def _propagate_add(self, other_uncert, result_data, correlation):\n        return super()._propagate_add_sub(\n            other_uncert,\n            result_data,\n            correlation,\n            subtract=False,\n            to_variance=_inverse,\n            from_variance=_inverse,\n        )\n\n    def _propagate_subtract(self, other_uncert, result_data, correlation):\n        return super()._propagate_add_sub(\n            other_uncert,\n            result_data,\n            correlation,\n            subtract=True,\n            to_variance=_inverse,\n            from_variance=_inverse,\n        )\n\n    def _propagate_multiply(self, other_uncert, result_data, correlation):\n        return super()._propagate_multiply_divide(\n            other_uncert,\n            result_data,\n            correlation,\n            divide=False,\n            to_variance=_inverse,\n            from_variance=_inverse,\n        )\n\n    def _propagate_divide(self, other_uncert, result_data, correlation):\n        return super()._propagate_multiply_divide(\n            other_uncert,\n            result_data,\n            correlation,\n            divide=True,\n            to_variance=_inverse,\n            from_variance=_inverse,\n        )\n\n    def _data_unit_to_uncertainty_unit(self, value):\n        return 1 / value**2\n\n    def _convert_to_variance(self):\n        new_array = None if self.array is None else 1 / self.array\n        new_unit = None if self.unit is None else 1 / self.unit\n        return VarianceUncertainty(new_array, unit=new_unit)\n\n    @classmethod\n    def _convert_from_variance(cls, var_uncert):\n        new_array = None if var_uncert.array is None else 1 / var_uncert.array\n        new_unit = None if var_uncert.unit is None else 1 / var_uncert.unit\n        return cls(new_array, unit=new_unit)", "metadata": {"file_name": "astropy/nddata/nduncertainty.py", "File Name": "astropy/nddata/nduncertainty.py", "Classes": "IncompatibleUncertaintiesException, MissingDataAssociationException, NDUncertainty, UnknownUncertainty, _VariancePropagationMixin, StdDevUncertainty, VarianceUncertainty, InverseVariance", "Functions": "_move_preserved_axes_first, _unravel_preserved_axes, from_variance_for_mean, _inverse"}}, {"code": "if isinstance(self._parent_nddata, weakref.ref):\n                    resolved_parent = self._parent_nddata()\n                    if resolved_parent is None:\n                        log.info(parent_lost_message)\n                    return resolved_parent\n                else:\n                    log.info(\"parent_nddata should be a weakref to an NDData object.\")\n                    return self._parent_nddata\n\n    @parent_nddata.setter\n    def parent_nddata(self, value):\n        if value is not None and not isinstance(value, weakref.ref):\n            # Save a weak reference on the uncertainty that points to this\n            # instance of NDData. Direct references should NOT be used:\n            # https://github.com/astropy/astropy/pull/4799#discussion_r61236832\n            value = weakref.ref(value)\n        # Set _parent_nddata here and access below with the property because value\n        # is a weakref\n        self._parent_nddata = value\n        # set uncertainty unit to that of the parent if it was not already set, unless initializing\n        # with empty parent (Value=None)\n        if value is not None:\n            parent_unit = self.parent_nddata.unit\n            # this will get the unit for masked quantity input:\n            parent_data_unit = getattr(self.parent_nddata.data, \"unit\", None)\n            if parent_unit is None and parent_data_unit is None:\n                self.unit = None\n            elif self.unit is None and parent_unit is not None:\n                # Set the uncertainty's unit to the appropriate value\n                self.unit = self._data_unit_to_uncertainty_unit(parent_unit)\n            elif parent_data_unit is not None:\n                # if the parent_nddata object has a unit, use it:\n                self.unit = self._data_unit_to_uncertainty_unit(parent_data_unit)\n            else:\n                # Check that units of uncertainty are compatible with those of\n                # the parent. If they are, no need to change units of the\n                # uncertainty or the data. If they are not, let the user know.", "metadata": {"file_name": "astropy/nddata/nduncertainty.py", "File Name": "astropy/nddata/nduncertainty.py", "Classes": "IncompatibleUncertaintiesException, MissingDataAssociationException, NDUncertainty, UnknownUncertainty, _VariancePropagationMixin, StdDevUncertainty, VarianceUncertainty, InverseVariance", "Functions": "_move_preserved_axes_first, _unravel_preserved_axes, from_variance_for_mean, _inverse"}}, {"code": "@dispatched_function\ndef put(a, ind, v, mode=\"raise\"):\n    \"\"\"Replaces specified elements of an array with given values.\n\n    Like `numpy.put`, but for masked array ``a`` and possibly masked\n    value ``v``.  Masked indices ``ind`` are not supported.\n    \"\"\"\n    from astropy.utils.masked import Masked\n\n    if isinstance(ind, Masked) or not isinstance(a, Masked):\n        raise NotImplementedError\n\n    v_data, v_mask = a._get_data_and_mask(v)\n    if v_data is not None:\n        np.put(a.unmasked, ind, v_data, mode=mode)\n    # v_mask of None will be correctly interpreted as False.\n    np.put(a.mask, ind, v_mask, mode=mode)\n\n\n@dispatched_function\ndef putmask(a, mask, values):\n    \"\"\"Changes elements of an array based on conditional and input values.\n\n    Like `numpy.putmask`, but for masked array ``a`` and possibly masked\n    ``values``.  Masked ``mask`` is not supported.\n    \"\"\"\n    from astropy.utils.masked import Masked\n\n    if isinstance(mask, Masked) or not isinstance(a, Masked):\n        raise NotImplementedError\n\n    values_data, values_mask = a._get_data_and_mask(values)\n    if values_data is not None:\n        np.putmask(a.unmasked, mask, values_data)\n    np.putmask(a.mask, mask, values_mask)", "metadata": {"file_name": "astropy/utils/masked/function_helpers.py", "File Name": "astropy/utils/masked/function_helpers.py", "Classes": "MaskedFormat", "Functions": "_get_data_and_masks, datetime_as_string, sinc, iscomplex, unwrap, nan_to_num, masked_a_helper, masked_m_helper, masked_v_helper, masked_arr_helper, broadcast_to, outer, empty_like, zeros_like, ones_like, full_like, put, putmask, place, copyto, packbits, unpackbits, bincount, msort, sort_complex, concatenate, append, block, broadcast_arrays, insert, count_nonzero, _masked_median_1d, _masked_median, median, _masked_quantile_1d, _masked_quantile, quantile, percentile, array_equal, array_equiv, where, choose, select, piecewise, interp, lexsort, apply_over_axes, _array2string, array2string, array_str, masked_nanfunc, nanfunc"}}, {"code": "@dispatched_function\ndef place(arr, mask, vals):\n    \"\"\"Change elements of an array based on conditional and input values.\n\n    Like `numpy.place`, but for masked array ``a`` and possibly masked\n    ``values``.  Masked ``mask`` is not supported.\n    \"\"\"\n    from astropy.utils.masked import Masked\n\n    if isinstance(mask, Masked) or not isinstance(arr, Masked):\n        raise NotImplementedError\n\n    vals_data, vals_mask = arr._get_data_and_mask(vals)\n    if vals_data is not None:\n        np.place(arr.unmasked, mask, vals_data)\n    np.place(arr.mask, mask, vals_mask)\n\n\n@dispatched_function\ndef copyto(dst, src, casting=\"same_kind\", where=True):\n    \"\"\"Copies values from one array to another, broadcasting as necessary.\n\n    Like `numpy.copyto`, but for masked destination ``dst`` and possibly\n    masked source ``src``.\n    \"\"\"\n    from astropy.utils.masked import Masked\n\n    if not isinstance(dst, Masked) or isinstance(where, Masked):\n        raise NotImplementedError\n\n    src_data, src_mask = dst._get_data_and_mask(src)\n\n    if src_data is not None:\n        np.copyto(dst.unmasked, src_data, casting=casting, where=where)\n    if src_mask is not None:\n        np.copyto(dst.mask, src_mask, where=where)", "metadata": {"file_name": "astropy/utils/masked/function_helpers.py", "File Name": "astropy/utils/masked/function_helpers.py", "Classes": "MaskedFormat", "Functions": "_get_data_and_masks, datetime_as_string, sinc, iscomplex, unwrap, nan_to_num, masked_a_helper, masked_m_helper, masked_v_helper, masked_arr_helper, broadcast_to, outer, empty_like, zeros_like, ones_like, full_like, put, putmask, place, copyto, packbits, unpackbits, bincount, msort, sort_complex, concatenate, append, block, broadcast_arrays, insert, count_nonzero, _masked_median_1d, _masked_median, median, _masked_quantile_1d, _masked_quantile, quantile, percentile, array_equal, array_equiv, where, choose, select, piecewise, interp, lexsort, apply_over_axes, _array2string, array2string, array_str, masked_nanfunc, nanfunc"}}, {"code": "@dispatched_function\ndef broadcast_arrays(*args, subok=True):\n    \"\"\"Broadcast arrays to a common shape.\n\n    Like `numpy.broadcast_arrays`, applied to both unmasked data and masks.\n    Note that ``subok`` is taken to mean whether or not subclasses of\n    the unmasked data and masks are allowed, i.e., for ``subok=False``,\n    `~astropy.utils.masked.MaskedNDArray` instances will be returned.\n    \"\"\"\n    from .core import Masked\n\n    are_masked = [isinstance(arg, Masked) for arg in args]\n    data = [\n        (arg.unmasked if is_masked else arg) for arg, is_masked in zip(args, are_masked)\n    ]\n    results = np.broadcast_arrays(*data, subok=subok)\n\n    shape = results[0].shape if isinstance(results, list) else results.shape\n    masks = [\n        (np.broadcast_to(arg.mask, shape, subok=subok) if is_masked else None)\n        for arg, is_masked in zip(args, are_masked)\n    ]\n    results = [\n        (Masked(result, mask) if mask is not None else result)\n        for (result, mask) in zip(results, masks)\n    ]\n    return results if len(results) > 1 else results[0]", "metadata": {"file_name": "astropy/utils/masked/function_helpers.py", "File Name": "astropy/utils/masked/function_helpers.py", "Classes": "MaskedFormat", "Functions": "_get_data_and_masks, datetime_as_string, sinc, iscomplex, unwrap, nan_to_num, masked_a_helper, masked_m_helper, masked_v_helper, masked_arr_helper, broadcast_to, outer, empty_like, zeros_like, ones_like, full_like, put, putmask, place, copyto, packbits, unpackbits, bincount, msort, sort_complex, concatenate, append, block, broadcast_arrays, insert, count_nonzero, _masked_median_1d, _masked_median, median, _masked_quantile_1d, _masked_quantile, quantile, percentile, array_equal, array_equiv, where, choose, select, piecewise, interp, lexsort, apply_over_axes, _array2string, array2string, array_str, masked_nanfunc, nanfunc"}}, {"code": "`VarianceUncertainty` should always be associated with an `NDData`-like\n    instance, either by creating it during initialization::\n\n        >>> from astropy.nddata import NDData, VarianceUncertainty\n        >>> ndd = NDData([1,2,3], unit='m',\n        ...              uncertainty=VarianceUncertainty([0.01, 0.01, 0.01]))\n        >>> ndd.uncertainty  # doctest: +FLOAT_CMP\n        VarianceUncertainty([0.01, 0.01, 0.01])\n\n    or by setting it manually on the `NDData` instance::\n\n        >>> ndd.uncertainty = VarianceUncertainty([0.04], unit='m^2', copy=True)\n        >>> ndd.uncertainty  # doctest: +FLOAT_CMP\n        VarianceUncertainty([0.04])\n\n    the uncertainty ``array`` can also be set directly::\n\n        >>> ndd.uncertainty.array = 4\n        >>> ndd.uncertainty\n        VarianceUncertainty(4)\n\n    .. note::\n        The unit will not be displayed.\n    \"\"\"\n\n    @property\n    def uncertainty_type(self):\n        \"\"\"``\"var\"`` : `VarianceUncertainty` implements variance.\"\"\"\n        return \"var\"\n\n    @property\n    def supports_correlated(self):\n        \"\"\"`True` : `VarianceUncertainty` allows to propagate correlated \\\n                    uncertainties.\n\n        ``correlation`` must be given, this class does not implement computing\n        it by itself.\n        \"\"\"", "metadata": {"file_name": "astropy/nddata/nduncertainty.py", "File Name": "astropy/nddata/nduncertainty.py", "Classes": "IncompatibleUncertaintiesException, MissingDataAssociationException, NDUncertainty, UnknownUncertainty, _VariancePropagationMixin, StdDevUncertainty, VarianceUncertainty, InverseVariance", "Functions": "_move_preserved_axes_first, _unravel_preserved_axes, from_variance_for_mean, _inverse"}}, {"code": "These values are\n        the same as those in `bbox_cutout`.\n\n    wcs : `~astropy.wcs.WCS` or None\n        A WCS object associated with the cutout array if a ``wcs``\n        was input.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from astropy.nddata.utils import Cutout2D\n    >>> from astropy import units as u\n    >>> data = np.arange(20.).reshape(5, 4)\n    >>> cutout1 = Cutout2D(data, (2, 2), (3, 3))\n    >>> print(cutout1.data)  # doctest: +FLOAT_CMP\n    [[ 5.  6.  7.]\n     [ 9. 10. 11.]\n     [13. 14. 15.]]\n\n    >>> print(cutout1.center_original)\n    (2.0, 2.0)\n    >>> print(cutout1.center_cutout)\n    (1.0, 1.0)\n    >>> print(cutout1.origin_original)\n    (1, 1)\n\n    >>> cutout2 = Cutout2D(data, (2, 2), 3)\n    >>> print(cutout2.data)  # doctest: +FLOAT_CMP\n    [[ 5.  6.  7.]\n     [ 9. 10. 11.]\n     [13. 14. 15.]]\n\n    >>> size = u.Quantity([3, 3], u.pixel)\n    >>> cutout3 = Cutout2D(data, (0, 0), size)\n    >>> print(cutout3.data)  # doctest: +FLOAT_CMP\n    [[0. 1.]\n     [4. 5.]]\n\n    >>> cutout4 = Cutout2D(data, (0, 0), (3 * u.pixel, 3))\n    >>> print(cutout4.data)  # doctest: +FLOAT_CMP\n    [[0. 1.]\n     [4. 5.]]", "metadata": {"file_name": "astropy/nddata/utils.py", "File Name": "astropy/nddata/utils.py", "Classes": "NoOverlapError, PartialOverlapError, Cutout2D", "Functions": "overlap_slices, extract_array, add_array, subpixel_indices"}}, {"code": "def _comparison_method(op):\n    \"\"\"\n    Create a comparison operator for MaskedNDArray.\n\n    Needed since for string dtypes the base operators bypass __array_ufunc__\n    and hence return unmasked results.\n    \"\"\"\n\n    def _compare(self, other):\n        other_data, other_mask = self._get_data_and_mask(other)\n        result = getattr(self.unmasked, op)(other_data)\n        if result is NotImplemented:\n            return NotImplemented\n        mask = self.mask | (other_mask if other_mask is not None else False)\n        return self._masked_result(result, mask, None)\n\n    return _compare\n\n\nclass MaskedIterator:\n    \"\"\"\n    Flat iterator object to iterate over Masked Arrays.\n\n    A `~astropy.utils.masked.MaskedIterator` iterator is returned by ``m.flat``\n    for any masked array ``m``.  It allows iterating over the array as if it\n    were a 1-D array, either in a for-loop or by calling its `next` method.\n\n    Iteration is done in C-contiguous style, with the last index varying the\n    fastest. The iterator can also be indexed using basic slicing or\n    advanced indexing.\n\n    Notes\n    -----\n    The design of `~astropy.utils.masked.MaskedIterator` follows that of\n    `~numpy.ma.core.MaskedIterator`.  It is not exported by the\n    `~astropy.utils.masked` module.  Instead of instantiating directly,\n    use the ``flat`` method in the masked array instance.\n    \"\"\"\n\n    def __init__(self, m):\n        self._masked = m\n        self._dataiter = m.unmasked.flat\n        self._maskiter = m.mask.flat\n\n    def __iter__(self):\n        return self\n\n    def __getitem__(self, indx):\n        out = self._dataiter.__getitem__(indx)\n        mask = self._maskiter.__getitem__(indx)\n        # For single elements, ndarray.flat.__getitem__ returns scalars; these\n        # need a new view as a Masked array.", "metadata": {"file_name": "astropy/utils/masked/core.py", "File Name": "astropy/utils/masked/core.py", "Classes": "Masked, MaskedInfoBase, MaskedNDArrayInfo, MaskedArraySubclassInfo, MaskedIterator, MaskedNDArray, MaskedRecarray", "Functions": "_comparison_method, _compare, argmin, argmax, argmin, argmax"}}], "traj": {"usage": {"embedding_tokens": 0}}}
{"instance_id": "django__django-10914", "found_files": ["django/setup.py", "django/__init__.py", "django/__main__.py", "django/shortcuts.py"], "node_info": [{"code": "if overlay_warning:\n    sys.stderr.write(\"\"\"\n\n========\nWARNING!\n========\n\nYou have just installed Django over top of an existing\ninstallation, without removing it first. Because of this,\nyour install may now include extraneous files from a\nprevious version that have since been removed from\nDjango. This is known to cause a variety of problems. You\nshould manually remove the\n\n%(existing_path)s\n\ndirectory and re-install Django.\n\n\"\"\" % {\"existing_path\": existing_path})", "metadata": {"file_name": "django/setup.py", "File Name": "django/setup.py", "Functions": "read"}}, {"code": "import os\nimport sys\nfrom distutils.sysconfig import get_python_lib\n\nfrom setuptools import find_packages, setup\n\nCURRENT_PYTHON = sys.version_info[:2]\nREQUIRED_PYTHON = (3, 6)\n\n# This check and everything above must remain compatible with Python 2.7.\nif CURRENT_PYTHON < REQUIRED_PYTHON:\n    sys.stderr.write(\"\"\"\n==========================\nUnsupported Python version\n==========================\n\nThis version of Django requires Python {}.{}, but you're trying to\ninstall it on Python {}.{}.\n\nThis may be because you are using a version of pip that doesn't\nunderstand the python_requires classifier. Make sure you\nhave pip >= 9.0 and setuptools >= 24.2, then try again:\n\n    $ python -m pip install --upgrade pip setuptools\n    $ python -m pip install django\n\nThis will install the latest version of Django which works on your\nversion of Python. If you can't upgrade your pip (or Python), request\nan older version of Django:\n\n    $ python -m pip install \"django<2\"\n\"\"\".format(*(REQUIRED_PYTHON + CURRENT_PYTHON)))\n    sys.exit(1)\n\n\n# Warn if we are installing over top of an existing installation. This can\n# cause issues where files that were deleted from a more recent Django are\n# still present in site-packages. See #18115.\noverlay_warning = False\nif \"install\" in sys.argv:\n    lib_paths = [get_python_lib()]\n    if lib_paths[0].startswith(\"/usr/lib/\"):\n        # We have to try also with an explicit prefix of /usr/local in order to\n        # catch Debian's custom user site-packages directory.\n        lib_paths.append(get_python_lib(prefix=\"/usr/local\"))\n    for lib_path in lib_paths:\n        existing_path = os.path.abspath(os.path.join(lib_path, \"django\"))\n        if os.path.exists(existing_path):\n            # We note the need for the warning here, but present it after the\n            # command is run, so it's more likely to be seen.\n            overlay_warning = True\n            break\n\n\nEXCLUDE_FROM_PACKAGES = ['django.conf.project_template',\n                         'django.conf.app_template',\n                         'django.bin']\n\n\n# Dynamically calculate the version based on django.VERSION.\nversion = __import__('django').get_version()", "metadata": {"file_name": "django/setup.py", "File Name": "django/setup.py", "Functions": "read"}}, {"code": "def read(fname):\n    with open(os.path.join(os.path.dirname(__file__), fname)) as f:\n        return f.read()\n\n\nsetup(\n    name='Django',\n    version=version,\n    python_requires='>={}.{}'.format(*REQUIRED_PYTHON),\n    url='https://www.djangoproject.com/',\n    author='Django Software Foundation',\n    author_email='foundation@djangoproject.com',\n    description=('A high-level Python Web framework that encourages '\n                 'rapid development and clean, pragmatic design.'),\n    long_description=read('README.rst'),\n    license='BSD',\n    packages=find_packages(exclude=EXCLUDE_FROM_PACKAGES),\n    include_package_data=True,\n    scripts=['django/bin/django-admin.py'],\n    entry_points={'console_scripts': [\n        'django-admin = django.core.management:execute_from_command_line',\n    ]},\n    install_requires=['pytz', 'sqlparse'],\n    extras_require={\n        \"bcrypt\": [\"bcrypt\"],\n        \"argon2\": [\"argon2-cffi >= 16.1.0\"],\n    },\n    zip_safe=False,\n    classifiers=[\n        'Development Status :: 2 - Pre-Alpha',\n        'Environment :: Web Environment',\n        'Framework :: Django',\n        'Intended Audience :: Developers',\n        'License :: OSI Approved :: BSD License',\n        'Operating System :: OS Independent',\n        'Programming Language :: Python',\n        'Programming Language :: Python :: 3',\n        'Programming Language :: Python :: 3.6',\n        'Programming Language :: Python :: 3.7',\n        'Programming Language :: Python :: 3 :: Only',\n        'Topic :: Internet :: WWW/HTTP',\n        'Topic :: Internet :: WWW/HTTP :: Dynamic Content',\n        'Topic :: Internet :: WWW/HTTP :: WSGI',\n        'Topic :: Software Development :: Libraries :: Application Frameworks',\n        'Topic :: Software Development :: Libraries :: Python Modules',\n    ],\n    project_urls={\n        'Documentation': 'https://docs.djangoproject.com/',\n        'Funding': 'https://www.djangoproject.com/fundraising/',\n        'Source': 'https://github.com/django/django',\n        'Tracker': 'https://code.djangoproject.com/',\n    },\n)", "metadata": {"file_name": "django/setup.py", "File Name": "django/setup.py", "Functions": "read"}}, {"code": "from django.utils.version import get_version\n\nVERSION = (3, 0, 0, 'alpha', 0)\n\n__version__ = get_version(VERSION)\n\n\ndef setup(set_prefix=True):\n    \"\"\"\n    Configure the settings (this happens as a side effect of accessing the\n    first setting), configure logging and populate the app registry.\n    Set the thread-local urlresolvers script prefix if `set_prefix` is True.\n    \"\"\"\n    from django.apps import apps\n    from django.conf import settings\n    from django.urls import set_script_prefix\n    from django.utils.log import configure_logging\n\n    configure_logging(settings.LOGGING_CONFIG, settings.LOGGING)\n    if set_prefix:\n        set_script_prefix(\n            '/' if settings.FORCE_SCRIPT_NAME is None else settings.FORCE_SCRIPT_NAME\n        )\n    apps.populate(settings.INSTALLED_APPS)", "metadata": {"file_name": "django/__init__.py", "File Name": "django/__init__.py", "Functions": "setup"}}, {"code": "\"\"\"\nInvokes django-admin when the django module is run as a script.\n\nExample: python -m django check\n\"\"\"\nfrom django.core import management\n\nif __name__ == \"__main__\":\n    management.execute_from_command_line()", "metadata": {"file_name": "django/__main__.py", "File Name": "django/__main__.py"}}, {"code": "def get_object_or_404(klass, *args, **kwargs):\n    \"\"\"\n    Use get() to return an object, or raise a Http404 exception if the object\n    does not exist.\n\n    klass may be a Model, Manager, or QuerySet object. All other passed\n    arguments and keyword arguments are used in the get() query.\n\n    Like with QuerySet.get(), MultipleObjectsReturned is raised if more than\n    one object is found.\n    \"\"\"\n    queryset = _get_queryset(klass)\n    if not hasattr(queryset, 'get'):\n        klass__name = klass.__name__ if isinstance(klass, type) else klass.__class__.__name__\n        raise ValueError(\n            \"First argument to get_object_or_404() must be a Model, Manager, \"\n            \"or QuerySet, not '%s'.\" % klass__name\n        )\n    try:\n        return queryset.get(*args, **kwargs)\n    except queryset.model.DoesNotExist:\n        raise Http404('No %s matches the given query.' % queryset.model._meta.object_name)\n\n\ndef get_list_or_404(klass, *args, **kwargs):\n    \"\"\"\n    Use filter() to return a list of objects, or raise a Http404 exception if\n    the list is empty.\n\n    klass may be a Model, Manager, or QuerySet object. All other passed\n    arguments and keyword arguments are used in the filter() query.\n    \"\"\"\n    queryset = _get_queryset(klass)\n    if not hasattr(queryset, 'filter'):\n        klass__name = klass.__name__ if isinstance(klass, type) else klass.__class__.__name__\n        raise ValueError(\n            \"First argument to get_list_or_404() must be a Model, Manager, or \"\n            \"QuerySet, not '%s'.\" % klass__name\n        )\n    obj_list = list(queryset.filter(*args, **kwargs))\n    if not obj_list:\n        raise Http404('No %s matches the given query.' % queryset.model._meta.object_name)\n    return obj_list", "metadata": {"file_name": "django/shortcuts.py", "File Name": "django/shortcuts.py", "Functions": "render, redirect, _get_queryset, get_object_or_404, get_list_or_404, resolve_url"}}, {"code": "def resolve_url(to, *args, **kwargs):\n    \"\"\"\n    Return a URL appropriate for the arguments passed.\n\n    The arguments could be:\n\n        * A model: the model's `get_absolute_url()` function will be called.\n\n        * A view name, possibly with arguments: `urls.reverse()` will be used\n          to reverse-resolve the name.\n\n        * A URL, which will be returned as-is.\n    \"\"\"\n    # If it's a model, use get_absolute_url()\n    if hasattr(to, 'get_absolute_url'):\n        return to.get_absolute_url()\n\n    if isinstance(to, Promise):\n        # Expand the lazy instance, as it can cause issues when it is passed\n        # further to some Python functions like urlparse.\n        to = str(to)\n\n    if isinstance(to, str):\n        # Handle relative URLs\n        if to.startswith(('./', '../')):\n            return to\n\n    # Next try a reverse URL resolution.\n    try:\n        return reverse(to, args=args, kwargs=kwargs)\n    except NoReverseMatch:\n        # If this is a callable, re-raise.\n        if callable(to):\n            raise\n        # If this doesn't \"feel\" like a URL, re-raise.\n        if '/' not in to and '.' not in to:\n            raise\n\n    # Finally, fall back and assume it's a URL\n    return to", "metadata": {"file_name": "django/shortcuts.py", "File Name": "django/shortcuts.py", "Functions": "render, redirect, _get_queryset, get_object_or_404, get_list_or_404, resolve_url"}}, {"code": "\"\"\"\nThis module collects helper functions and classes that \"span\" multiple levels\nof MVC. In other words, these functions/classes introduce controlled coupling\nfor convenience's sake.\n\"\"\"\nfrom django.http import (\n    Http404, HttpResponse, HttpResponsePermanentRedirect, HttpResponseRedirect,\n)\nfrom django.template import loader\nfrom django.urls import NoReverseMatch, reverse\nfrom django.utils.functional import Promise\n\n\ndef render(request, template_name, context=None, content_type=None, status=None, using=None):\n    \"\"\"\n    Return a HttpResponse whose content is filled with the result of calling\n    django.template.loader.render_to_string() with the passed arguments.\n    \"\"\"\n    content = loader.render_to_string(template_name, context, request, using=using)\n    return HttpResponse(content, content_type, status)\n\n\ndef redirect(to, *args, permanent=False, **kwargs):\n    \"\"\"\n    Return an HttpResponseRedirect to the appropriate URL for the arguments\n    passed.\n\n    The arguments could be:\n\n        * A model: the model's `get_absolute_url()` function will be called.\n\n        * A view name, possibly with arguments: `urls.reverse()` will be used\n          to reverse-resolve the name.\n\n        * A URL, which will be used as-is for the redirect location.\n\n    Issues a temporary redirect by default; pass permanent=True to issue a\n    permanent redirect.\n    \"\"\"\n    redirect_class = HttpResponsePermanentRedirect if permanent else HttpResponseRedirect\n    return redirect_class(resolve_url(to, *args, **kwargs))\n\n\ndef _get_queryset(klass):\n    \"\"\"\n    Return a QuerySet or a Manager.\n    Duck typing in action: any class with a `get()` method (for\n    get_object_or_404) or a `filter()` method (for get_list_or_404) might do\n    the job.\n    \"\"\"\n    # If it is a model class or anything else with ._default_manager\n    if hasattr(klass, '_default_manager'):\n        return klass._default_manager.all()\n    return klass", "metadata": {"file_name": "django/shortcuts.py", "File Name": "django/shortcuts.py", "Functions": "render, redirect, _get_queryset, get_object_or_404, get_list_or_404, resolve_url"}}], "traj": {"usage": {"embedding_tokens": 0}}}
{"instance_id": "astropy__astropy-6938", "found_files": ["astropy/io/fits/fitsrec.py", "astropy/io/fits/column.py", "astropy/io/fits/util.py", "astropy/io/fits/_numpy_hacks.py", "astropy/io/fits/scripts/fitsdiff.py", "astropy/io/fits/card.py", "astropy/io/fits/hdu/base.py", "astropy/io/fits/hdu/table.py", "astropy/io/fits/diff.py", "astropy/io/fits/hdu/hdulist.py", "astropy/io/fits/file.py", "astropy/io/fits/verify.py", "astropy/io/fits/convenience.py", "astropy/io/fits/hdu/image.py", "astropy/io/fits/scripts/fitscheck.py", "astropy/io/fits/hdu/groups.py", "astropy/io/fits/hdu/compressed.py", "astropy/io/fits/connect.py", "astropy/io/fits/__init__.py", "astropy/io/fits/fitstime.py"], "node_info": [{"code": "starts = self._coldefs.starts[:]\n        spans = self._coldefs.spans\n        format = self._coldefs[col_idx].format\n\n        # The the index of the \"end\" column of the record, beyond\n        # which we can't write\n        end = super().field(-1).itemsize\n        starts.append(end + starts[-1])\n\n        if col_idx > 0:\n            lead = starts[col_idx] - starts[col_idx - 1] - spans[col_idx - 1]\n        else:\n            lead = 0\n\n        if lead < 0:\n            warnings.warn('Column {!r} starting point overlaps the previous '\n                          'column.'.format(col_idx + 1))\n\n        trail = starts[col_idx + 1] - starts[col_idx] - spans[col_idx]\n\n        if trail < 0:\n            warnings.warn('Column {!r} ending point overlaps the next '\n                          'column.'.format(col_idx + 1))\n\n        # TODO: It would be nice if these string column formatting\n        # details were left to a specialized class, as is the case\n        # with FormatX and FormatP\n        if 'A' in format:\n            _pc = '{:'\n        else:\n            _pc = '{:>'\n\n        fmt = ''.join([_pc, format[1:], ASCII2STR[format[0]], '}',\n                       (' ' * trail)])\n\n        # Even if the format precision is 0, we should output a decimal point\n        # as long as there is space to do so--not including a decimal point in\n        # a float value is discouraged by the FITS Standard\n        trailing_decimal = (format.precision == 0 and\n                            format.format in ('F', 'E', 'D'))\n\n        # not using numarray.strings's num2char because the\n        # result is not allowed to expand (as C/Python does).\n        for jdx, value in enumerate(input_field):\n            value = fmt.format(value)\n            if len(value) > starts[col_idx + 1] - starts[col_idx]:\n                raise ValueError(\n                    \"Value {!r} does not fit into the output's itemsize of \"\n                    \"{}.", "metadata": {"file_name": "astropy/io/fits/fitsrec.py", "File Name": "astropy/io/fits/fitsrec.py", "Classes": "FITS_record, FITS_rec, _UnicodeArrayEncodeError", "Functions": "_get_recarray_field, _ascii_encode, _has_unicode_fields"}}, {"code": "if not n:\n                # The input column had an empty array, so just use the fill\n                # value\n                continue\n\n            field = _get_recarray_field(data, idx)\n            name = column.name\n            fitsformat = column.format\n            recformat = fitsformat.recformat\n\n            outarr = field[:n]\n            inarr = arr[:n]\n\n            if isinstance(recformat, _FormatX):\n                # Data is a bit array\n                if inarr.shape[-1] == recformat.repeat:\n                    _wrapx(inarr, outarr, recformat.repeat)\n                    continue\n            elif isinstance(recformat, _FormatP):\n                data._cache_field(name, _makep(inarr, field, recformat,\n                                               nrows=nrows))\n                continue\n            # TODO: Find a better way of determining that the column is meant\n            # to be FITS L formatted\n            elif recformat[-2:] == FITS2NUMPY['L'] and inarr.dtype == bool:\n                # column is boolean\n                # The raw data field should be filled with either 'T' or 'F'\n                # (not 0).  Use 'F' as a default\n                field[:] = ord('F')\n                # Also save the original boolean array in data._converted so\n                # that it doesn't have to be re-converted\n                converted = np.zeros(field.shape, dtype=bool)\n                converted[:n] = inarr\n                data._cache_field(name, converted)\n                # TODO: Maybe this step isn't necessary at all if _scale_back\n                # will handle it?", "metadata": {"file_name": "astropy/io/fits/fitsrec.py", "File Name": "astropy/io/fits/fitsrec.py", "Classes": "FITS_record, FITS_rec, _UnicodeArrayEncodeError", "Functions": "_get_recarray_field, _ascii_encode, _has_unicode_fields"}}, {"code": "if isinstance(recformat, _FormatX):\n            # special handling for the X format\n            return self._convert_x(field, recformat)\n\n        (_str, _bool, _number, _scale, _zero, bscale, bzero, dim) = \\\n            self._get_scale_factors(column)\n\n        indx = self.names.index(column.name)\n\n        # ASCII table, convert strings to numbers\n        # TODO:\n        # For now, check that these are ASCII columns by checking the coldefs\n        # type; in the future all columns (for binary tables, ASCII tables, or\n        # otherwise) should \"know\" what type they are already and how to handle\n        # converting their data from FITS format to native format and vice\n        # versa...\n        if not _str and isinstance(self._coldefs, _AsciiColDefs):\n            field = self._convert_ascii(column, field)\n\n        # Test that the dimensions given in dim are sensible; otherwise\n        # display a warning and ignore them\n        if dim:\n            # See if the dimensions already match, if not, make sure the\n            # number items will fit in the specified dimensions\n            if field.ndim > 1:\n                actual_shape = field.shape[1:]\n                if _str:\n                    actual_shape = actual_shape + (field.itemsize,)\n            else:\n                actual_shape = field.shape[0]\n\n            if dim == actual_shape:\n                # The array already has the correct dimensions, so we\n                # ignore dim and don't convert\n                dim = None\n            else:\n                nitems = reduce(operator.mul, dim)\n                if _str:\n                    actual_nitems = field.itemsize\n                elif len(field.shape) == 1:  # No repeat count in TFORMn, equivalent to 1\n                    actual_nitems = 1\n                else:\n                    actual_nitems = field.shape[1]\n                if nitems > actual_nitems:\n                    warnings.warn(\n                        'TDIM{} value {:d} does not fit with the size of '\n                        'the array items ({:d}).  TDIM{:d} will be ignored.'", "metadata": {"file_name": "astropy/io/fits/fitsrec.py", "File Name": "astropy/io/fits/fitsrec.py", "Classes": "FITS_record, FITS_rec, _UnicodeArrayEncodeError", "Functions": "_get_recarray_field, _ascii_encode, _has_unicode_fields"}}, {"code": "\".format(value, spans[col_idx]))\n\n            if trailing_decimal and value[0] == ' ':\n                # We have some extra space in the field for the trailing\n                # decimal point\n                value = value[1:] + '.'\n\n            output_field[jdx] = value\n\n        # Replace exponent separator in floating point numbers\n        if 'D' in format:\n            output_field.replace(encode_ascii('E'), encode_ascii('D'))\n\n\ndef _get_recarray_field(array, key):\n    \"\"\"\n    Compatibility function for using the recarray base class's field method.\n    This incorporates the legacy functionality of returning string arrays as\n    Numeric-style chararray objects.\n    \"\"\"\n\n    # Numpy >= 1.10.dev recarray no longer returns chararrays for strings\n    # This is currently needed for backwards-compatibility and for\n    # automatic truncation of trailing whitespace\n    field = np.recarray.field(array, key)\n    if (field.dtype.char in ('S', 'U') and\n            not isinstance(field, chararray.chararray)):\n        field = field.view(chararray.chararray)\n    return field\n\n\nclass _UnicodeArrayEncodeError(UnicodeEncodeError):\n    def __init__(self, encoding, object_, start, end, reason, index):\n        super().__init__(encoding, object_, start, end, reason)\n        self.index = index", "metadata": {"file_name": "astropy/io/fits/fitsrec.py", "File Name": "astropy/io/fits/fitsrec.py", "Classes": "FITS_record, FITS_rec, _UnicodeArrayEncodeError", "Functions": "_get_recarray_field, _ascii_encode, _has_unicode_fields"}}, {"code": "For example if recformat.dtype is 'a' we want\n                # an array of characters.\n                dtype = np.array([], dtype=recformat.dtype).dtype\n\n                if update_heap_pointers and name in self._converted:\n                    # The VLA has potentially been updated, so we need to\n                    # update the array descriptors\n                    raw_field[:] = 0  # reset\n                    npts = [len(arr) for arr in self._converted[name]]\n\n                    raw_field[:len(npts), 0] = npts\n                    raw_field[1:, 1] = (np.add.accumulate(raw_field[:-1, 0]) *\n                                        dtype.itemsize)\n                    raw_field[:, 1][:] += heapsize\n\n                heapsize += raw_field[:, 0].sum() * dtype.itemsize\n                # Even if this VLA has not been read or updated, we need to\n                # include the size of its constituent arrays in the heap size\n                # total\n\n            if isinstance(recformat, _FormatX) and name in self._converted:\n                _wrapx(self._converted[name], raw_field, recformat.repeat)\n                continue\n\n            _str, _bool, _number, _scale, _zero, bscale, bzero, _ = \\\n                self._get_scale_factors(column)\n\n            field = self._converted.get(name, raw_field)\n\n            # conversion for both ASCII and binary tables\n            if _number or _str:\n                if _number and (_scale or _zero) and column._physical_values:\n                    dummy = field.copy()\n                    if _zero:\n                        dummy -= bzero\n                    if _scale:\n                        dummy /= bscale\n                    # This will set the raw values in the recarray back to\n                    # their non-physical storage values, so the column should\n                    # be mark is not scaled\n                    column._physical_values = False\n                elif _str or isinstance(self._coldefs, _AsciiColDefs):\n                    dummy = field\n                else:\n                    continue\n\n                # ASCII table, convert numbers to strings\n                if isinstance(self._coldefs, _AsciiColDefs):\n                    self.", "metadata": {"file_name": "astropy/io/fits/fitsrec.py", "File Name": "astropy/io/fits/fitsrec.py", "Classes": "FITS_record, FITS_rec, _UnicodeArrayEncodeError", "Functions": "_get_recarray_field, _ascii_encode, _has_unicode_fields"}}, {"code": "This is because the code in BinTableHDU\n        # (and a few other places) does essentially the following:\n        #\n        # data._coldefs = columns  # The ColDefs object holding this Column\n        # for col in columns:\n        #     col.array = data.field(col.name)\n        #\n        # This way each columns .array attribute now points to the field in the\n        # table data.  It's actually a pretty confusing interface (since it\n        # replaces the array originally pointed to by .array), but it's the way\n        # things have been for a long, long time.\n        #\n        # However, this results, in *many* cases, in a reference cycle.\n        # Because the array returned by data.field(col.name), while sometimes\n        # an array that owns its own data, is usually like a slice of the\n        # original data.  It has the original FITS_rec as the array .base.\n        # This results in the following reference cycle (for the n-th column):\n        #\n        #    data -> data._coldefs -> data._coldefs[n] ->\n        #     data._coldefs[n].array -> data._coldefs[n].array.base -> data\n        #\n        # Because ndarray objects do not handled by Python's garbage collector\n        # the reference cycle cannot be broken.  Therefore the FITS_rec's\n        # refcount never goes to zero, its __del__ is never called, and its\n        # memory is never freed.  This didn't occur in *all* cases, but it did\n        # occur in many cases.\n        #\n        # To get around this, Column.array is no longer a simple attribute\n        # like it was previously.", "metadata": {"file_name": "astropy/io/fits/column.py", "File Name": "astropy/io/fits/column.py", "Classes": "Delayed, _BaseColumnFormat, _ColumnFormat, _AsciiColumnFormat, _FormatX, _FormatP, _FormatQ, ColumnAttribute, Column, ColDefs, _AsciiColDefs, _VLF", "Functions": "_get_index, _unwrapx, _wrapx, _makep, _parse_tformat, _parse_ascii_tformat, _parse_tdim, _scalar_to_format, _cmp_recformats, _convert_fits2record, _convert_record2fits, _dtype_to_recformat, _convert_ascii_format, convert_int"}}, {"code": "inarr = np.where(inarr == np.False_, ord('F'), ord('T'))\n            elif (columns[idx]._physical_values and\n                    columns[idx]._pseudo_unsigned_ints):\n                # Temporary hack...\n                bzero = column.bzero\n                converted = np.zeros(field.shape, dtype=inarr.dtype)\n                converted[:n] = inarr\n                data._cache_field(name, converted)\n                if n < nrows:\n                    # Pre-scale rows below the input data\n                    field[n:] = -bzero\n\n                inarr = inarr - bzero\n            elif isinstance(columns, _AsciiColDefs):\n                # Regardless whether the format is character or numeric, if the\n                # input array contains characters then it's already in the raw\n                # format for ASCII tables\n                if fitsformat._pseudo_logical:\n                    # Hack to support converting from 8-bit T/F characters\n                    # Normally the column array is a chararray of 1 character\n                    # strings, but we need to view it as a normal ndarray of\n                    # 8-bit ints to fill it with ASCII codes for 'T' and 'F'\n                    outarr = field.view(np.uint8, np.ndarray)[:n]\n                elif arr.dtype.kind not in ('S', 'U'):\n                    # Set up views of numeric columns with the appropriate\n                    # numeric dtype\n                    # Fill with the appropriate blanks for the column format\n                    data._cache_field(name, np.zeros(nrows, dtype=arr.dtype))\n                    outarr = data._converted[name][:n]\n\n                outarr[:] = inarr\n                continue\n\n            if inarr.shape != outarr.shape:\n                if (inarr.dtype.kind == outarr.dtype.kind and\n                        inarr.dtype.kind in ('U', 'S') and\n                        inarr.dtype != outarr.dtype):\n\n                    inarr_rowsize = inarr[0].size\n                    inarr = inarr.flatten().view(outarr.dtype)\n\n                # This is a special case to handle input arrays with\n                # non-trivial TDIMn.", "metadata": {"file_name": "astropy/io/fits/fitsrec.py", "File Name": "astropy/io/fits/fitsrec.py", "Classes": "FITS_record, FITS_rec, _UnicodeArrayEncodeError", "Functions": "_get_recarray_field, _ascii_encode, _has_unicode_fields"}}, {"code": "# NOTE: The *column* index may not be the same as the field index in\n        # the recarray, if the column is a phantom column\n        column = self.columns[key]\n        name = column.name\n        format = column.format\n\n        if format.dtype.itemsize == 0:\n            warnings.warn(\n                'Field {!r} has a repeat count of 0 in its format code, '\n                'indicating an empty field.'.format(key))\n            return np.array([], dtype=format.dtype)\n\n        # If field's base is a FITS_rec, we can run into trouble because it\n        # contains a reference to the ._coldefs object of the original data;\n        # this can lead to a circular reference; see ticket #49\n        base = self\n        while (isinstance(base, FITS_rec) and\n                isinstance(base.base, np.recarray)):\n            base = base.base\n        # base could still be a FITS_rec in some cases, so take care to\n        # use rec.recarray.field to avoid a potential infinite\n        # recursion\n        field = _get_recarray_field(base, name)\n\n        if name not in self._converted:\n            recformat = format.recformat\n            # TODO: If we're now passing the column to these subroutines, do we\n            # really need to pass them the recformat?\n            if isinstance(recformat, _FormatP):\n                # for P format\n                converted = self._convert_p(column, field, recformat)\n            else:\n                # Handle all other column data types which are fixed-width\n                # fields\n                converted = self._convert_other(column, field, recformat)\n\n            # Note: Never assign values directly into the self._converted dict;\n            # always go through self._cache_field; this way self._converted is\n            # only used to store arrays that are not already direct views of\n            # our own data.\n            self._cache_field(name, converted)\n            return converted\n\n        return self._converted[name]\n\n    def _cache_field(self, name, field):\n        \"\"\"\n        Do not store fields in _converted if one of its bases is self,\n        or if it has a common base with self.", "metadata": {"file_name": "astropy/io/fits/fitsrec.py", "File Name": "astropy/io/fits/fitsrec.py", "Classes": "FITS_record, FITS_rec, _UnicodeArrayEncodeError", "Functions": "_get_recarray_field, _ascii_encode, _has_unicode_fields"}}, {"code": ".format(indx + 1, self._coldefs[indx].dims,\n                                actual_nitems, indx + 1))\n                    dim = None\n\n        # further conversion for both ASCII and binary tables\n        # For now we've made columns responsible for *knowing* whether their\n        # data has been scaled, but we make the FITS_rec class responsible for\n        # actually doing the scaling\n        # TODO: This also needs to be fixed in the effort to make Columns\n        # responsible for scaling their arrays to/from FITS native values\n        if not column.ascii and column.format.p_format:\n            format_code = column.format.p_format\n        else:\n            # TODO: Rather than having this if/else it might be nice if the\n            # ColumnFormat class had an attribute guaranteed to give the format\n            # of actual values in a column regardless of whether the true\n            # format is something like P or Q\n            format_code = column.format.format\n\n        if (_number and (_scale or _zero) and not column._physical_values):\n            # This is to handle pseudo unsigned ints in table columns\n            # TODO: For now this only really works correctly for binary tables\n            # Should it work for ASCII tables as well?", "metadata": {"file_name": "astropy/io/fits/fitsrec.py", "File Name": "astropy/io/fits/fitsrec.py", "Classes": "FITS_record, FITS_rec, _UnicodeArrayEncodeError", "Functions": "_get_recarray_field, _ascii_encode, _has_unicode_fields"}}, {"code": "# This issue should have a workaround in Numpy too, but hasn't been\n    # implemented there yet: https://github.com/astropy/astropy/issues/839\n    #\n    # Apparently Windows has its own fwrite bug:\n    # https://github.com/numpy/numpy/issues/2256\n\n    if (sys.platform == 'darwin' and arr.nbytes >= _OSX_WRITE_LIMIT + 1 and\n            arr.nbytes % 4096 == 0):\n        # chunksize is a count of elements in the array, not bytes\n        chunksize = _OSX_WRITE_LIMIT // arr.itemsize\n    elif sys.platform.startswith('win'):\n        chunksize = _WIN_WRITE_LIMIT // arr.itemsize\n    else:\n        # Just pass the whole array to the write routine\n        return write(arr, outfile)\n\n    # Write one chunk at a time for systems whose fwrite chokes on large\n    # writes.\n    idx = 0\n    arr = arr.view(np.ndarray).flatten()\n    while idx < arr.nbytes:\n        write(arr[idx:idx + chunksize], outfile)\n        idx += chunksize", "metadata": {"file_name": "astropy/io/fits/util.py", "File Name": "astropy/io/fits/util.py", "Classes": "NotifierMixin, SigintHandler", "Functions": "first, itersubclasses, ignore_sigint, pairwise, encode_ascii, decode_ascii, isreadable, iswritable, isfile, fileobj_open, fileobj_name, fileobj_closed, fileobj_mode, _fileobj_normalize_mode, fileobj_is_binary, translate, fill, _array_from_file, _array_to_file, _array_to_file_like, _write_string, _convert_array, _unsigned_zero, _is_pseudo_unsigned, _is_int, _str_to_num, _words_group, _tmp_name, _get_array_mmap, _free_space_check, _extract_number, get_testdata_filepath, _rstrip_inplace, wrapped, maybe_fill"}}, {"code": "_scale_back_ascii(indx, dummy, raw_field)\n                # binary table string column\n                elif isinstance(raw_field, chararray.chararray):\n                    self._scale_back_strings(indx, dummy, raw_field)\n                # all other binary table columns\n                else:\n                    if len(raw_field) and isinstance(raw_field[0],\n                                                     np.integer):\n                        dummy = np.around(dummy)\n\n                    if raw_field.shape == dummy.shape:\n                        raw_field[:] = dummy\n                    else:\n                        # Reshaping the data is necessary in cases where the\n                        # TDIMn keyword was used to shape a column's entries\n                        # into arrays\n                        raw_field[:] = dummy.ravel().view(raw_field.dtype)\n\n                del dummy\n\n            # ASCII table does not have Boolean type\n            elif _bool and name in self._converted:\n                choices = (np.array([ord('F')], dtype=np.int8)[0],\n                           np.array([ord('T')], dtype=np.int8)[0])\n                raw_field[:] = np.choose(field, choices)\n\n        # Store the updated heapsize\n        self._heapsize = heapsize\n\n    def _scale_back_strings(self, col_idx, input_field, output_field):\n        # There are a few possibilities this has to be able to handle properly\n        # The input_field, which comes from the _converted column is of dtype\n        # 'Un' so that elements read out of the array are normal str\n        # objects (i.e. unicode strings)\n        #\n        # At the other end the *output_field* may also be of type 'S' or of\n        # type 'U'.  It will *usually* be of type 'S' because when reading\n        # an existing FITS table the raw data is just ASCII strings, and\n        # represented in Numpy as an S array.  However, when a user creates\n        # a new table from scratch, they *might* pass in a column containing\n        # unicode strings (dtype 'U').  Therefore the output_field of the\n        # raw array is actually a unicode array.", "metadata": {"file_name": "astropy/io/fits/fitsrec.py", "File Name": "astropy/io/fits/fitsrec.py", "Classes": "FITS_record, FITS_rec, _UnicodeArrayEncodeError", "Functions": "_get_recarray_field, _ascii_encode, _has_unicode_fields"}}, {"code": "reconst_func, reconst_func_args, state = super().__reduce__()\n\n        # Define FITS_rec-specific attrs that get added to state\n        column_state = []\n        meta = []\n\n        for attrs in ['_converted', '_heapoffset', '_heapsize', '_nfields',\n                      '_gap', '_uint', 'parnames', '_coldefs']:\n\n            with suppress(AttributeError):\n                # _coldefs can be Delayed, and file objects cannot be\n                # picked, it needs to be deepcopied first\n                if attrs == '_coldefs':\n                    column_state.append(self._coldefs.__deepcopy__(None))\n                else:\n                    column_state.append(getattr(self, attrs))\n                meta.append(attrs)\n\n        state = state + (column_state, meta)\n\n        return reconst_func, reconst_func_args, state\n\n    def __array_finalize__(self, obj):\n        if obj is None:\n            return\n\n        if isinstance(obj, FITS_rec):\n            self._character_as_bytes = obj._character_as_bytes\n\n        if isinstance(obj, FITS_rec) and obj.dtype == self.dtype:\n            self._converted = obj._converted\n            self._heapoffset = obj._heapoffset\n            self._heapsize = obj._heapsize\n            self._col_weakrefs = obj._col_weakrefs\n            self._coldefs = obj._coldefs\n            self._nfields = obj._nfields\n            self._gap = obj._gap\n            self._uint = obj._uint\n        elif self.dtype.fields is not None:\n            # This will allow regular ndarrays with fields, rather than\n            # just other FITS_rec objects\n            self._nfields = len(self.dtype.fields)\n            self._converted = {}\n\n            self._heapoffset = getattr(obj, '_heapoffset', 0)\n            self._heapsize = getattr(obj, '_heapsize', 0)\n\n            self._gap = getattr(obj, '_gap', 0)\n            self._uint = getattr(obj, '_uint', False)\n            self._col_weakrefs = weakref.WeakSet()\n            self._coldefs = ColDefs(self)\n\n            # Work around chicken-egg problem.", "metadata": {"file_name": "astropy/io/fits/fitsrec.py", "File Name": "astropy/io/fits/fitsrec.py", "Classes": "FITS_record, FITS_rec, _UnicodeArrayEncodeError", "Functions": "_get_recarray_field, _ascii_encode, _has_unicode_fields"}}, {"code": "NUMPY2FITS['f2'] = 'E'\n\n# This is the order in which values are converted to FITS types\n# Note that only double precision floating point/complex are supported\nFORMATORDER = ['L', 'B', 'I', 'J', 'K', 'D', 'M', 'A']\n\n# Convert single precision floating point/complex to double precision.\nFITSUPCONVERTERS = {'E': 'D', 'C': 'M'}\n\n# mapping from ASCII table TFORM data type to numpy data type\n# A: Character\n# I: Integer (32-bit)\n# J: Integer (64-bit; non-standard)\n# F: Float (64-bit; fixed decimal notation)\n# E: Float (64-bit; exponential notation)\n# D: Float (64-bit; exponential notation, always 64-bit by convention)\nASCII2NUMPY = {'A': 'a', 'I': 'i4', 'J': 'i8', 'F': 'f8', 'E': 'f8', 'D': 'f8'}\n\n# Maps FITS ASCII column format codes to the appropriate Python string\n# formatting codes for that type.\nASCII2STR = {'A': '', 'I': 'd', 'J': 'd', 'F': 'f', 'E': 'E', 'D': 'E'}\n\n# For each ASCII table format code, provides a default width (and decimal\n# precision) for when one isn't given explicitly in the column format\nASCII_DEFAULT_WIDTHS = {'A': (1, 0), 'I': (10, 0), 'J': (15, 0),\n                        'E': (15, 7), 'F': (16, 7), 'D': (25, 17)}", "metadata": {"file_name": "astropy/io/fits/column.py", "File Name": "astropy/io/fits/column.py", "Classes": "Delayed, _BaseColumnFormat, _ColumnFormat, _AsciiColumnFormat, _FormatX, _FormatP, _FormatQ, ColumnAttribute, Column, ColDefs, _AsciiColDefs, _VLF", "Functions": "_get_index, _unwrapx, _wrapx, _makep, _parse_tformat, _parse_ascii_tformat, _parse_tdim, _scalar_to_format, _cmp_recformats, _convert_fits2record, _convert_record2fits, _dtype_to_recformat, _convert_ascii_format, convert_int"}}, {"code": "We're\n        # just using them as a template, and returning a table filled with\n        # zeros/blanks\n        if fill:\n            return data\n\n        # Otherwise we have to fill the recarray with data from the input\n        # columns\n        for idx, column in enumerate(columns):\n            # For each column in the ColDef object, determine the number of\n            # rows in that column.  This will be either the number of rows in\n            # the ndarray associated with the column, or the number of rows\n            # given in the call to this function, which ever is smaller.  If\n            # the input FILL argument is true, the number of rows is set to\n            # zero so that no data is copied from the original input data.\n            arr = column.array\n\n            if arr is None:\n                array_size = 0\n            else:\n                array_size = len(arr)\n\n            n = min(array_size, nrows)\n\n            # TODO: At least *some* of this logic is mostly redundant with the\n            # _convert_foo methods in this class; see if we can eliminate some\n            # of that duplication.", "metadata": {"file_name": "astropy/io/fits/fitsrec.py", "File Name": "astropy/io/fits/fitsrec.py", "Classes": "FITS_record, FITS_rec, _UnicodeArrayEncodeError", "Functions": "_get_recarray_field, _ascii_encode, _has_unicode_fields"}}, {"code": "If it doesn't have a reasonable ASCII representation then\n        # raise an exception\n    else:\n        format, width, precision = _parse_ascii_tformat(format)\n\n        # This gives a sensible \"default\" dtype for a given ASCII\n        # format code\n        recformat = ASCII2NUMPY[format]\n\n        # The following logic is taken from CFITSIO:\n        # For integers, if the width <= 4 we can safely use 16-bit ints for all\n        # values [for the non-standard J format code just always force 64-bit]\n        if format == 'I' and width <= 4:\n            recformat = 'i2'\n        elif format == 'A':\n            recformat += str(width)\n\n        return recformat", "metadata": {"file_name": "astropy/io/fits/column.py", "File Name": "astropy/io/fits/column.py", "Classes": "Delayed, _BaseColumnFormat, _ColumnFormat, _AsciiColumnFormat, _FormatX, _FormatP, _FormatQ, ColumnAttribute, Column, ColDefs, _AsciiColDefs, _VLF", "Functions": "_get_index, _unwrapx, _wrapx, _makep, _parse_tformat, _parse_ascii_tformat, _parse_tdim, _scalar_to_format, _cmp_recformats, _convert_fits2record, _convert_record2fits, _dtype_to_recformat, _convert_ascii_format, convert_int"}}, {"code": "cls, args, state = dtype.__reduce__()\n\n    names, fields = state[3:5]\n    fields = fields.copy()\n\n    itemsize = 0  # We will re-determine the itemsize based on the type\n                  # of the field with the largest (offset + itemsize)\n\n    if fields is None or len(offsets) != len(names):\n        raise ValueError(\n            \"Dtype must be a structured dtype, and length of offsets list \"\n            \"must be the same as the number of fields.\")\n\n    for name, offset in zip(names, offsets):\n        field = fields[name]\n        itemsize = max(itemsize, offset + field[0].itemsize)\n\n        if offset != field[1]:\n            fields[name] = (field[0], offset)\n\n    new_typespec = '|V{0}'.format(itemsize)\n\n    new_state = state[:4] + (fields, itemsize) + state[6:]\n\n    new_dtype = cls(new_typespec, *args[1:])\n    new_dtype.__setstate__(new_state)\n\n    return new_dtype", "metadata": {"file_name": "astropy/io/fits/_numpy_hacks.py", "File Name": "astropy/io/fits/_numpy_hacks.py", "Functions": "realign_dtype"}}, {"code": "',\n            AstropyDeprecationWarning)\n        opts.rtol = opts.tolerance\n    if opts.rtol is None:\n        opts.rtol = 0.0\n    if opts.atol is None:\n        opts.atol = 0.0\n\n    if opts.exact_comparisons:\n        # override the options so that each is the most restrictive\n        opts.ignore_keywords = []\n        opts.ignore_comments = []\n        opts.ignore_fields = []\n        opts.rtol = 0.0\n        opts.atol = 0.0\n        opts.ignore_blanks = False\n        opts.ignore_blank_cards = False\n\n    if not opts.quiet:\n        setup_logging(opts.output_file)\n    files = match_files(args)\n\n    close_file = False\n    if opts.quiet:\n        out_file = None\n    elif opts.output_file:\n        out_file = open(opts.output_file, 'w')\n        close_file = True\n    else:\n        out_file = sys.stdout\n\n    identical = []\n    try:\n        for a, b in files:\n            # TODO: pass in any additional arguments here too\n            diff = fits.diff.FITSDiff(\n                a, b,\n                ignore_keywords=opts.ignore_keywords,\n                ignore_comments=opts.ignore_comments,\n                ignore_fields=opts.ignore_fields,\n                numdiffs=opts.numdiffs,\n                rtol=opts.rtol,\n                atol=opts.atol,\n                ignore_blanks=opts.ignore_blanks,\n                ignore_blank_cards=opts.ignore_blank_cards)\n\n            diff.report(fileobj=out_file)\n            identical.append(diff.identical)\n\n        return int(not all(identical))\n    finally:\n        if close_file:\n            out_file.close()\n        # Close the file if used for the logging output, and remove handlers to\n        # avoid having them multiple times for unit tests.\n        for handler in log.handlers:\n            if isinstance(handler, logging.FileHandler):\n                handler.close()\n            log.removeHandler(handler)", "metadata": {"file_name": "astropy/io/fits/scripts/fitsdiff.py", "File Name": "astropy/io/fits/scripts/fitsdiff.py", "Classes": "HelpFormatter, LevelFilter", "Functions": "handle_options, setup_logging, match_files, main, store_list"}}, {"code": "To avoid creating\n    # large temporary mask arrays, we loop over chunks (attempting to do that\n    # on a 1-D version of the array; large memory may still be needed in the\n    # unlikely case that a string array has small first dimension and cannot\n    # be represented as a contiguous 1-D array in memory).\n\n    dt = array.dtype\n\n    if dt.kind not in 'SU':\n        raise TypeError(\"This function can only be used on string arrays\")\n    # View the array as appropriate integers. The last dimension will\n    # equal the number of characters in each string.\n    bpc = 1 if dt.kind == 'S' else 4\n    dt_int = \"{0}{1}u{2}\".format(dt.itemsize // bpc, dt.byteorder, bpc)\n    b = array.view(dt_int, np.ndarray)\n    # For optimal speed, work in chunks of the internal ufunc buffer size.\n    bufsize = np.getbufsize()\n    # Attempt to have the strings as a 1-D array to give the chunk known size.\n    # Note: the code will work if this fails; the chunks will just be larger.\n    if b.ndim > 2:\n        try:\n            b.shape = -1, b.shape[-1]\n        except AttributeError:  # can occur for non-contiguous arrays\n            pass\n    for j in range(0, b.shape[0], bufsize):\n        c = b[j:j + bufsize]\n        # Mask which will tell whether we're in a sequence of trailing spaces.", "metadata": {"file_name": "astropy/io/fits/util.py", "File Name": "astropy/io/fits/util.py", "Classes": "NotifierMixin, SigintHandler", "Functions": "first, itersubclasses, ignore_sigint, pairwise, encode_ascii, decode_ascii, isreadable, iswritable, isfile, fileobj_open, fileobj_name, fileobj_closed, fileobj_mode, _fileobj_normalize_mode, fileobj_is_binary, translate, fill, _array_from_file, _array_to_file, _array_to_file_like, _write_string, _convert_array, _unsigned_zero, _is_pseudo_unsigned, _is_int, _str_to_num, _words_group, _tmp_name, _get_array_mmap, _free_space_check, _extract_number, get_testdata_filepath, _rstrip_inplace, wrapped, maybe_fill"}}, {"code": "format = column.format\n        recformat = ASCII2NUMPY[format[0]]\n        # if the string = TNULL, return ASCIITNULL\n        nullval = str(column.null).strip().encode('ascii')\n        if len(nullval) > format.width:\n            nullval = nullval[:format.width]\n\n        # Before using .replace make sure that any trailing bytes in each\n        # column are filled with spaces, and *not*, say, nulls; this causes\n        # functions like replace to potentially leave gibberish bytes in the\n        # array buffer.\n        dummy = np.char.ljust(field, format.width)\n        dummy = np.char.replace(dummy, encode_ascii('D'), encode_ascii('E'))\n        null_fill = encode_ascii(str(ASCIITNULL).rjust(format.width))\n\n        # Convert all fields equal to the TNULL value (nullval) to empty fields.\n        # TODO: These fields really should be conerted to NaN or something else undefined.\n        # Currently they are converted to empty fields, which are then set to zero.\n        dummy = np.where(np.char.strip(dummy) == nullval, null_fill, dummy)\n\n        # always replace empty fields, see https://github.com/astropy/astropy/pull/5394\n        if nullval != b'':\n            dummy = np.where(np.char.strip(dummy) == b'', null_fill, dummy)\n\n        try:\n            dummy = np.array(dummy, dtype=recformat)\n        except ValueError as exc:\n            indx = self.names.index(column.name)\n            raise ValueError(\n                '{}; the header may be missing the necessary TNULL{} '\n                'keyword or the table contains invalid data'.format(\n                    exc, indx + 1))\n\n        return dummy\n\n    def _convert_other(self, column, field, recformat):\n        \"\"\"Perform conversions on any other fixed-width column data types.\n\n        This may not perform any conversion at all if it's not necessary, in\n        which case the original column array is returned.\n        \"\"\"", "metadata": {"file_name": "astropy/io/fits/fitsrec.py", "File Name": "astropy/io/fits/fitsrec.py", "Classes": "FITS_record, FITS_rec, _UnicodeArrayEncodeError", "Functions": "_get_recarray_field, _ascii_encode, _has_unicode_fields"}}, {"code": "# By design each row of the outarray is 1-D, while each row of\n                # the input array may be n-D\n                if outarr.ndim > 1:\n                    # The normal case where the first dimension is the rows\n                    inarr_rowsize = inarr[0].size\n                    inarr = inarr.reshape(n, inarr_rowsize)\n                    outarr[:, :inarr_rowsize] = inarr\n                else:\n                    # Special case for strings where the out array only has one\n                    # dimension (the second dimension is rolled up into the\n                    # strings\n                    outarr[:n] = inarr.ravel()\n            else:\n                outarr[:] = inarr\n\n        # Now replace the original column array references with the new\n        # fields\n        # This is required to prevent the issue reported in\n        # https://github.com/spacetelescope/PyFITS/issues/99\n        for idx in range(len(columns)):\n            columns._arrays[idx] = data.field(idx)\n\n        return data\n\n    def __repr__(self):\n        # Force use of the normal ndarray repr (rather than the new\n        # one added for recarray in Numpy 1.10) for backwards compat\n        return np.ndarray.__repr__(self)\n\n    def __getitem__(self, key):\n        if self._coldefs is None:\n            return super().__getitem__(key)\n\n        if isinstance(key, str):\n            return self.field(key)\n\n        # Have to view as a recarray then back as a FITS_rec, otherwise the\n        # circular reference fix/hack in FITS_rec.field() won't preserve\n        # the slice.\n        out = self.view(np.recarray)[key]\n        if type(out) is not np.recarray:\n            # Oops, we got a single element rather than a view. In that case,\n            # return a Record, which has no __getstate__ and is more efficient.", "metadata": {"file_name": "astropy/io/fits/fitsrec.py", "File Name": "astropy/io/fits/fitsrec.py", "Classes": "FITS_record, FITS_rec, _UnicodeArrayEncodeError", "Functions": "_get_recarray_field, _ascii_encode, _has_unicode_fields"}}, {"code": "valid = {}\n        invalid = {}\n\n        format, recformat = cls._determine_formats(format, start, dim, ascii)\n        valid.update(format=format, recformat=recformat)\n\n        # Currently we don't have any validation for name, unit, bscale, or\n        # bzero so include those by default\n        # TODO: Add validation for these keywords, obviously\n        for k, v in [('name', name), ('unit', unit), ('bscale', bscale),\n                     ('bzero', bzero)]:\n            if v is not None and v != '':\n                valid[k] = v\n\n        # Validate null option\n        # Note: Enough code exists that thinks empty strings are sensible\n        # inputs for these options that we need to treat '' as None\n        if null is not None and null != '':\n            msg = None\n            if isinstance(format, _AsciiColumnFormat):\n                null = str(null)\n                if len(null) > format.width:\n                    msg = (\n                        \"ASCII table null option (TNULLn) is longer than \"\n                        \"the column's character width and will be truncated \"\n                        \"(got {!r}).\".format(null))\n            else:\n                tnull_formats = ('B', 'I', 'J', 'K')\n\n                if not _is_int(null):\n                    # Make this an exception instead of a warning, since any\n                    # non-int value is meaningless\n                    msg = (\n                        'Column null option (TNULLn) must be an integer for '\n                        'binary table columns (got {!r}).  The invalid value '\n                        'will be ignored for the purpose of formatting '\n                        'the data in this column.", "metadata": {"file_name": "astropy/io/fits/column.py", "File Name": "astropy/io/fits/column.py", "Classes": "Delayed, _BaseColumnFormat, _ColumnFormat, _AsciiColumnFormat, _FormatX, _FormatP, _FormatQ, ColumnAttribute, Column, ColDefs, _AsciiColDefs, _VLF", "Functions": "_get_index, _unwrapx, _wrapx, _makep, _parse_tformat, _parse_ascii_tformat, _parse_tdim, _scalar_to_format, _cmp_recformats, _convert_fits2record, _convert_record2fits, _dtype_to_recformat, _convert_ascii_format, convert_int"}}, {"code": "# TODO: This should be checked by the FITS verification code\n        if dim is not None and dim != '':\n            msg = None\n            dims_tuple = tuple()\n            # NOTE: If valid, the dim keyword's value in the the valid dict is\n            # a tuple, not the original string; if invalid just the original\n            # string is returned\n            if isinstance(format, _AsciiColumnFormat):\n                msg = (\n                    'Column dim option (TDIMn) is not allowed for ASCII table '\n                    'columns (got {!r}).  The invalid keyword will be ignored '\n                    'for the purpose of formatting this column.'.format(dim))\n\n            elif isinstance(dim, str):\n                dims_tuple = _parse_tdim(dim)\n            elif isinstance(dim, tuple):\n                dims_tuple = dim\n            else:\n                msg = (\n                    \"`dim` argument must be a string containing a valid value \"\n                    \"for the TDIMn header keyword associated with this column, \"\n                    \"or a tuple containing the C-order dimensions for the \"\n                    \"column.  The invalid value will be ignored for the purpose \"\n                    \"of formatting this column.\")\n\n            if dims_tuple:\n                if reduce(operator.mul, dims_tuple) > format.repeat:\n                    msg = (\n                        \"The repeat count of the column format {!r} for column {!r} \"\n                        \"is fewer than the number of elements per the TDIM \"\n                        \"argument {!r}.  The invalid TDIMn value will be ignored \"\n                        \"for the purpose of formatting this column.", "metadata": {"file_name": "astropy/io/fits/column.py", "File Name": "astropy/io/fits/column.py", "Classes": "Delayed, _BaseColumnFormat, _ColumnFormat, _AsciiColumnFormat, _FormatX, _FormatP, _FormatQ, ColumnAttribute, Column, ColDefs, _AsciiColDefs, _VLF", "Functions": "_get_index, _unwrapx, _wrapx, _makep, _parse_tformat, _parse_ascii_tformat, _parse_tdim, _scalar_to_format, _cmp_recformats, _convert_fits2record, _convert_record2fits, _dtype_to_recformat, _convert_ascii_format, convert_int"}}, {"code": "recformat))\n            elif 'L' in format:\n                # boolean needs to be scaled back to storage values ('T', 'F')\n                if array.dtype == np.dtype('bool'):\n                    return np.where(array == np.False_, ord('F'), ord('T'))\n                else:\n                    return np.where(array == 0, ord('F'), ord('T'))\n            elif 'X' in format:\n                return _convert_array(array, np.dtype('uint8'))\n            else:\n                # Preserve byte order of the original array for now; see #77\n                numpy_format = array.dtype.byteorder + format.recformat\n\n                # Handle arrays passed in as unsigned ints as pseudo-unsigned\n                # int arrays; blatantly tacked in here for now--we need columns\n                # to have explicit knowledge of whether they treated as\n                # pseudo-unsigned\n                bzeros = {2: np.uint16(2**15), 4: np.uint32(2**31),\n                          8: np.uint64(2**63)}\n                if (array.dtype.kind == 'u' and\n                        array.dtype.itemsize in bzeros and\n                        self.bscale in (1, None, '') and\n                        self.bzero == bzeros[array.dtype.itemsize]):\n                    # Basically the array is uint, has scale == 1.0, and the\n                    # bzero is the appropriate value for a pseudo-unsigned\n                    # integer of the input dtype, then go ahead and assume that\n                    # uint is assumed\n                    numpy_format = numpy_format.replace('i', 'u')\n                    self._pseudo_unsigned_ints = True\n\n                # The .base here means we're dropping the shape information,", "metadata": {"file_name": "astropy/io/fits/column.py", "File Name": "astropy/io/fits/column.py", "Classes": "Delayed, _BaseColumnFormat, _ColumnFormat, _AsciiColumnFormat, _FormatX, _FormatP, _FormatQ, ColumnAttribute, Column, ColDefs, _AsciiColDefs, _VLF", "Functions": "_get_index, _unwrapx, _wrapx, _makep, _parse_tformat, _parse_ascii_tformat, _parse_tdim, _scalar_to_format, _cmp_recformats, _convert_fits2record, _convert_record2fits, _dtype_to_recformat, _convert_ascii_format, convert_int"}}, {"code": "If\n            `False`, copy the data from input, undefined cells will still\n            be filled with zeros/blanks.\n        \"\"\"\n\n        if not isinstance(columns, ColDefs):\n            columns = ColDefs(columns)\n\n        # read the delayed data\n        for column in columns:\n            arr = column.array\n            if isinstance(arr, Delayed):\n                if arr.hdu.data is None:\n                    column.array = None\n                else:\n                    column.array = _get_recarray_field(arr.hdu.data,\n                                                       arr.field)\n        # Reset columns._arrays (which we may want to just do away with\n        # altogether\n        del columns._arrays\n\n        # use the largest column shape as the shape of the record\n        if nrows == 0:\n            for arr in columns._arrays:\n                if arr is not None:\n                    dim = arr.shape[0]\n                else:\n                    dim = 0\n                if dim > nrows:\n                    nrows = dim\n\n        raw_data = np.empty(columns.dtype.itemsize * nrows, dtype=np.uint8)\n        raw_data.fill(ord(columns._padding_byte))\n        data = np.recarray(nrows, dtype=columns.dtype, buf=raw_data).view(cls)\n        data._character_as_bytes = character_as_bytes\n\n        # Make sure the data is a listener for changes to the columns\n        columns._add_listener(data)\n\n        # Previously this assignment was made from hdu.columns, but that's a\n        # bug since if a _TableBaseHDU has a FITS_rec in its .data attribute\n        # the _TableBaseHDU.columns property is actually returned from\n        # .data._coldefs, so this assignment was circular!  Don't make that\n        # mistake again.\n        # All of this is an artifact of the fragility of the FITS_rec class,\n        # and that it can't just be initialized by columns...\n        data._coldefs = columns\n\n        # If fill is True we don't copy anything from the column arrays.", "metadata": {"file_name": "astropy/io/fits/fitsrec.py", "File Name": "astropy/io/fits/fitsrec.py", "Classes": "FITS_record, FITS_rec, _UnicodeArrayEncodeError", "Functions": "_get_recarray_field, _ascii_encode, _has_unicode_fields"}}, {"code": "return self._record_type(self, key)\n\n        # We got a view; change it back to our class, and add stuff\n        out = out.view(type(self))\n        out._coldefs = ColDefs(self._coldefs)\n        arrays = []\n        out._converted = {}\n        for idx, name in enumerate(self._coldefs.names):\n            #\n            # Store the new arrays for the _coldefs object\n            #\n            arrays.append(self._coldefs._arrays[idx][key])\n\n            # Ensure that the sliced FITS_rec will view the same scaled\n            # columns as the original; this is one of the few cases where\n            # it is not necessary to use _cache_field()\n            if name in self._converted:\n                dummy = self._converted[name]\n                field = np.ndarray.__getitem__(dummy, key)\n                out._converted[name] = field\n\n        out._coldefs._arrays = arrays\n        return out\n\n    def __setitem__(self, key, value):\n        if self._coldefs is None:\n            return super().__setitem__(key, value)\n\n        if isinstance(key, str):\n            self[key][:] = value\n            return\n\n        if isinstance(key, slice):\n            end = min(len(self), key.stop or len(self))\n            end = max(0, end)\n            start = max(0, key.start or 0)\n            end = min(end, start + len(value))\n\n            for idx in range(start, end):\n                self.__setitem__(idx, value[idx - start])\n            return\n\n        if isinstance(value, FITS_record):\n            for idx in range(self._nfields):\n                self.field(self.names[idx])[key] = value.field(self.names[idx])\n        elif isinstance(value, (tuple, list, np.void)):\n            if self._nfields == len(value):\n                for idx in range(self._nfields):\n                    self.field(idx)[key] = value[idx]\n            else:\n                raise ValueError('Input tuple or list required to have {} '\n                                 'elements.'.format(self._nfields))\n        else:\n            raise TypeError('Assignment requires a FITS_record, tuple, or '\n                            'list as input.')", "metadata": {"file_name": "astropy/io/fits/fitsrec.py", "File Name": "astropy/io/fits/fitsrec.py", "Classes": "FITS_record, FITS_rec, _UnicodeArrayEncodeError", "Functions": "_get_recarray_field, _ascii_encode, _has_unicode_fields"}}, {"code": "'.format(self.keyword))\n\n        # Don't try to verify cards that already don't meet any recognizable\n        # standard\n        if self._invalid:\n            return errs\n\n        # verify the equal sign position\n        if (self.keyword not in self._commentary_keywords and\n            (self._image and self._image[:9].upper() != 'HIERARCH ' and\n             self._image.find('=') != 8)):\n            errs.append(self.run_option(\n                option,\n                err_text='Card {!r} is not FITS standard (equal sign not '\n                         'at column 8).'.format(self.keyword),\n                fix_text=fix_text,\n                fix=self._fix_value))\n\n        # verify the key, it is never fixable\n        # always fix silently the case where \"=\" is before column 9,\n        # since there is no way to communicate back to the _keys.\n        if ((self._image and self._image[:8].upper() == 'HIERARCH') or\n                self._hierarch):\n            pass\n        else:\n            if self._image:\n                # PyFITS will auto-uppercase any standard keyword, so lowercase\n                # keywords can only occur if they came from the wild\n                keyword = self._split()[0]\n                if keyword != keyword.upper():\n                    # Keyword should be uppercase unless it's a HIERARCH card\n                    errs.append(self.run_option(\n                        option,\n                        err_text='Card keyword {!r} is not upper case.'.format(\n                                  keyword),\n                        fix_text=fix_text,\n                        fix=self._fix_keyword))\n\n            keyword = self.keyword\n            if self.field_specifier:\n                keyword = keyword.split('.", "metadata": {"file_name": "astropy/io/fits/card.py", "File Name": "astropy/io/fits/card.py", "Classes": "Undefined, Card", "Functions": "_int_or_float, _format_float, _pad"}}, {"code": "*([deDE] *[+-]? *\\d+)?'\n    _numr_FSC = r'[+-]?' + _digits_FSC\n    _numr_NFSC = r'[+-]? *' + _digits_NFSC\n\n    # This regex helps delete leading zeros from numbers, otherwise\n    # Python might evaluate them as octal values (this is not-greedy, however,\n    # so it may not strip leading zeros from a float, which is fine)\n    _number_FSC_RE = re.compile(r'(?P<sign>[+-])?0*?(?P<digt>{})'.format(\n            _digits_FSC))\n    _number_NFSC_RE = re.compile(r'(?P<sign>[+-])? *0*?(?P<digt>{})'.format(\n            _digits_NFSC))\n\n    # FSC commentary card string which must contain printable ASCII characters.\n    # Note: \\Z matches the end of the string without allowing newlines\n    _ascii_text_re = re.compile(r'[ -~]*\\Z')\n\n    # Checks for a valid value/comment string.  It returns a match object\n    # for a valid value/comment string.\n    # The valu group will return a match if a FITS string, boolean,\n    # number, or complex value is found, otherwise it will return\n    # None, meaning the keyword is undefined.  The comment field will\n    # return a match if the comment separator is found, though the\n    # comment maybe an empty string.\n    _value_FSC_RE = re.compile(\n        r'(?P<valu_field> *'\n            r'(?P<valu>'\n\n                #  The <strg> regex is not correct for all cases, but\n                #  it comes pretty darn close.  It appears to find the\n                #  end of a string rather well, but will accept\n                #  strings with an odd number of single quotes,\n                #  instead of issuing an error.  The FITS standard\n                #  appears vague on this issue and only states that a\n                #  string should not end with two single quotes,\n                #  whereas it should not end with an even number of\n                #  quotes to be precise.", "metadata": {"file_name": "astropy/io/fits/card.py", "File Name": "astropy/io/fits/card.py", "Classes": "Undefined, Card", "Functions": "_int_or_float, _format_float, _pad"}}, {"code": "elif (isinstance(s, np.ndarray) and\n          issubclass(s.dtype.type, np.bytes_)):\n        # np.char.encode/decode annoyingly don't preserve the type of the\n        # array, hence the view() call\n        # It also doesn't necessarily preserve widths of the strings,\n        # hence the astype()\n        if s.size == 0:\n            # Numpy apparently also has a bug that if a string array is\n            # empty calling np.char.decode on it returns an empty float64\n            # array wth\n            dt = s.dtype.str.replace('S', 'U')\n            ns = np.array([], dtype=dt).view(type(s))\n        else:\n            ns = np.char.decode(s, 'ascii').view(type(s))\n        if ns.dtype.itemsize / 4 != s.dtype.itemsize:\n            ns = ns.astype((np.str_, s.dtype.itemsize))\n        return ns\n    elif (isinstance(s, np.ndarray) and\n          not issubclass(s.dtype.type, np.str_)):\n        # Don't silently pass through on non-string arrays; we don't want\n        # to hide errors where things that are not stringy are attempting\n        # to be decoded\n        raise TypeError('string operation on non-string array')\n    return s", "metadata": {"file_name": "astropy/io/fits/util.py", "File Name": "astropy/io/fits/util.py", "Classes": "NotifierMixin, SigintHandler", "Functions": "first, itersubclasses, ignore_sigint, pairwise, encode_ascii, decode_ascii, isreadable, iswritable, isfile, fileobj_open, fileobj_name, fileobj_closed, fileobj_mode, _fileobj_normalize_mode, fileobj_is_binary, translate, fill, _array_from_file, _array_to_file, _array_to_file_like, _write_string, _convert_array, _unsigned_zero, _is_pseudo_unsigned, _is_int, _str_to_num, _words_group, _tmp_name, _get_array_mmap, _free_space_check, _extract_number, get_testdata_filepath, _rstrip_inplace, wrapped, maybe_fill"}}, {"code": "#\n                #  Note that a non-greedy match is done for a string,\n                #  since a greedy match will find a single-quote after\n                #  the comment separator resulting in an incorrect\n                #  match.\n                r'\\'(?P<strg>([ -~]+?|\\'\\'|)) *?\\'(?=$|/| )|'\n                r'(?P<bool>[FT])|'\n                r'(?P<numr>' + _numr_FSC + r')|'\n                r'(?P<cplx>\\( *'\n                    r'(?P<real>' + _numr_FSC + r') *, *'\n                    r'(?P<imag>' + _numr_FSC + r') *\\))'\n            r')? *)'\n        r'(?P<comm_field>'\n            r'(?P<sepr>/ *)'\n            r'(?P<comm>[!-~][ -~]*)?'\n        r')?$')\n\n    _value_NFSC_RE = re.compile(\n        r'(?P<valu_field> *'\n            r'(?P<valu>'\n                r'\\'(?P<strg>([ -~]+?|\\'\\'|) *?)\\'(?=$|/| )|'\n                r'(?P<bool>[FT])|'\n                r'(?P<numr>' + _numr_NFSC + r')|'\n                r'(?P<cplx>\\( *'\n                    r'(?P<real>' + _numr_NFSC + r') *, *'\n                    r'(?P<imag>' + _numr_NFSC + r') *\\))'\n            r')? *)'\n        r'(?P<comm_field>'\n            r'(?P<sepr>/ *)'\n            r'(?P<comm>(.|\\n)*)'\n        r')?$')\n\n    _rvkc_identifier = r'[a-zA-Z_]\\w*'\n    _rvkc_field = _rvkc_identifier + r'(\\.\\d+)?'\n    _rvkc_field_specifier_s = r'{}(\\.", "metadata": {"file_name": "astropy/io/fits/card.py", "File Name": "astropy/io/fits/card.py", "Classes": "Undefined, Card", "Functions": "_int_or_float, _format_float, _pad"}}, {"code": "if isfile(infile):\n\n        global CHUNKED_FROMFILE\n        if CHUNKED_FROMFILE is None:\n            if (sys.platform == 'darwin' and\n                    LooseVersion(platform.mac_ver()[0]) < LooseVersion('10.9')):\n                CHUNKED_FROMFILE = True\n            else:\n                CHUNKED_FROMFILE = False\n\n        if CHUNKED_FROMFILE:\n            chunk_size = int(1024 ** 3 / dtype.itemsize)  # 1Gb to be safe\n            if count < chunk_size:\n                return np.fromfile(infile, dtype=dtype, count=count)\n            else:\n                array = np.empty(count, dtype=dtype)\n                for beg in range(0, count, chunk_size):\n                    end = min(count, beg + chunk_size)\n                    array[beg:end] = np.fromfile(infile, dtype=dtype, count=end - beg)\n                return array\n        else:\n            return np.fromfile(infile, dtype=dtype, count=count)\n    else:\n        # treat as file-like object with \"read\" method; this includes gzip file\n        # objects, because numpy.fromfile just reads the compressed bytes from\n        # their underlying file object, instead of the decompressed bytes\n        read_size = np.dtype(dtype).itemsize * count\n        s = infile.read(read_size)\n        array = np.frombuffer(s, dtype=dtype, count=count)\n        # copy is needed because np.frombuffer returns a read-only view of the\n        # underlying buffer\n        array = array.copy()\n        return array", "metadata": {"file_name": "astropy/io/fits/util.py", "File Name": "astropy/io/fits/util.py", "Classes": "NotifierMixin, SigintHandler", "Functions": "first, itersubclasses, ignore_sigint, pairwise, encode_ascii, decode_ascii, isreadable, iswritable, isfile, fileobj_open, fileobj_name, fileobj_closed, fileobj_mode, _fileobj_normalize_mode, fileobj_is_binary, translate, fill, _array_from_file, _array_to_file, _array_to_file_like, _write_string, _convert_array, _unsigned_zero, _is_pseudo_unsigned, _is_int, _str_to_num, _words_group, _tmp_name, _get_array_mmap, _free_space_check, _extract_number, get_testdata_filepath, _rstrip_inplace, wrapped, maybe_fill"}}, {"code": "\".format(coord_unit))\n\n            if msg is None:\n                valid['coord_unit'] = coord_unit\n            else:\n                invalid['coord_unit'] = (coord_unit, msg)\n\n        for k, v in [('coord_ref_point', coord_ref_point),\n                     ('coord_ref_value', coord_ref_value),\n                     ('coord_inc', coord_inc)]:\n            if v is not None and v != '':\n                msg = None\n                if not isinstance(v, numbers.Real):\n                    msg = (\n                        \"Column {} option ({}n) must be a real floating type (got {!r}). \"\n                        \"The invalid value will be ignored for the purpose of formatting \"\n                        \"the data in this column.\".format(k, ATTRIBUTE_TO_KEYWORD[k], v))\n\n                if msg is None:\n                    valid[k] = v\n                else:\n                    invalid[k] = (v, msg)\n\n        if time_ref_pos is not None and time_ref_pos != '':\n            msg=None\n            if not isinstance(time_ref_pos, str):\n                msg = (\n                    \"Time coordinate reference position option (TRPOSn) must be \"\n                    \"a string (got {!r}). The invalid keyword will be ignored for \"\n                    \"the purpose of formatting this column.", "metadata": {"file_name": "astropy/io/fits/column.py", "File Name": "astropy/io/fits/column.py", "Classes": "Delayed, _BaseColumnFormat, _ColumnFormat, _AsciiColumnFormat, _FormatX, _FormatP, _FormatQ, ColumnAttribute, Column, ColDefs, _AsciiColDefs, _VLF", "Functions": "_get_index, _unwrapx, _wrapx, _makep, _parse_tformat, _parse_ascii_tformat, _parse_tdim, _scalar_to_format, _cmp_recformats, _convert_fits2record, _convert_record2fits, _dtype_to_recformat, _convert_ascii_format, convert_int"}}, {"code": "data.dtype)):\n            for keyword in ('BSCALE', 'BZERO'):\n                with suppress(KeyError):\n                    del self._header[keyword]\n\n    def _writeheader(self, fileobj):\n        offset = 0\n        if not fileobj.simulateonly:\n            with suppress(AttributeError, OSError):\n                offset = fileobj.tell()\n\n            self._header.tofile(fileobj)\n\n            try:\n                size = fileobj.tell() - offset\n            except (AttributeError, OSError):\n                size = len(str(self._header))\n        else:\n            size = len(str(self._header))\n\n        return offset, size\n\n    def _writedata(self, fileobj):\n        # TODO: A lot of the simulateonly stuff should be moved back into the\n        # _File class--basically it should turn write and flush into a noop\n        offset = 0\n        size = 0\n\n        if not fileobj.simulateonly:\n            fileobj.flush()\n            try:\n                offset = fileobj.tell()\n            except OSError:\n                offset = 0\n\n        if self._data_loaded or self._data_needs_rescale:\n            if self.data is not None:\n                size += self._writedata_internal(fileobj)\n            # pad the FITS data block\n            if size > 0:\n                padding = _pad_length(size) * self._padding_byte\n                # TODO: Not that this is ever likely, but if for some odd\n                # reason _padding_byte is > 0x80 this will fail; but really if\n                # somebody's custom fits format is doing that, they're doing it\n                # wrong and should be reprimanded harshly.", "metadata": {"file_name": "astropy/io/fits/hdu/base.py", "File Name": "astropy/io/fits/hdu/base.py", "Classes": "_Delayed, InvalidHDUException, _BaseHDUMeta, _BaseHDU, _CorruptedHDU, _NonstandardHDU, _ValidHDU, ExtensionHDU, NonstandardExtHDU", "Functions": "_hdu_class_from_header, block_iter, fix, fix, fix, fix, fix"}}, {"code": "# TODO: I need to read this code a little more closely\n                    # again, but I think it can be simplified quite a bit with\n                    # the use of some appropriate utility functions\n                    update_coldefs = {}\n                    if 'u' in [data.dtype[k].kind for k in data.dtype.names]:\n                        self._uint = True\n                        bzeros = {2: np.uint16(2**15), 4: np.uint32(2**31),\n                                  8: np.uint64(2**63)}\n\n                        new_dtype = [\n                            (k, data.dtype[k].kind.replace('u', 'i') +\n                            str(data.dtype[k].itemsize))\n                            for k in data.dtype.names]\n\n                        new_data = np.zeros(data.shape, dtype=new_dtype)\n\n                        for k in data.dtype.fields:\n                            dtype = data.dtype[k]\n                            if dtype.kind == 'u':\n                                new_data[k] = data[k] - bzeros[dtype.itemsize]\n                                update_coldefs[k] = bzeros[dtype.itemsize]\n                            else:\n                                new_data[k] = data[k]\n                        self.data = new_data.view(self._data_type)\n                        # Uck...\n                        self.data._uint = True\n                    else:\n                        self.data = data.view(self._data_type)\n                    for k in update_coldefs:\n                        indx = _get_index(self.data.names, k)\n                        self.data._coldefs[indx].bzero = update_coldefs[k]\n                        # This is so bad that we have to update this in\n                        # duplicate...\n                        self.data._coldefs.bzeros[indx] = update_coldefs[k]\n                        # More uck...\n                        self.data._coldefs[indx]._physical_values = False\n                        self.data._coldefs[indx]._pseudo_unsigned_ints = True\n\n                # TODO: Too much of the code in this class uses header keywords\n                # in making calculations related to the data size.  This is\n                # unreliable, however, in cases when users mess with the header\n                # unintentionally--code that does this should be cleaned up.", "metadata": {"file_name": "astropy/io/fits/hdu/table.py", "File Name": "astropy/io/fits/hdu/table.py", "Classes": "FITSTableDumpDialect, _TableLikeHDU, _TableBaseHDU, TableHDU, BinTableHDU", "Functions": "_binary_table_byte_swap, format_value, update_recformats, format_value"}}, {"code": "if reverse:\n        recformat, kind, dtype = _dtype_to_recformat(format)\n        itemsize = dtype.itemsize\n\n        if kind == 'a':\n            return 'A' + str(itemsize)\n        elif NUMPY2FITS.get(recformat) == 'L':\n            # Special case for logical/boolean types--for ASCII tables we\n            # represent these as single character columns containing 'T' or 'F'\n            # (a la the storage format for Logical columns in binary tables)\n            return 'A1'\n        elif kind == 'i':\n            # Use for the width the maximum required to represent integers\n            # of that byte size plus 1 for signs, but use a minimum of the\n            # default width (to keep with existing behavior)\n            width = 1 + len(str(2 ** (itemsize * 8)))\n            width = max(width, ASCII_DEFAULT_WIDTHS['I'][0])\n            return 'I' + str(width)\n        elif kind == 'f':\n            # This is tricky, but go ahead and use D if float-64, and E\n            # if float-32 with their default widths\n            if itemsize >= 8:\n                format = 'D'\n            else:\n                format = 'E'\n            width = '.'.join(str(w) for w in ASCII_DEFAULT_WIDTHS[format])\n            return format + width\n        # TODO: There may be reasonable ways to represent other Numpy types so\n        # let's see what other possibilities there are besides just 'a', 'i',\n        # and 'f'.", "metadata": {"file_name": "astropy/io/fits/column.py", "File Name": "astropy/io/fits/column.py", "Classes": "Delayed, _BaseColumnFormat, _ColumnFormat, _AsciiColumnFormat, _FormatX, _FormatP, _FormatQ, ColumnAttribute, Column, ColDefs, _AsciiColDefs, _VLF", "Functions": "_get_index, _unwrapx, _wrapx, _makep, _parse_tformat, _parse_ascii_tformat, _parse_tdim, _scalar_to_format, _cmp_recformats, _convert_fits2record, _convert_record2fits, _dtype_to_recformat, _convert_ascii_format, convert_int"}}, {"code": "# mapping from TFORM data type to numpy data type (code)\n# L: Logical (Boolean)\n# B: Unsigned Byte\n# I: 16-bit Integer\n# J: 32-bit Integer\n# K: 64-bit Integer\n# E: Single-precision Floating Point\n# D: Double-precision Floating Point\n# C: Single-precision Complex\n# M: Double-precision Complex\n# A: Character\nFITS2NUMPY = {'L': 'i1', 'B': 'u1', 'I': 'i2', 'J': 'i4', 'K': 'i8', 'E': 'f4',\n              'D': 'f8', 'C': 'c8', 'M': 'c16', 'A': 'a'}\n\n# the inverse dictionary of the above\nNUMPY2FITS = {val: key for key, val in FITS2NUMPY.items()}\n# Normally booleans are represented as ints in Astropy, but if passed in a numpy\n# boolean array, that should be supported\nNUMPY2FITS['b1'] = 'L'\n# Add unsigned types, which will be stored as signed ints with a TZERO card.\nNUMPY2FITS['u2'] = 'I'\nNUMPY2FITS['u4'] = 'J'\nNUMPY2FITS['u8'] = 'K'\n# Add half precision floating point numbers which will be up-converted to\n# single precision.", "metadata": {"file_name": "astropy/io/fits/column.py", "File Name": "astropy/io/fits/column.py", "Classes": "Delayed, _BaseColumnFormat, _ColumnFormat, _AsciiColumnFormat, _FormatX, _FormatP, _FormatQ, ColumnAttribute, Column, ColDefs, _AsciiColDefs, _VLF", "Functions": "_get_index, _unwrapx, _wrapx, _makep, _parse_tformat, _parse_ascii_tformat, _parse_tdim, _scalar_to_format, _cmp_recformats, _convert_fits2record, _convert_record2fits, _dtype_to_recformat, _convert_ascii_format, convert_int"}}, {"code": "self.ignore_keywords = {k.upper() for k in ignore_keywords}\n        self.ignore_comments = {k.upper() for k in ignore_comments}\n\n        self.rtol = rtol\n        self.atol = atol\n\n        if tolerance is not None:  # This should be removed in the next astropy version\n            warnings.warn(\n                '\"tolerance\" was deprecated in version 2.0 and will be removed in '\n                'a future version. Use argument \"rtol\" instead.", "metadata": {"file_name": "astropy/io/fits/diff.py", "File Name": "astropy/io/fits/diff.py", "Classes": "_BaseDiff, FITSDiff, HDUDiff, HeaderDiff, ImageDataDiff, RawDataDiff, TableDataDiff", "Functions": "diff_values, report_diff_values, report_diff_keyword_attr, where_not_allclose, get_header_values_comments"}}, {"code": "if msg is None:\n                valid['disp'] = disp\n            else:\n                invalid['disp'] = (disp, msg)\n\n        # Validate the start option\n        if start is not None and start != '':\n            msg = None\n            if not isinstance(format, _AsciiColumnFormat):\n                # The 'start' option only applies to ASCII columns\n                msg = (\n                    'Column start option (TBCOLn) is not allowed for binary '\n                    'table columns (got {!r}).  The invalid keyword will be '\n                    'ignored for the purpose of formatting the data in this '\n                    'column.'.format(start))\n            else:\n                try:\n                    start = int(start)\n                except (TypeError, ValueError):\n                    pass\n\n                if not _is_int(start) or start < 1:\n                    msg = (\n                        'Column start option (TBCOLn) must be a positive integer '\n                        '(got {!r}).  The invalid value will be ignored for the '\n                        'purpose of formatting the data in this column.'.format(start))\n\n            if msg is None:\n                valid['start'] = start\n            else:\n                invalid['start'] = (start, msg)\n\n        # Process TDIMn options\n        # ASCII table columns can't have a TDIMn keyword associated with it;\n        # for now we just issue a warning and ignore it.", "metadata": {"file_name": "astropy/io/fits/column.py", "File Name": "astropy/io/fits/column.py", "Classes": "Delayed, _BaseColumnFormat, _ColumnFormat, _AsciiColumnFormat, _FormatX, _FormatP, _FormatQ, ColumnAttribute, Column, ColDefs, _AsciiColDefs, _VLF", "Functions": "_get_index, _unwrapx, _wrapx, _makep, _parse_tformat, _parse_ascii_tformat, _parse_tdim, _scalar_to_format, _cmp_recformats, _convert_fits2record, _convert_record2fits, _dtype_to_recformat, _convert_ascii_format, convert_int"}}, {"code": "Now each Column has a ._parent_fits_rec\n        # attribute which is a weakref to a FITS_rec object.  Code that\n        # previously assigned each col.array to field in a FITS_rec (as in\n        # the example a few paragraphs above) is still used, however now\n        # array.setter checks if a reference cycle will be created.  And if\n        # so, instead of saving directly to the Column's __dict__, it creates\n        # the ._prent_fits_rec weakref, and all lookups of the column's .array\n        # go through that instead.\n        #\n        # This alone does not fully solve the problem.  Because\n        # _parent_fits_rec is a weakref, if the user ever holds a reference to\n        # the Column, but deletes all references to the underlying FITS_rec,\n        # the .array attribute would suddenly start returning None instead of\n        # the array data.  This problem is resolved on FITS_rec's end.  See the\n        # note in the FITS_rec._coldefs property for the rest of the story.\n\n        # If the Columns's array is not a reference to an existing FITS_rec,\n        # then it is just stored in self.__dict__; otherwise check the\n        # _parent_fits_rec reference if it 's still available.", "metadata": {"file_name": "astropy/io/fits/column.py", "File Name": "astropy/io/fits/column.py", "Classes": "Delayed, _BaseColumnFormat, _ColumnFormat, _AsciiColumnFormat, _FormatX, _FormatP, _FormatQ, ColumnAttribute, Column, ColDefs, _AsciiColDefs, _VLF", "Functions": "_get_index, _unwrapx, _wrapx, _makep, _parse_tformat, _parse_ascii_tformat, _parse_tdim, _scalar_to_format, _cmp_recformats, _convert_fits2record, _convert_record2fits, _dtype_to_recformat, _convert_ascii_format, convert_int"}}, {"code": "But we still want to make\n        # sure the data is encodable as ASCII.  Later when we write out the\n        # array we use, in the dtype 'U' case, a different write routine\n        # that writes row by row and encodes any 'U' columns to ASCII.\n\n        # If the output_field is non-ASCII we will worry about ASCII encoding\n        # later when writing; otherwise we can do it right here\n        if input_field.dtype.kind == 'U' and output_field.dtype.kind == 'S':\n            try:\n                _ascii_encode(input_field, out=output_field)\n            except _UnicodeArrayEncodeError as exc:\n                raise ValueError(\n                    \"Could not save column '{0}': Contains characters that \"\n                    \"cannot be encoded as ASCII as required by FITS, starting \"\n                    \"at the index {1!r} of the column, and the index {2} of \"\n                    \"the string at that location.\".format(\n                        self._coldefs[col_idx].name,\n                        exc.index[0] if len(exc.index) == 1 else exc.index,\n                        exc.start))\n        else:\n            # Otherwise go ahead and do a direct copy into--if both are type\n            # 'U' we'll handle encoding later\n            input_field = input_field.flatten().view(output_field.dtype)\n            output_field.flat[:] = input_field\n\n        # Ensure that blanks at the end of each string are\n        # converted to nulls instead of spaces, see Trac #15\n        # and #111\n        _rstrip_inplace(output_field)\n\n    def _scale_back_ascii(self, col_idx, input_field, output_field):\n        \"\"\"\n        Convert internal array values back to ASCII table representation.\n\n        The ``input_field`` is the internal representation of the values, and\n        the ``output_field`` is the character array representing the ASCII\n        output that will be written.\n        \"\"\"", "metadata": {"file_name": "astropy/io/fits/fitsrec.py", "File Name": "astropy/io/fits/fitsrec.py", "Classes": "FITS_record, FITS_rec, _UnicodeArrayEncodeError", "Functions": "_get_recarray_field, _ascii_encode, _has_unicode_fields"}}, {"code": "recformat = cls._convert_format(format, guess_format)\n        except VerifyError:\n            # For whatever reason our guess was wrong (for example if we got\n            # just 'F' that's not a valid binary format, but it an ASCII format\n            # code albeit with the width/precision omitted\n            guess_format = (_AsciiColumnFormat\n                            if guess_format is _ColumnFormat\n                            else _ColumnFormat)\n            # If this fails too we're out of options--it is truly an invalid\n            # format, or at least not supported\n            format, recformat = cls._convert_format(format, guess_format)\n\n        return format, recformat\n\n    def _convert_to_valid_data_type(self, array):\n        # Convert the format to a type we understand\n        if isinstance(array, Delayed):\n            return array\n        elif array is None:\n            return array\n        else:\n            format = self.format\n            dims = self._dims\n\n            if dims:\n                shape = dims[:-1] if 'A' in format else dims\n                shape = (len(array),) + shape\n                array = array.reshape(shape)\n\n            if 'P' in format or 'Q' in format:\n                return array\n            elif 'A' in format:\n                if array.dtype.char in 'SU':\n                    if dims:\n                        # The 'last' dimension (first in the order given\n                        # in the TDIMn keyword itself) is the number of\n                        # characters in each string\n                        fsize = dims[-1]\n                    else:\n                        fsize = np.dtype(format.recformat).itemsize\n                    return chararray.array(array, itemsize=fsize, copy=False)\n                else:\n                    return _convert_array(array, np.dtype(format.", "metadata": {"file_name": "astropy/io/fits/column.py", "File Name": "astropy/io/fits/column.py", "Classes": "Delayed, _BaseColumnFormat, _ColumnFormat, _AsciiColumnFormat, _FormatX, _FormatP, _FormatQ, ColumnAttribute, Column, ColDefs, _AsciiColDefs, _VLF", "Functions": "_get_index, _unwrapx, _wrapx, _makep, _parse_tformat, _parse_ascii_tformat, _parse_tdim, _scalar_to_format, _cmp_recformats, _convert_fits2record, _convert_record2fits, _dtype_to_recformat, _convert_ascii_format, convert_int"}}, {"code": "\\n{}\\n'\n                    'There may be extra bytes after the last HDU or the '\n                    'file is corrupted.'.format(\n                        len(self), indent(str(exc))), VerifyWarning)\n                del exc\n                self._read_all = True\n                return False\n        finally:\n            compressed.COMPRESSION_ENABLED = saved_compression_enabled\n            self._in_read_next_hdu = False\n\n        return True\n\n    def _verify(self, option='warn'):\n        errs = _ErrList([], unit='HDU')\n\n        # the first (0th) element must be a primary HDU\n        if len(self) > 0 and (not isinstance(self[0], PrimaryHDU)) and \\\n                             (not isinstance(self[0], _NonstandardHDU)):\n            err_text = \"HDUList's 0th element is not a primary HDU.\"\n            fix_text = 'Fixed by inserting one as 0th HDU.'\n\n            def fix(self=self):\n                self.insert(0, PrimaryHDU())\n\n            err = self.run_option(option, err_text=err_text,\n                                  fix_text=fix_text, fix=fix)\n            errs.append(err)\n\n        if len(self) > 1 and ('EXTEND' not in self[0].header or\n                              self[0].header['EXTEND'] is not True):\n            err_text = ('Primary HDU does not contain an EXTEND keyword '\n                        'equal to T even though there are extension HDUs.')\n            fix_text = 'Fixed by inserting or updating the EXTEND keyword.'\n\n            def fix(header=self[0].header):\n                naxis = header['NAXIS']\n                if naxis == 0:\n                    after = 'NAXIS'\n                else:\n                    after = 'NAXIS' + str(naxis)\n                header.set('EXTEND', value=True, after=after)\n\n            errs.append(self.run_option(option, err_text=err_text,\n                                        fix_text=fix_text, fix=fix))\n\n        # each element calls their own verify\n        for idx, hdu in enumerate(self):\n            if idx > 0 and (not isinstance(hdu, ExtensionHDU)):\n                err_text = (\"HDUList's element {} is not an \"\n                            \"extension HDU.", "metadata": {"file_name": "astropy/io/fits/hdu/hdulist.py", "File Name": "astropy/io/fits/hdu/hdulist.py", "Classes": "HDUList", "Functions": "fitsopen, get_first_ext, fix, fix"}}, {"code": "else:\n                width = 0 if width is None else width\n                precision = 1 if precision is None else precision\n    else:\n        format = format.upper()\n        width = match.group('width')\n        if width is None:\n            if strict:\n                raise VerifyError('Format {!r} is not unambiguously an ASCII '\n                                  'table format.')\n            else:\n                # Just use a default width of 0 if unspecified\n                width = 0\n        precision = 0\n\n    def convert_int(val):\n        msg = ('Format {!r} is not valid--field width and decimal precision '\n               'must be integers.')\n        try:\n            val = int(val)\n        except (ValueError, TypeError):\n            raise VerifyError(msg.format(tform))\n\n        return val\n\n    if width and precision:\n        # This should only be the case for floating-point formats\n        width, precision = convert_int(width), convert_int(precision)\n    elif width:\n        # Just for integer/string formats; ignore precision\n        width = convert_int(width)\n    else:\n        # For any format, if width was unspecified use the set defaults\n        width, precision = ASCII_DEFAULT_WIDTHS[format]\n\n    if width <= 0:\n        raise VerifyError(\"Format {!r} not valid--field width must be a \"\n                          \"positive integeter.\".format(tform))\n\n    if precision >= width:\n        raise VerifyError(\"Format {!r} not valid--the number of decimal digits \"\n                          \"must be less than the format's total \"\n                          \"width {}.\".format(tform, width))\n\n    return format, width, precision", "metadata": {"file_name": "astropy/io/fits/column.py", "File Name": "astropy/io/fits/column.py", "Classes": "Delayed, _BaseColumnFormat, _ColumnFormat, _AsciiColumnFormat, _FormatX, _FormatP, _FormatQ, ColumnAttribute, Column, ColDefs, _AsciiColDefs, _VLF", "Functions": "_get_index, _unwrapx, _wrapx, _makep, _parse_tformat, _parse_ascii_tformat, _parse_tdim, _scalar_to_format, _cmp_recformats, _convert_fits2record, _convert_record2fits, _dtype_to_recformat, _convert_ascii_format, convert_int"}}, {"code": "itemsize != 0:\n            raise ValueError('size {} not a multiple of {}'.format(size, dtype))\n\n        if isinstance(shape, int):\n            shape = (shape,)\n\n        if not (size or shape):\n            warnings.warn('No size or shape given to readarray(); assuming a '\n                          'shape of (1,)', AstropyUserWarning)\n            shape = (1,)\n\n        if size and not shape:\n            shape = (size // dtype.itemsize,)\n\n        if size and shape:\n            actualsize = np.prod(shape) * dtype.itemsize\n\n            if actualsize > size:\n                raise ValueError('size {} is too few bytes for a {} array of '\n                                 '{}'.format(size, shape, dtype))\n            elif actualsize < size:\n                raise ValueError('size {} is too many bytes for a {} array of '\n                                 '{}'.format(size, shape, dtype))\n\n        filepos = self._file.tell()\n\n        try:\n            if self.memmap:\n                if self._mmap is None:\n                    # Instantiate Memmap array of the file offset at 0 (so we\n                    # can return slices of it to offset anywhere else into the\n                    # file)\n                    memmap = Memmap(self._file, mode=MEMMAP_MODES[self.mode],\n                                    dtype=np.uint8)\n\n                    # Now we immediately discard the memmap array; we are\n                    # really just using it as a factory function to instantiate\n                    # the mmap object in a convenient way (may later do away\n                    # with this usage)\n                    self._mmap = memmap.base\n\n                    # Prevent dorking with self._memmap._mmap by memmap.__del__\n                    # in Numpy 1.6 (see\n                    # https://github.com/numpy/numpy/commit/dcc355a0b179387eeba10c95baf2e1eb21d417c7)\n                    memmap._mmap = None\n                    del memmap\n\n                return np.ndarray(shape=shape, dtype=dtype, offset=offset,\n                                  buffer=self._mmap)\n            else:\n                count = reduce(operator.mul, shape)\n                self._file.seek(offset)\n                data = _array_from_file(self._file, dtype,", "metadata": {"file_name": "astropy/io/fits/file.py", "File Name": "astropy/io/fits/file.py", "Classes": "_File", "Functions": "_normalize_fits_mode"}}, {"code": "i = field.dtype.str.index(field.dtype.kind)\n                    field_width = int(field.dtype.str[i+1:])\n                    item = np.char.encode(item, 'ascii')\n\n                fileobj.writearray(item)\n                if field_width is not None:\n                    j = item.dtype.str.index(item.dtype.kind)\n                    item_length = int(item.dtype.str[j+1:])\n                    # Fix padding problem (see #5296).\n                    padding = '\\x00'*(field_width - item_length)\n                    fileobj.write(padding.encode('ascii'))\n\n    _tdump_file_format = textwrap.dedent(\"\"\"\n\n        - **datafile:** Each line of the data file represents one row of table\n          data.  The data is output one column at a time in column order.  If\n          a column contains an array, each element of the column array in the\n          current row is output before moving on to the next column.  Each row\n          ends with a new line.\n\n          Integer data is output right-justified in a 21-character field\n          followed by a blank.  Floating point data is output right justified\n          using 'g' format in a 21-character field with 15 digits of\n          precision, followed by a blank.  String data that does not contain\n          whitespace is output left-justified in a field whose width matches\n          the width specified in the ``TFORM`` header parameter for the\n          column, followed by a blank.  When the string data contains\n          whitespace characters, the string is enclosed in quotation marks\n          (``\"\"``).  For the last data element in a row, the trailing blank in\n          the field is replaced by a new line character.\n\n          For column data containing variable length arrays ('P' format), the\n          array data is preceded by the string ``'VLA_Length= '`` and the\n          integer length of the array for that row, left-justified in a\n          21-character field, followed by a blank.\n\n          .. note::\n\n              This format does *not* support variable length arrays using the\n              ('Q' format) due to difficult to overcome ambiguities.", "metadata": {"file_name": "astropy/io/fits/hdu/table.py", "File Name": "astropy/io/fits/hdu/table.py", "Classes": "FITSTableDumpDialect, _TableLikeHDU, _TableBaseHDU, TableHDU, BinTableHDU", "Functions": "_binary_table_byte_swap, format_value, update_recformats, format_value"}}, {"code": "This results in a reference cycle that cannot be broken since\n        ndarrays do not participate in cyclic garbage collection.\n        \"\"\"\n\n        base = field\n        while True:\n            self_base = self\n            while True:\n                if self_base is base:\n                    return\n\n                if getattr(self_base, 'base', None) is not None:\n                    self_base = self_base.base\n                else:\n                    break\n\n            if getattr(base, 'base', None) is not None:\n                base = base.base\n            else:\n                break\n\n        self._converted[name] = field\n\n    def _update_column_attribute_changed(self, column, idx, attr, old_value,\n                                         new_value):\n        \"\"\"\n        Update how the data is formatted depending on changes to column\n        attributes initiated by the user through the `Column` interface.\n\n        Dispatches column attribute change notifications to individual methods\n        for each attribute ``_update_column_<attr>``\n        \"\"\"\n\n        method_name = '_update_column_{0}'.format(attr)\n        if hasattr(self, method_name):\n            # Right now this is so we can be lazy and not implement updaters\n            # for every attribute yet--some we may not need at all, TBD\n            getattr(self, method_name)(column, idx, old_value, new_value)\n\n    def _update_column_name(self, column, idx, old_name, name):\n        \"\"\"Update the dtype field names when a column name is changed.\"\"\"\n\n        dtype = self.dtype\n        # Updating the names on the dtype should suffice\n        dtype.names = dtype.names[:idx] + (name,) + dtype.names[idx + 1:]\n\n    def _convert_x(self, field, recformat):\n        \"\"\"Convert a raw table column to a bit array as specified by the\n        FITS X format.\n        \"\"\"\n\n        dummy = np.zeros(self.shape + (recformat.repeat,), dtype=np.bool_)\n        _unwrapx(field, dummy, recformat.repeat)\n        return dummy\n\n    def _convert_p(self, column, field, recformat):\n        \"\"\"Convert a raw table column of FITS P or Q format descriptors\n        to a VLA column with the array data returned from the heap.\n        \"\"\"", "metadata": {"file_name": "astropy/io/fits/fitsrec.py", "File Name": "astropy/io/fits/fitsrec.py", "Classes": "FITS_record, FITS_rec, _UnicodeArrayEncodeError", "Functions": "_get_recarray_field, _ascii_encode, _has_unicode_fields"}}, {"code": "def _format_float(value):\n    \"\"\"Format a floating number to make sure it gets the decimal point.\"\"\"\n\n    value_str = '{:.16G}'.format(value)\n    if '.' not in value_str and 'E' not in value_str:\n        value_str += '.0'\n    elif 'E' in value_str:\n        # On some Windows builds of Python (and possibly other platforms?) the\n        # exponent is zero-padded out to, it seems, three digits.  Normalize\n        # the format to pad only to two digits.\n        significand, exponent = value_str.split('E')\n        if exponent[0] in ('+', '-'):\n            sign = exponent[0]\n            exponent = exponent[1:]\n        else:\n            sign = ''\n        value_str = '{}E{}{:02d}'.format(significand, sign, int(exponent))\n\n    # Limit the value string to at most 20 characters.\n    str_len = len(value_str)\n\n    if str_len > 20:\n        idx = value_str.find('E')\n\n        if idx < 0:\n            value_str = value_str[:20]\n        else:\n            value_str = value_str[:20 - (str_len - idx)] + value_str[idx:]\n\n    return value_str\n\n\ndef _pad(input):\n    \"\"\"Pad blank space to the input string to be multiple of 80.\"\"\"\n\n    _len = len(input)\n    if _len == Card.length:\n        return input\n    elif _len > Card.length:\n        strlen = _len % Card.length\n        if strlen == 0:\n            return input\n        else:\n            return input + ' ' * (Card.length - strlen)\n\n    # minimum length is 80\n    else:\n        strlen = _len % Card.length\n        return input + ' ' * (Card.length - strlen)", "metadata": {"file_name": "astropy/io/fits/card.py", "File Name": "astropy/io/fits/card.py", "Classes": "Undefined, Card", "Functions": "_int_or_float, _format_float, _pad"}}, {"code": "orig_dtype = data.dtype\n\n    names = []\n    formats = []\n    offsets = []\n\n    to_swap = []\n\n    if sys.byteorder == 'little':\n        swap_types = ('<', '=')\n    else:\n        swap_types = ('<',)\n\n    for idx, name in enumerate(orig_dtype.names):\n        field = _get_recarray_field(data, idx)\n\n        field_dtype, field_offset = orig_dtype.fields[name]\n        names.append(name)\n        formats.append(field_dtype)\n        offsets.append(field_offset)\n\n        if isinstance(field, chararray.chararray):\n            continue\n\n        # only swap unswapped\n        # must use field_dtype.base here since for multi-element dtypes,\n        # the .str with be '|V<N>' where <N> is the total bytes per element\n        if field.itemsize > 1 and field_dtype.base.str[0] in swap_types:\n            to_swap.append(field)\n            # Override the dtype for this field in the new record dtype with\n            # the byteswapped version\n            formats[-1] = field_dtype.newbyteorder()\n\n        # deal with var length table\n        recformat = data.columns._recformats[idx]\n        if isinstance(recformat, _FormatP):\n            coldata = data.field(idx)\n            for c in coldata:\n                if (not isinstance(c, chararray.chararray) and\n                        c.itemsize > 1 and c.dtype.str[0] in swap_types):\n                    to_swap.append(c)\n\n    for arr in reversed(to_swap):\n        arr.byteswap(True)\n\n    new_dtype = nh.realign_dtype(np.dtype(list(zip(names, formats))),\n                                 offsets)\n\n    data.dtype = new_dtype\n\n    yield data\n\n    for arr in to_swap:\n        arr.byteswap(True)\n\n    data.dtype = orig_dtype", "metadata": {"file_name": "astropy/io/fits/hdu/table.py", "File Name": "astropy/io/fits/hdu/table.py", "Classes": "FITSTableDumpDialect, _TableLikeHDU, _TableBaseHDU, TableHDU, BinTableHDU", "Functions": "_binary_table_byte_swap, format_value, update_recformats, format_value"}}, {"code": "if self.field_specifier:\n            return float(self._value)\n\n        if self._value is not None:\n            value = self._value\n        elif self._valuestring is not None or self._image:\n            self._value = self._parse_value()\n            value = self._value\n        else:\n            self._value = value = ''\n\n        if conf.strip_header_whitespace and isinstance(value, str):\n            value = value.rstrip()\n\n        return value\n\n    @value.setter\n    def value(self, value):\n        if self._invalid:\n            raise ValueError(\n                'The value of invalid/unparseable cards cannot set.  Either '\n                'delete this card from the header or replace it.')\n\n        if value is None:\n            value = ''\n        oldvalue = self._value\n        if oldvalue is None:\n            oldvalue = ''\n\n        if not isinstance(value,\n                          (str, int, float, complex, bool, Undefined,\n                           np.floating, np.integer, np.complexfloating,\n                           np.bool_)):\n            raise ValueError('Illegal value: {!r}.'.format(value))\n\n        if isinstance(value, float) and (np.isnan(value) or np.isinf(value)):\n            raise ValueError(\"Floating point {!r} values are not allowed \"\n                             \"in FITS headers.\".format(value))\n\n        elif isinstance(value, str):\n            m = self._ascii_text_re.match(value)\n            if not m:\n                raise ValueError(\n                    'FITS header values must contain standard printable ASCII '\n                    'characters; {!r} contains characters not representable in '\n                    'ASCII or non-printable characters.'.format(value))\n        elif isinstance(value, bytes):\n            # Allow str, but only if they can be decoded to ASCII text; note\n            # this is not even allowed on Python 3 since the `bytes` type is\n            # not included in `str`.", "metadata": {"file_name": "astropy/io/fits/card.py", "File Name": "astropy/io/fits/card.py", "Classes": "Undefined, Card", "Functions": "_int_or_float, _format_float, _pad"}}, {"code": "if self._image is not None:\n            # If we already have a card image, don't try to rebuild a new card\n            # image, which self.image would do\n            image = self._image\n        else:\n            image = self.image\n\n        if self.keyword in self._commentary_keywords.union(['CONTINUE']):\n            keyword, valuecomment = image.split(' ', 1)\n        else:\n            try:\n                delim_index = image.index(self._value_indicator)\n            except ValueError:\n                delim_index = None\n\n            # The equal sign may not be any higher than column 10; anything\n            # past that must be considered part of the card value\n            if delim_index is None:\n                keyword = image[:KEYWORD_LENGTH]\n                valuecomment = image[KEYWORD_LENGTH:]\n            elif delim_index > 10 and image[:9] != 'HIERARCH ':\n                keyword = image[:8]\n                valuecomment = image[8:]\n            else:\n                keyword, valuecomment = image.split(self._value_indicator, 1)\n        return keyword.strip(), valuecomment.strip()\n\n    def _fix_keyword(self):\n        if self.field_specifier:\n            keyword, field_specifier = self._keyword.split('.', 1)\n            self._keyword = '.'.join([keyword.upper(), field_specifier])\n        else:\n            self._keyword = self._keyword.upper()\n        self._modified = True\n\n    def _fix_value(self):\n        \"\"\"Fix the card image for fixable non-standard compliance.\"\"\"", "metadata": {"file_name": "astropy/io/fits/card.py", "File Name": "astropy/io/fits/card.py", "Classes": "Undefined, Card", "Functions": "_int_or_float, _format_float, _pad"}}, {"code": "def handle_options(argv=None):\n    # This is a callback--less trouble than actually adding a new action type\n    def store_list(option, opt, value, parser):\n        setattr(parser.values, option.dest, [])\n        # Accept either a comma-separated list or a filename (starting with @)\n        # containing a value on each line\n        if value and value[0] == '@':\n            value = value[1:]\n            if not os.path.exists(value):\n                log.warning('{} argument {} does not exist'.format(opt, value))\n                return\n            try:\n                values = [v.strip() for v in open(value, 'r').readlines()]\n                setattr(parser.values, option.dest, values)\n            except OSError as exc:\n                log.warning('reading {} for {} failed: {}; ignoring this '\n                            'argument'.format(value, opt, exc))\n                del exc\n        else:\n            setattr(parser.values, option.dest,\n                    [v.strip() for v in value.split(',')])\n\n    parser = optparse.OptionParser(usage=USAGE, epilog=EPILOG,\n                                   formatter=HelpFormatter())\n\n    parser.add_option(\n        '-q', '--quiet', action='store_true',\n        help='Produce no output and just return a status code.')\n\n    parser.add_option(\n        '-n', '--num-diffs', type='int', default=10, dest='numdiffs',\n        metavar='INTEGER',\n        help='Max number of data differences (image pixel or table element) '\n             'to report per extension (default %default).')\n\n    parser.add_option(\n        '-d', '--difference-tolerance', type='float', default=None,\n        dest='tolerance', metavar='NUMBER',\n        help='DEPRECATED. Alias for \"--relative-tolerance\". '\n             'Deprecated, provided for backward compatibility (default %default).')\n\n    parser.add_option(\n        '-r', '--rtol', '--relative-tolerance', type='float', default=None,\n        dest='rtol', metavar='NUMBER',\n        help='The relative tolerance for comparison of two numbers, '\n             'specifically two floating point numbers.  This applies to data '\n             'in both images and tables, and to floating point keyword values '\n             'in headers (default %default).')", "metadata": {"file_name": "astropy/io/fits/scripts/fitsdiff.py", "File Name": "astropy/io/fits/scripts/fitsdiff.py", "Classes": "HelpFormatter, LevelFilter", "Functions": "handle_options, setup_logging, match_files, main, store_list"}}, {"code": "'.format(option))\n\n        if opt == 'ignore':\n            return\n\n        errs = self._verify(opt)\n\n        # Break the verify option into separate options related to reporting of\n        # errors, and fixing of fixable errors\n        if '+' in opt:\n            fix_opt, report_opt = opt.split('+')\n        elif opt in ['fix', 'silentfix']:\n            # The original default behavior for 'fix' and 'silentfix' was to\n            # raise an exception for unfixable errors\n            fix_opt, report_opt = opt, 'exception'\n        else:\n            fix_opt, report_opt = None, opt\n\n        if fix_opt == 'silentfix' and report_opt == 'ignore':\n            # Fixable errors were fixed, but don't report anything\n            return\n\n        if fix_opt == 'silentfix':\n            # Don't print out fixable issues; the first element of each verify\n            # item is a boolean indicating whether or not the issue was fixable\n            line_filter = lambda x: not x[0]\n        elif fix_opt == 'fix' and report_opt == 'ignore':\n            # Don't print *unfixable* issues, but do print fixed issues; this\n            # is probably not very useful but the option exists for\n            # completeness\n            line_filter = operator.itemgetter(0)\n        else:\n            line_filter = None\n\n        unfixable = False\n        messages = []\n        for fixable, message in errs.iter_lines(filter=line_filter):\n            if fixable is not None:\n                unfixable = not fixable\n            messages.append(message)\n\n        if messages:\n            messages.insert(0, 'Verification reported errors:')\n            messages.append('Note: astropy.io.fits uses zero-based indexing.\\n')\n\n            if fix_opt == 'silentfix' and not unfixable:\n                return\n            elif report_opt == 'warn' or (fix_opt == 'fix' and not unfixable):\n                for line in messages:\n                    warnings.warn(line, VerifyWarning)\n            else:\n                raise VerifyError('\\n' + '\\n'.join(messages))", "metadata": {"file_name": "astropy/io/fits/verify.py", "File Name": "astropy/io/fits/verify.py", "Classes": "VerifyError, VerifyWarning, _Verify, _ErrList"}}, {"code": "EPILOG = \"\"\"\nIf the two files are identical within the specified conditions, it will report\n\"No difference is found.\" If the value(s) of -c and -k takes the form\n'@filename', list is in the text file 'filename', and each line in that text\nfile contains one keyword.\n\nExample\n-------\n\n    fitsdiff -k filename,filtnam1 -n 5 -r 1.e-6 test1.fits test2\n\nThis command will compare files test1.fits and test2.fits, report maximum of 5\ndifferent pixels values per extension, only report data values larger than\n1.e-6 relative to each other, and will neglect the different values of keywords\nFILENAME and FILTNAM1 (or their very existence).\n\nfitsdiff command-line arguments can also be set using the environment variable\nFITSDIFF_SETTINGS.  If the FITSDIFF_SETTINGS environment variable is present,\neach argument present will override the corresponding argument on the\ncommand-line unless the --exact option is specified.  The FITSDIFF_SETTINGS\nenvironment variable exists to make it easier to change the\nbehavior of fitsdiff on a global level, such as in a set of regression tests.\n\"\"\".strip()\n\n\nclass HelpFormatter(optparse.TitledHelpFormatter):\n    def format_epilog(self, epilog):\n        return '\\n{}\\n'.format(fill(epilog, self.width))", "metadata": {"file_name": "astropy/io/fits/scripts/fitsdiff.py", "File Name": "astropy/io/fits/scripts/fitsdiff.py", "Classes": "HelpFormatter, LevelFilter", "Functions": "handle_options, setup_logging, match_files, main, store_list"}}, {"code": "mask = np.ones(c.shape[:-1], dtype=bool)\n        # Loop over the characters in the strings, in reverse order. We process\n        # the i-th character of all strings in the chunk at the same time. If\n        # the character is 32, this corresponds to a space, and we then change\n        # this to 0. We then construct a new mask to find rows where the\n        # i-th character is 0 (null) and the i-1-th is 32 (space) and repeat.\n        for i in range(-1, -c.shape[-1], -1):\n            mask &= c[..., i] == 32\n            c[..., i][mask] = 0\n            mask = c[..., i] == 0\n\n    return array", "metadata": {"file_name": "astropy/io/fits/util.py", "File Name": "astropy/io/fits/util.py", "Classes": "NotifierMixin, SigintHandler", "Functions": "first, itersubclasses, ignore_sigint, pairwise, encode_ascii, decode_ascii, isreadable, iswritable, isfile, fileobj_open, fileobj_name, fileobj_closed, fileobj_mode, _fileobj_normalize_mode, fileobj_is_binary, translate, fill, _array_from_file, _array_to_file, _array_to_file_like, _write_string, _convert_array, _unsigned_zero, _is_pseudo_unsigned, _is_int, _str_to_num, _words_group, _tmp_name, _get_array_mmap, _free_space_check, _extract_number, get_testdata_filepath, _rstrip_inplace, wrapped, maybe_fill"}}, {"code": "value = None\n        keyword, valuecomment = self._split()\n        m = self._value_NFSC_RE.match(valuecomment)\n\n        # for the unparsable case\n        if m is None:\n            try:\n                value, comment = valuecomment.split('/', 1)\n                self.value = value.strip()\n                self.comment = comment.strip()\n            except (ValueError, IndexError):\n                self.value = valuecomment\n            self._valuestring = self._value\n            return\n        elif m.group('numr') is not None:\n            numr = self._number_NFSC_RE.match(m.group('numr'))\n            value = translate(numr.group('digt'), FIX_FP_TABLE, ' ')\n            if numr.group('sign') is not None:\n                value = numr.group('sign') + value\n\n        elif m.group('cplx') is not None:\n            real = self._number_NFSC_RE.match(m.group('real'))\n            rdigt = translate(real.group('digt'), FIX_FP_TABLE, ' ')\n            if real.group('sign') is not None:\n                rdigt = real.group('sign') + rdigt\n\n            imag = self._number_NFSC_RE.match(m.group('imag'))\n            idigt = translate(imag.group('digt'), FIX_FP_TABLE, ' ')\n            if imag.group('sign') is not None:\n                idigt = imag.group('sign') + idigt\n            value = '({}, {})'.format(rdigt, idigt)\n        self._valuestring = value\n        # The value itself has not been modified, but its serialized\n        # representation (as stored in self._valuestring) has been changed, so\n        # still set this card as having been modified (see ticket #137)\n        self._modified = True\n\n    def _format_keyword(self):\n        if self.keyword:\n            if self.field_specifier:\n                return '{:{len}}'.format(self.keyword.split('.', 1)[0],\n                                         len=KEYWORD_LENGTH)\n            elif self._hierarch:\n                return 'HIERARCH {} '.format(self.keyword)\n            else:\n                return '{:{len}}'.format(self.keyword,", "metadata": {"file_name": "astropy/io/fits/card.py", "File Name": "astropy/io/fits/card.py", "Classes": "Undefined, Card", "Functions": "_int_or_float, _format_float, _pad"}}, {"code": "dummy = _VLF([None] * len(self), dtype=recformat.dtype)\n        raw_data = self._get_raw_data()\n\n        if raw_data is None:\n            raise OSError(\n                \"Could not find heap data for the {!r} variable-length \"\n                \"array column.\".format(column.name))\n\n        for idx in range(len(self)):\n            offset = field[idx, 1] + self._heapoffset\n            count = field[idx, 0]\n\n            if recformat.dtype == 'a':\n                dt = np.dtype(recformat.dtype + str(1))\n                arr_len = count * dt.itemsize\n                da = raw_data[offset:offset + arr_len].view(dt)\n                da = np.char.array(da.view(dtype=dt), itemsize=count)\n                dummy[idx] = decode_ascii(da)\n            else:\n                dt = np.dtype(recformat.dtype)\n                arr_len = count * dt.itemsize\n                dummy[idx] = raw_data[offset:offset + arr_len].view(dt)\n                dummy[idx].dtype = dummy[idx].dtype.newbyteorder('>')\n                # Each array in the field may now require additional\n                # scaling depending on the other scaling parameters\n                # TODO: The same scaling parameters apply to every\n                # array in the column so this is currently very slow; we\n                # really only need to check once whether any scaling will\n                # be necessary and skip this step if not\n                # TODO: Test that this works for X format; I don't think\n                # that it does--the recformat variable only applies to the P\n                # format not the X format\n                dummy[idx] = self._convert_other(column, dummy[idx],\n                                                 recformat)\n\n        return dummy\n\n    def _convert_ascii(self, column, field):\n        \"\"\"\n        Special handling for ASCII table columns to convert columns containing\n        numeric types to actual numeric arrays from the string representation.\n        \"\"\"", "metadata": {"file_name": "astropy/io/fits/fitsrec.py", "File Name": "astropy/io/fits/fitsrec.py", "Classes": "FITS_record, FITS_rec, _UnicodeArrayEncodeError", "Functions": "_get_recarray_field, _ascii_encode, _has_unicode_fields"}}, {"code": "if 'array' in self.__dict__:\n            return self.__dict__['array']\n        elif self._parent_fits_rec is not None:\n            parent = self._parent_fits_rec()\n            if parent is not None:\n                return parent[self.name]\n        else:\n            return None\n\n    @array.setter\n    def array(self, array):\n        # The following looks over the bases of the given array to check if it\n        # has a ._coldefs attribute (i.e. is a FITS_rec) and that that _coldefs\n        # contains this Column itself, and would create a reference cycle if we\n        # stored the array directly in self.__dict__.\n        # In this case it instead sets up the _parent_fits_rec weakref to the\n        # underlying FITS_rec, so that array.getter can return arrays through\n        # self._parent_fits_rec().field(self.name), rather than storing a\n        # hard reference to the field like it used to.\n        base = array\n        while True:\n            if (hasattr(base, '_coldefs') and\n                    isinstance(base._coldefs, ColDefs)):\n                for col in base._coldefs:\n                    if col is self and self._parent_fits_rec is None:\n                        self._parent_fits_rec = weakref.ref(base)\n\n                        # Just in case the user already set .array to their own\n                        # array.", "metadata": {"file_name": "astropy/io/fits/column.py", "File Name": "astropy/io/fits/column.py", "Classes": "Delayed, _BaseColumnFormat, _ColumnFormat, _AsciiColumnFormat, _FormatX, _FormatP, _FormatQ, ColumnAttribute, Column, ColDefs, _AsciiColDefs, _VLF", "Functions": "_get_index, _unwrapx, _wrapx, _makep, _parse_tformat, _parse_ascii_tformat, _parse_tdim, _scalar_to_format, _cmp_recformats, _convert_fits2record, _convert_record2fits, _dtype_to_recformat, _convert_ascii_format, convert_int"}}, {"code": "',\n                AstropyDeprecationWarning)\n            self.rtol = tolerance  # when tolerance is provided *always* ignore `rtol`\n                                   # during the transition/deprecation period\n\n        self.ignore_blanks = ignore_blanks\n        self.ignore_blank_cards = ignore_blank_cards\n\n        self.ignore_keyword_patterns = set()\n        self.ignore_comment_patterns = set()\n        for keyword in list(self.ignore_keywords):\n            keyword = keyword.upper()\n            if keyword != '*' and glob.has_magic(keyword):\n                self.ignore_keywords.remove(keyword)\n                self.ignore_keyword_patterns.add(keyword)\n        for keyword in list(self.ignore_comments):\n            keyword = keyword.upper()\n            if keyword != '*' and glob.has_magic(keyword):\n                self.ignore_comments.remove(keyword)\n                self.ignore_comment_patterns.add(keyword)\n\n        # Keywords appearing in each header\n        self.common_keywords = []\n\n        # Set to the number of keywords in each header if the counts differ\n        self.diff_keyword_count = ()\n\n        # Set if the keywords common to each header (excluding ignore_keywords)\n        # appear in different positions within the header\n        # TODO: Implement this\n        self.diff_keyword_positions = ()\n\n        # Keywords unique to each header (excluding keywords in\n        # ignore_keywords)\n        self.diff_keywords = ()\n\n        # Keywords that have different numbers of duplicates in each header\n        # (excluding keywords in ignore_keywords)\n        self.diff_duplicate_keywords = {}\n\n        # Keywords common to each header but having different values (excluding\n        # keywords in ignore_keywords)\n        self.diff_keyword_values = defaultdict(list)\n\n        # Keywords common to each header but having different comments\n        # (excluding keywords in ignore_keywords or in ignore_comments)\n        self.diff_keyword_comments = defaultdict(list)\n\n        if isinstance(a, str):\n            a = Header.fromstring(a)\n        if isinstance(b, str):\n            b = Header.fromstring(b)\n\n        if not (isinstance(a, Header) and isinstance(b, Header)):\n            raise TypeError('HeaderDiff can only diff astropy.io.fits.Header '\n                            'objects or strings containing FITS headers.')", "metadata": {"file_name": "astropy/io/fits/diff.py", "File Name": "astropy/io/fits/diff.py", "Classes": "_BaseDiff, FITSDiff, HDUDiff, HeaderDiff, ImageDataDiff, RawDataDiff, TableDataDiff", "Functions": "diff_values, report_diff_values, report_diff_keyword_attr, where_not_allclose, get_header_values_comments"}}, {"code": "err_msg = ('Redundant/conflicting extension arguments(s): {}'.format(\n            {'args': args, 'ext': ext, 'extname': extname,\n             'extver': extver}))\n\n    # This code would be much simpler if just one way of specifying an\n    # extension were picked.  But now we need to support all possible ways for\n    # the time being.\n    if len(args) == 1:\n        # Must be either an extension number, an extension name, or an\n        # (extname, extver) tuple\n        if _is_int(args[0]) or (isinstance(ext, tuple) and len(ext) == 2):\n            if ext is not None or extname is not None or extver is not None:\n                raise TypeError(err_msg)\n            ext = args[0]\n        elif isinstance(args[0], str):\n            # The first arg is an extension name; it could still be valid\n            # to provide an extver kwarg\n            if ext is not None or extname is not None:\n                raise TypeError(err_msg)\n            extname = args[0]\n        else:\n            # Take whatever we have as the ext argument; we'll validate it\n            # below\n            ext = args[0]\n    elif len(args) == 2:\n        # Must be an extname and extver\n        if ext is not None or extname is not None or extver is not None:\n            raise TypeError(err_msg)\n        extname = args[0]\n        extver = args[1]\n    elif len(args) > 2:\n        raise TypeError('Too many positional arguments.')\n\n    if (ext is not None and\n            not (_is_int(ext) or\n                 (isinstance(ext, tuple) and len(ext) == 2 and\n                  isinstance(ext[0], str) and _is_int(ext[1])))):\n        raise ValueError(\n            'The ext keyword must be either an extension number '\n            '(zero-indexed) or a (extname, extver) tuple.')", "metadata": {"file_name": "astropy/io/fits/convenience.py", "File Name": "astropy/io/fits/convenience.py", "Functions": "getheader, getdata, getval, setval, delval, writeto, table_to_hdu, append, update, info, printdiff, tabledump, tableload, _getext, _makehdu, _stat_filename_or_fileobj, _get_file_mode"}}, {"code": "\".format(\n                            name, format, dim))\n\n            if msg is None:\n                valid['dim'] = dims_tuple\n            else:\n                invalid['dim'] = (dim, msg)\n\n        if coord_type is not None and coord_type != '':\n            msg = None\n            if not isinstance(coord_type, str):\n                msg = (\n                    \"Coordinate/axis type option (TCTYPn) must be a string \"\n                    \"(got {!r}). The invalid keyword will be ignored for the \"\n                    \"purpose of formatting this column.\".format(coord_type))\n            elif len(coord_type) > 8:\n                msg = (\n                    \"Coordinate/axis type option (TCTYPn) must be a string \"\n                    \"of atmost 8 characters (got {!r}). The invalid keyword \"\n                    \"will be ignored for the purpose of formatting this \"\n                    \"column.\".format(coord_type))\n\n            if msg is None:\n                valid['coord_type'] = coord_type\n            else:\n                invalid['coord_type'] = (coord_type, msg)\n\n        if coord_unit is not None and coord_unit != '':\n            msg = None\n            if not isinstance(coord_unit, str):\n                msg = (\n                    \"Coordinate/axis unit option (TCUNIn) must be a string \"\n                    \"(got {!r}). The invalid keyword will be ignored for the \"\n                    \"purpose of formatting this column.", "metadata": {"file_name": "astropy/io/fits/column.py", "File Name": "astropy/io/fits/column.py", "Classes": "Delayed, _BaseColumnFormat, _ColumnFormat, _AsciiColumnFormat, _FormatX, _FormatP, _FormatQ, ColumnAttribute, Column, ColDefs, _AsciiColDefs, _VLF", "Functions": "_get_index, _unwrapx, _wrapx, _makep, _parse_tformat, _parse_ascii_tformat, _parse_tdim, _scalar_to_format, _cmp_recformats, _convert_fits2record, _convert_record2fits, _dtype_to_recformat, _convert_ascii_format, convert_int"}}, {"code": "'.format(null))\n\n                elif not (format.format in tnull_formats or\n                          (format.format in ('P', 'Q') and\n                           format.p_format in tnull_formats)):\n                    # TODO: We should also check that TNULLn's integer value\n                    # is in the range allowed by the column's format\n                    msg = (\n                        'Column null option (TNULLn) is invalid for binary '\n                        'table columns of type {!r} (got {!r}).  The invalid '\n                        'value will be ignored for the purpose of formatting '\n                        'the data in this column.'.format(format, null))\n\n            if msg is None:\n                valid['null'] = null\n            else:\n                invalid['null'] = (null, msg)\n\n        # Validate the disp option\n        # TODO: Add full parsing and validation of TDISPn keywords\n        if disp is not None and disp != '':\n            msg = None\n            if not isinstance(disp, str):\n                msg = (\n                    'Column disp option (TDISPn) must be a string (got {!r}).'\n                    'The invalid value will be ignored for the purpose of '\n                    'formatting the data in this column.'.format(disp))\n\n            elif (isinstance(format, _AsciiColumnFormat) and\n                    disp[0].upper() == 'L'):\n                # disp is at least one character long and has the 'L' format\n                # which is not recognized for ASCII tables\n                msg = (\n                    \"Column disp option (TDISPn) may not use the 'L' format \"\n                    \"with ASCII table columns.  The invalid value will be \"\n                    \"ignored for the purpose of formatting the data in this \"\n                    \"column.\")", "metadata": {"file_name": "astropy/io/fits/column.py", "File Name": "astropy/io/fits/column.py", "Classes": "Delayed, _BaseColumnFormat, _ColumnFormat, _AsciiColumnFormat, _FormatX, _FormatP, _FormatQ, ColumnAttribute, Column, ColDefs, _AsciiColDefs, _VLF", "Functions": "_get_index, _unwrapx, _wrapx, _makep, _parse_tformat, _parse_ascii_tformat, _parse_tdim, _scalar_to_format, _cmp_recformats, _convert_fits2record, _convert_record2fits, _dtype_to_recformat, _convert_ascii_format, convert_int"}}, {"code": "# Licensed under a 3-clause BSD style license - see PYFITS.rst\n\nimport copy\nimport operator\nimport re\nimport sys\nimport warnings\nimport weakref\nimport numbers\n\nfrom functools import reduce\nfrom collections import OrderedDict\nfrom contextlib import suppress\n\nimport numpy as np\nfrom numpy import char as chararray\n\nfrom . import _numpy_hacks as nh\nfrom .card import Card, CARD_LENGTH\nfrom .util import (pairwise, _is_int, _convert_array, encode_ascii, cmp,\n                   NotifierMixin)\nfrom .verify import VerifyError, VerifyWarning\n\nfrom ...utils import lazyproperty, isiterable, indent\n\n__all__ = ['Column', 'ColDefs', 'Delayed']", "metadata": {"file_name": "astropy/io/fits/column.py", "File Name": "astropy/io/fits/column.py", "Classes": "Delayed, _BaseColumnFormat, _ColumnFormat, _AsciiColumnFormat, _FormatX, _FormatP, _FormatQ, ColumnAttribute, Column, ColDefs, _AsciiColDefs, _VLF", "Functions": "_get_index, _unwrapx, _wrapx, _makep, _parse_tformat, _parse_ascii_tformat, _parse_tdim, _scalar_to_format, _cmp_recformats, _convert_fits2record, _convert_record2fits, _dtype_to_recformat, _convert_ascii_format, convert_int"}}, {"code": "# any of the input argument (except array) can be a Card or just\n        # a number/string\n        kwargs = {'ascii': ascii}\n        for attr in KEYWORD_ATTRIBUTES:\n            value = locals()[attr]  # get the argument's value\n\n            if isinstance(value, Card):\n                value = value.value\n\n            kwargs[attr] = value\n\n        valid_kwargs, invalid_kwargs = self._verify_keywords(**kwargs)\n\n        if invalid_kwargs:\n            msg = ['The following keyword arguments to Column were invalid:']\n\n            for val in invalid_kwargs.values():\n                msg.append(indent(val[1]))\n\n            raise VerifyError('\\n'.join(msg))\n\n        for attr in KEYWORD_ATTRIBUTES:\n            setattr(self, attr, valid_kwargs.get(attr))\n\n        # TODO: Try to eliminate the following two special cases\n        # for recformat and dim:\n        # This is not actually stored as an attribute on columns for some\n        # reason\n        recformat = valid_kwargs['recformat']\n\n        # The 'dim' keyword's original value is stored in self.dim, while\n        # *only* the tuple form is stored in self._dims.\n        self._dims = self.dim\n        self.dim = dim\n\n        # Awful hack to use for now to keep track of whether the column holds\n        # pseudo-unsigned int data\n        self._pseudo_unsigned_ints = False\n\n        # if the column data is not ndarray, make it to be one, i.e.\n        # input arrays can be just list or tuple, not required to be ndarray\n        # does not include Object array because there is no guarantee\n        # the elements in the object array are consistent.", "metadata": {"file_name": "astropy/io/fits/column.py", "File Name": "astropy/io/fits/column.py", "Classes": "Delayed, _BaseColumnFormat, _ColumnFormat, _AsciiColumnFormat, _FormatX, _FormatP, _FormatQ, ColumnAttribute, Column, ColDefs, _AsciiColDefs, _VLF", "Functions": "_get_index, _unwrapx, _wrapx, _makep, _parse_tformat, _parse_ascii_tformat, _parse_tdim, _scalar_to_format, _cmp_recformats, _convert_fits2record, _convert_record2fits, _dtype_to_recformat, _convert_ascii_format, convert_int"}}, {"code": "recformat) < 0:\n                    recformats[idx] = recformat\n\n        # TODO: The handling of VLAs could probably be simplified a bit\n        for row in linereader:\n            nrows += 1\n            if coldefs is not None:\n                continue\n            col = 0\n            idx = 0\n            while idx < len(row):\n                if row[idx] == 'VLA_Length=':\n                    if col < len(vla_lengths):\n                        vla_length = vla_lengths[col]\n                    else:\n                        vla_length = int(row[idx + 1])\n                        vla_lengths.append(vla_length)\n                    idx += 2\n                    while vla_length:\n                        update_recformats(row[idx], col)\n                        vla_length -= 1\n                        idx += 1\n                    col += 1\n                else:\n                    if col >= len(vla_lengths):\n                        vla_lengths.append(None)\n                    update_recformats(row[idx], col)\n                    col += 1\n                    idx += 1\n\n        # Update the recformats for any VLAs\n        for idx, length in enumerate(vla_lengths):\n            if length is not None:\n                recformats[idx] = str(length) + recformats[idx]\n\n        dtype = np.rec.format_parser(recformats, names, None).dtype\n\n        # TODO: In the future maybe enable loading a bit at a time so that we\n        # can convert from this format to an actual FITS file on disk without\n        # needing enough physical memory to hold the entire thing at once\n        hdu = BinTableHDU.from_columns(np.recarray(shape=1, dtype=dtype),\n                                       nrows=nrows, fill=True)\n\n        # TODO: It seems to me a lot of this could/should be handled from\n        # within the FITS_rec class rather than here.", "metadata": {"file_name": "astropy/io/fits/hdu/table.py", "File Name": "astropy/io/fits/hdu/table.py", "Classes": "FITSTableDumpDialect, _TableLikeHDU, _TableBaseHDU, TableHDU, BinTableHDU", "Functions": "_binary_table_byte_swap, format_value, update_recformats, format_value"}}, {"code": "if self.data is None:\n            return\n\n        # Determine the destination (numpy) data type\n        if type is None:\n            type = BITPIX2DTYPE[self._bitpix]\n        _type = getattr(np, type)\n\n        # Determine how to scale the data\n        # bscale and bzero takes priority\n        if bscale is not None and bzero is not None:\n            _scale = bscale\n            _zero = bzero\n        elif bscale is not None:\n            _scale = bscale\n            _zero = 0\n        elif bzero is not None:\n            _scale = 1\n            _zero = bzero\n        elif (option == 'old' and self._orig_bscale is not None and\n                self._orig_bzero is not None):\n            _scale = self._orig_bscale\n            _zero = self._orig_bzero\n        elif option == 'minmax' and not issubclass(_type, np.floating):\n            min = np.minimum.reduce(self.data.flat)\n            max = np.maximum.reduce(self.data.flat)\n\n            if _type == np.uint8:  # uint8 case\n                _zero = min\n                _scale = (max - min) / (2.0 ** 8 - 1)\n            else:\n                _zero = (max + min) / 2.0\n\n                # throw away -2^N\n                nbytes = 8 * _type().itemsize\n                _scale = (max - min) / (2.0 ** nbytes - 2)\n        else:\n            _scale = 1\n            _zero = 0\n\n        # Do the scaling\n        if _zero != 0:\n            # 0.9.6.3 to avoid out of range error for BZERO = +32768\n            # We have to explcitly cast _zero to prevent numpy from raising an\n            # error when doing self.data -= zero, and we do this instead of\n            # self.data = self.data - zero to avoid doubling memory usage.", "metadata": {"file_name": "astropy/io/fits/hdu/image.py", "File Name": "astropy/io/fits/hdu/image.py", "Classes": "_ImageBaseHDU, Section, PrimaryHDU, ImageHDU, _IndexInfo"}}, {"code": "if isinstance(self, ExtensionHDU):\n            firstkey = 'XTENSION'\n            firstval = self._extension\n        else:\n            firstkey = 'SIMPLE'\n            firstval = True\n\n        self.req_cards(firstkey, 0, None, firstval, option, errs)\n        self.req_cards('BITPIX', 1, lambda v: (_is_int(v) and is_valid(v)), 8,\n                       option, errs)\n        self.req_cards('NAXIS', 2,\n                       lambda v: (_is_int(v) and 0 <= v <= 999), 0,\n                       option, errs)\n\n        naxis = self._header.get('NAXIS', 0)\n        if naxis < 1000:\n            for ax in range(3, naxis + 3):\n                key = 'NAXIS' + str(ax - 2)\n                self.req_cards(key, ax,\n                               lambda v: (_is_int(v) and v >= 0),\n                               _extract_number(self._header[key], default=1),\n                               option, errs)\n\n            # Remove NAXISj cards where j is not in range 1, naxis inclusive.\n            for keyword in self._header:\n                if keyword.startswith('NAXIS') and len(keyword) > 5:\n                    try:\n                        number = int(keyword[5:])\n                        if number <= 0 or number > naxis:\n                            raise ValueError\n                    except ValueError:\n                        err_text = (\"NAXISj keyword out of range ('{}' when \"\n                                    \"NAXIS == {})\".format(keyword, naxis))\n\n                        def fix(self=self, keyword=keyword):\n                            del self._header[keyword]\n\n                        errs.append(\n                            self.run_option(option=option, err_text=err_text,\n                                            fix=fix, fix_text=\"Deleted.\"))", "metadata": {"file_name": "astropy/io/fits/hdu/base.py", "File Name": "astropy/io/fits/hdu/base.py", "Classes": "_Delayed, InvalidHDUException, _BaseHDUMeta, _BaseHDU, _CorruptedHDU, _NonstandardHDU, _ValidHDU, ExtensionHDU, NonstandardExtHDU", "Functions": "_hdu_class_from_header, block_iter, fix, fix, fix, fix, fix"}}, {"code": "def verify_checksums(filename):\n    \"\"\"\n    Prints a message if any HDU in `filename` has a bad checksum or datasum.\n    \"\"\"\n\n    with catch_warnings() as wlist:\n        with fits.open(filename, checksum=OPTIONS.checksum_kind) as hdulist:\n            for i, hdu in enumerate(hdulist):\n                # looping on HDUs is needed to read them and verify the\n                # checksums\n                if not OPTIONS.ignore_missing:\n                    if not hdu._checksum:\n                        log.warning('MISSING {!r} .. Checksum not found '\n                                    'in HDU #{}'.format(filename, i))\n                        return 1\n                    if not hdu._datasum:\n                        log.warning('MISSING {!r} .. Datasum not found '\n                                    'in HDU #{}'.format(filename, i))\n                        return 1\n\n    for w in wlist:\n        if str(w.message).startswith(('Checksum verification failed',\n                                      'Datasum verification failed')):\n            log.warning('BAD %r %s', filename, str(w.message))\n            return 1\n\n    log.info('OK {!r}'.format(filename))\n    return 0\n\n\ndef verify_compliance(filename):\n    \"\"\"Check for FITS standard compliance.\"\"\"\n\n    with fits.open(filename) as hdulist:\n        try:\n            hdulist.verify('exception')\n        except fits.VerifyError as exc:\n            log.warning('NONCOMPLIANT %r .. %s',\n                        filename, str(exc).replace('\\n', ' '))\n            return 1\n    return 0\n\n\ndef update(filename):\n    \"\"\"\n    Sets the ``CHECKSUM`` and ``DATASUM`` keywords for each HDU of `filename`.\n\n    Also updates fixes standards violations if possible and requested.\n    \"\"\"\n\n    output_verify = 'silentfix' if OPTIONS.compliance else 'ignore'\n    with fits.open(filename, do_not_scale_image_data=True,\n                   checksum=OPTIONS.checksum_kind, mode='update') as hdulist:\n        hdulist.flush(output_verify=output_verify)", "metadata": {"file_name": "astropy/io/fits/scripts/fitscheck.py", "File Name": "astropy/io/fits/scripts/fitscheck.py", "Functions": "handle_options, setup_logging, verify_checksums, verify_compliance, update, process_file, main"}}, {"code": "self.ignore_keywords = {k.upper() for k in ignore_keywords}\n        self.ignore_comments = {k.upper() for k in ignore_comments}\n        self.ignore_fields = {k.upper() for k in ignore_fields}\n\n        self.rtol = rtol\n        self.atol = atol\n\n        if tolerance is not None:  # This should be removed in the next astropy version\n            warnings.warn(\n                '\"tolerance\" was deprecated in version 2.0 and will be removed in '\n                'a future version. Use argument \"rtol\" instead.',\n                AstropyDeprecationWarning)\n            self.rtol = tolerance  # when tolerance is provided *always* ignore `rtol`\n                                   # during the transition/deprecation period\n\n        self.numdiffs = numdiffs\n        self.ignore_blanks = ignore_blanks\n\n        self.diff_extnames = ()\n        self.diff_extvers = ()\n        self.diff_extlevels = ()\n        self.diff_extension_types = ()\n        self.diff_headers = None\n        self.diff_data = None\n\n        super().__init__(a, b)\n\n    def _diff(self):\n        if self.a.name != self.b.name:\n            self.diff_extnames = (self.a.name, self.b.name)\n\n        if self.a.ver != self.b.ver:\n            self.diff_extvers = (self.a.ver, self.b.ver)\n\n        if self.a.level != self.b.level:\n            self.diff_extlevels = (self.a.level, self.b.level)\n\n        if self.a.header.get('XTENSION') != self.b.header.get('XTENSION'):\n            self.diff_extension_types = (self.a.header.get('XTENSION'),\n                                         self.b.header.get('XTENSION'))\n\n        self.diff_headers = HeaderDiff.fromdiff(self, self.a.header.copy(),\n                                                self.b.header.copy())\n\n        if self.a.", "metadata": {"file_name": "astropy/io/fits/diff.py", "File Name": "astropy/io/fits/diff.py", "Classes": "_BaseDiff, FITSDiff, HDUDiff, HeaderDiff, ImageDataDiff, RawDataDiff, TableDataDiff", "Functions": "diff_values, report_diff_values, report_diff_keyword_attr, where_not_allclose, get_header_values_comments"}}, {"code": "size = 0\n\n        if self.data is not None:\n            self.data._scale_back()\n\n            # Based on the system type, determine the byteorders that\n            # would need to be swapped to get to big-endian output\n            if sys.byteorder == 'little':\n                swap_types = ('<', '=')\n            else:\n                swap_types = ('<',)\n            # deal with unsigned integer 16, 32 and 64 data\n            if _is_pseudo_unsigned(self.data.dtype):\n                # Convert the unsigned array to signed\n                output = np.array(\n                    self.data - _unsigned_zero(self.data.dtype),\n                    dtype='>i{}'.format(self.data.dtype.itemsize))\n                should_swap = False\n            else:\n                output = self.data\n                fname = self.data.dtype.names[0]\n                byteorder = self.data.dtype.fields[fname][0].str[0]\n                should_swap = (byteorder in swap_types)\n\n            if not fileobj.simulateonly:\n\n                if should_swap:\n                    if output.flags.writeable:\n                        output.byteswap(True)\n                        try:\n                            fileobj.writearray(output)\n                        finally:\n                            output.byteswap(True)\n                    else:\n                        # For read-only arrays, there is no way around making\n                        # a byteswapped copy of the data.\n                        fileobj.writearray(output.byteswap(False))\n                else:\n                    fileobj.writearray(output)\n\n            size += output.size * output.itemsize\n        return size\n\n    def _verify(self, option='warn'):\n        errs = super()._verify(option=option)\n\n        # Verify locations and values of mandatory keywords.", "metadata": {"file_name": "astropy/io/fits/hdu/groups.py", "File Name": "astropy/io/fits/hdu/groups.py", "Classes": "Group, GroupData, GroupsHDU", "Functions": "_par_indices, _unique_parnames"}}, {"code": "the data is still a \"view\" (for now)\n                hcopy = header.copy(strip=True)\n                cards.extend(hcopy.cards)\n\n            self._header = Header(cards)\n\n            if isinstance(data, np.ndarray) and data.dtype.fields is not None:\n                # self._data_type is FITS_rec.\n                if isinstance(data, self._data_type):\n                    self.data = data\n                else:\n                    # Just doing a view on the input data screws up unsigned\n                    # columns, so treat those more carefully.", "metadata": {"file_name": "astropy/io/fits/hdu/table.py", "File Name": "astropy/io/fits/hdu/table.py", "Classes": "FITSTableDumpDialect, _TableLikeHDU, _TableBaseHDU, TableHDU, BinTableHDU", "Functions": "_binary_table_byte_swap, format_value, update_recformats, format_value"}}, {"code": "csum = first_tile.view(dtype='uint8').sum()\n\n            # Since CFITSIO uses an unsigned long (which may be different on\n            # different platforms) go ahead and truncate the sum to its\n            # unsigned long value and take the result modulo 10000\n            return (ctypes.c_ulong(csum).value % 10000) + 1\n        elif seed == DITHER_SEED_CLOCK:\n            # This isn't exactly the same algorithm as CFITSIO, but that's okay\n            # since the result is meant to be arbitrary. The primary difference\n            # is that CFITSIO incorporates the HDU number into the result in\n            # the hopes of heading off the possibility of the same seed being\n            # generated for two HDUs at the same time.  Here instead we just\n            # add in the HDU object's id\n            return ((sum(int(x) for x in math.modf(time.time())) + id(self)) %\n                    10000) + 1\n        else:\n            return seed", "metadata": {"file_name": "astropy/io/fits/hdu/compressed.py", "File Name": "astropy/io/fits/hdu/compressed.py", "Classes": "CompImageHeader, CompImageHDU"}}, {"code": "parser.add_option(\n        '-a', '--atol', '--absolute-tolerance', type='float', default=None,\n        dest='atol', metavar='NUMBER',\n        help='The absolute tolerance for comparison of two numbers, '\n             'specifically two floating point numbers.  This applies to data '\n             'in both images and tables, and to floating point keyword values '\n             'in headers (default %default).')\n\n    parser.add_option(\n        '-b', '--no-ignore-blanks', action='store_false',\n        dest='ignore_blanks', default=True,\n        help=\"Don't ignore trailing blanks (whitespace) in string values.  \"\n             \"Otherwise trailing blanks both in header keywords/values and in \"\n             \"table column values) are not treated as significant i.e., \"\n             \"without this option 'ABCDEF   ' and 'ABCDEF' are considered \"\n             \"equivalent. \")\n\n    parser.add_option(\n        '--no-ignore-blank-cards', action='store_false',\n        dest='ignore_blank_cards', default=True,\n        help=\"Don't ignore entirely blank cards in headers.  Normally fitsdiff \"\n             \"does not consider blank cards when comparing headers, but this \"\n             \"will ensure that even blank cards match up. \")\n\n    parser.add_option(\n        '--exact', action='store_true',\n        dest='exact_comparisons', default=False,\n        help=\"Report ALL differences, \"\n             \"overriding command-line options and FITSDIFF_SETTINGS. \")\n\n    parser.add_option(\n        '-o', '--output-file', metavar='FILE',\n        help='Output results to this file; otherwise results are printed to '\n             'stdout.')\n\n    group = optparse.OptionGroup(parser, 'Header Comparison Options')\n\n    group.add_option(\n        '-k', '--ignore-keywords', action='callback', callback=store_list,\n        nargs=1, type='str', default=[], dest='ignore_keywords',\n        metavar='KEYWORDS',\n        help='Comma-separated list of keywords not to be compared.  Keywords '\n             'may contain wildcard patterns.  To exclude all keywords, use '\n             '\"*\"; make sure to have double or single quotes around the '\n             'asterisk on the command-line.')", "metadata": {"file_name": "astropy/io/fits/scripts/fitsdiff.py", "File Name": "astropy/io/fits/scripts/fitsdiff.py", "Classes": "HelpFormatter, LevelFilter", "Functions": "handle_options, setup_logging, match_files, main, store_list"}}, {"code": "def _convert_record2fits(format):\n    \"\"\"\n    Convert record format spec to FITS format spec.\n    \"\"\"\n\n    recformat, kind, dtype = _dtype_to_recformat(format)\n    shape = dtype.shape\n    itemsize = dtype.base.itemsize\n    if dtype.char == 'U':\n        # Unicode dtype--itemsize is 4 times actual ASCII character length,\n        # which what matters for FITS column formats\n        # Use dtype.base--dtype may be a multi-dimensional dtype\n        itemsize = itemsize // 4\n\n    option = str(itemsize)\n\n    ndims = len(shape)\n    repeat = 1\n    if ndims > 0:\n        nel = np.array(shape, dtype='i8').prod()\n        if nel > 1:\n            repeat = nel\n\n    if kind == 'a':\n        # This is a kludge that will place string arrays into a\n        # single field, so at least we won't lose data.  Need to\n        # use a TDIM keyword to fix this, declaring as (slength,\n        # dim1, dim2, ...)  as mwrfits does\n\n        ntot = int(repeat) * int(option)\n\n        output_format = str(ntot) + 'A'\n    elif recformat in NUMPY2FITS:  # record format\n        if repeat != 1:\n            repeat = str(repeat)\n        else:\n            repeat = ''\n        output_format = repeat + NUMPY2FITS[recformat]\n    else:\n        raise ValueError('Illegal format {}.'.format(format))\n\n    return output_format", "metadata": {"file_name": "astropy/io/fits/column.py", "File Name": "astropy/io/fits/column.py", "Classes": "Delayed, _BaseColumnFormat, _ColumnFormat, _AsciiColumnFormat, _FormatX, _FormatP, _FormatQ, ColumnAttribute, Column, ColDefs, _AsciiColDefs, _VLF", "Functions": "_get_index, _unwrapx, _wrapx, _makep, _parse_tformat, _parse_ascii_tformat, _parse_tdim, _scalar_to_format, _cmp_recformats, _convert_fits2record, _convert_record2fits, _dtype_to_recformat, _convert_ascii_format, convert_int"}}, {"code": "rtol = tolerance  # when tolerance is provided *always* ignore `rtol`\n                                   # during the transition/deprecation period\n\n        self.ignore_blanks = ignore_blanks\n        self.ignore_blank_cards = ignore_blank_cards\n\n        self.diff_hdu_count = ()\n        self.diff_hdus = []\n\n        try:\n            super().__init__(a, b)\n        finally:\n            if close_a:\n                a.close()\n            if close_b:\n                b.close()\n\n    def _diff(self):\n        if len(self.a) != len(self.b):\n            self.diff_hdu_count = (len(self.a), len(self.b))\n\n        # For now, just compare the extensions one by one in order...might\n        # allow some more sophisticated types of diffing later...\n        # TODO: Somehow or another simplify the passing around of diff\n        # options--this will become important as the number of options grows\n        for idx in range(min(len(self.a), len(self.b))):\n            hdu_diff = HDUDiff.fromdiff(self, self.a[idx], self.b[idx])\n\n            if not hdu_diff.identical:\n                self.diff_hdus.append((idx, hdu_diff))\n\n    def _report(self):\n        wrapper = textwrap.TextWrapper(initial_indent='  ',\n                                       subsequent_indent='  ')\n\n        # print out heading and parameter values\n        filenamea = self.a.filename()\n        if not filenamea:\n            filenamea = '<{} object at {:#x}>'.format(\n                self.a.__class__.__name__, id(self.a))\n\n        filenameb = self.b.filename()\n        if not filenameb:\n            filenameb = '<{} object at {:#x}>'.format(\n                self.b.__class__.__name__, id(self.b))\n\n        self._fileobj.write('\\n')\n        self._writeln(' fitsdiff: {}'.format(__version__))\n        self._writeln(' a: {}\\n b: {}'.format(filenamea, filenameb))\n        if self.", "metadata": {"file_name": "astropy/io/fits/diff.py", "File Name": "astropy/io/fits/diff.py", "Classes": "_BaseDiff, FITSDiff, HDUDiff, HeaderDiff, ImageDataDiff, RawDataDiff, TableDataDiff", "Functions": "diff_values, report_diff_values, report_diff_keyword_attr, where_not_allclose, get_header_values_comments"}}, {"code": "fix_value : str, int, float, complex, bool, None\n            A valid value for a FITS keyword to to use if the given ``test``\n            fails to replace an invalid value.  In other words, this provides\n            a default value to use as a replacement if the keyword's current\n            value is invalid.  If `None`, there is no replacement value and the\n            keyword is unfixable.\n\n        option : str\n            Output verification option.  Must be one of ``\"fix\"``,\n            ``\"silentfix\"``, ``\"ignore\"``, ``\"warn\"``, or\n            ``\"exception\"``.  May also be any combination of ``\"fix\"`` or\n            ``\"silentfix\"`` with ``\"+ignore\"``, ``+warn``, or ``+exception\"\n            (e.g. ``\"fix+warn\"``).  See :ref:`verify` for more info.\n\n        errlist : list\n            A list of validation errors already found in the FITS file; this is\n            used primarily for the validation system to collect errors across\n            multiple HDUs and multiple calls to `req_cards`.\n\n        Notes\n        -----\n        If ``pos=None``, the card can be anywhere in the header.  If the card\n        does not exist, the new card will have the ``fix_value`` as its value\n        when created.  Also check the card's value by using the ``test``\n        argument.\n        \"\"\"", "metadata": {"file_name": "astropy/io/fits/hdu/base.py", "File Name": "astropy/io/fits/hdu/base.py", "Classes": "_Delayed, InvalidHDUException, _BaseHDUMeta, _BaseHDU, _CorruptedHDU, _NonstandardHDU, _ValidHDU, ExtensionHDU, NonstandardExtHDU", "Functions": "_hdu_class_from_header, block_iter, fix, fix, fix, fix, fix"}}, {"code": "ignore_keywords:\n            ignore_keywords = ' '.join(sorted(self.ignore_keywords))\n            self._writeln(' Keyword(s) not to be compared:\\n{}'\n                          .format(wrapper.fill(ignore_keywords)))\n\n        if self.ignore_comments:\n            ignore_comments = ' '.join(sorted(self.ignore_comments))\n            self._writeln(' Keyword(s) whose comments are not to be compared'\n                          ':\\n{}'.format(wrapper.fill(ignore_comments)))\n        if self.ignore_fields:\n            ignore_fields = ' '.join(sorted(self.ignore_fields))\n            self._writeln(' Table column(s) not to be compared:\\n{}'\n                          .format(wrapper.fill(ignore_fields)))\n        self._writeln(' Maximum number of different data values to be '\n                      'reported: {}'.format(self.numdiffs))\n        self._writeln(' Relative tolerance: {}, Absolute tolerance: {}'\n                      .format(self.rtol, self.atol))\n\n        if self.diff_hdu_count:\n            self._fileobj.write('\\n')\n            self._writeln('Files contain different numbers of HDUs:')\n            self._writeln(' a: {}'.format(self.diff_hdu_count[0]))\n            self._writeln(' b: {}'.format(self.diff_hdu_count[1]))\n\n            if not self.diff_hdus:\n                self._writeln('No differences found between common HDUs.')\n                return\n        elif not self.diff_hdus:\n            self._fileobj.write('\\n')\n            self._writeln('No differences found.')\n            return\n\n        for idx, hdu_diff in self.diff_hdus:\n            # print out the extension heading\n            if idx == 0:\n                self._fileobj.write('\\n')\n                self._writeln('Primary HDU:')\n            else:\n                self._fileobj.write('\\n')\n                self._writeln('Extension HDU {}:'.format(idx))\n            hdu_diff.report(self._fileobj, indent=self._indent + 1)", "metadata": {"file_name": "astropy/io/fits/diff.py", "File Name": "astropy/io/fits/diff.py", "Classes": "_BaseDiff, FITSDiff, HDUDiff, HeaderDiff, ImageDataDiff, RawDataDiff, TableDataDiff", "Functions": "diff_values, report_diff_values, report_diff_keyword_attr, where_not_allclose, get_header_values_comments"}}, {"code": "# for commentary cards, no need to parse further\n        # Likewise for invalid cards\n        if self.keyword.upper() in self._commentary_keywords or self._invalid:\n            return self._image[KEYWORD_LENGTH:].rstrip()\n\n        if self._check_if_rvkc(self._image):\n            return self._value\n\n        if len(self._image) > self.length:\n            values = []\n            for card in self._itersubcards():\n                value = card.value.rstrip().replace(\"''\", \"'\")\n                if value and value[-1] == '&':\n                    value = value[:-1]\n                values.append(value)\n\n            value = ''.join(values)\n\n            self._valuestring = value\n            return value\n\n        m = self._value_NFSC_RE.match(self._split()[1])\n\n        if m is None:\n            raise VerifyError(\"Unparsable card ({}), fix it first with \"\n                              \".verify('fix').\".format(self.keyword))\n\n        if m.group('bool') is not None:\n            value = m.group('bool') == 'T'\n        elif m.group('strg') is not None:\n            value = re.sub(\"''\", \"'\", m.group('strg'))\n        elif m.group('numr') is not None:\n            #  Check for numbers with leading 0s.\n            numr = self._number_NFSC_RE.match(m.group('numr'))\n            digt = translate(numr.group('digt'), FIX_FP_TABLE2, ' ')\n            if numr.group('sign') is None:\n                sign = ''\n            else:\n                sign = numr.group('sign')\n            value = _str_to_num(sign + digt)\n\n        elif m.group('cplx') is not None:\n            #  Check for numbers with leading 0s.", "metadata": {"file_name": "astropy/io/fits/card.py", "File Name": "astropy/io/fits/card.py", "Classes": "Undefined, Card", "Functions": "_int_or_float, _format_float, _pad"}}, {"code": "# Licensed under a 3-clause BSD style license - see PYFITS.rst\n\n\n\nimport datetime\nimport os\nimport sys\nimport warnings\nfrom contextlib import suppress\nfrom inspect import signature, Parameter\n\nimport numpy as np\n\nfrom .. import conf\nfrom ..file import _File\nfrom ..header import Header, _pad_length\nfrom ..util import (_is_int, _is_pseudo_unsigned, _unsigned_zero,\n                    itersubclasses, decode_ascii, _get_array_mmap, first,\n                    _free_space_check, _extract_number)\nfrom ..verify import _Verify, _ErrList\n\nfrom ....utils import lazyproperty\nfrom ....utils.exceptions import AstropyUserWarning\nfrom ....utils.decorators import deprecated_renamed_argument\n\n\nclass _Delayed:\n    pass\n\n\nDELAYED = _Delayed()\n\n\nBITPIX2DTYPE = {8: 'uint8', 16: 'int16', 32: 'int32', 64: 'int64',\n                -32: 'float32', -64: 'float64'}\n\"\"\"Maps FITS BITPIX values to Numpy dtype names.\"\"\"\n\nDTYPE2BITPIX = {'uint8': 8, 'int16': 16, 'uint16': 16, 'int32': 32,\n                'uint32': 32, 'int64': 64, 'uint64': 64, 'float32': -32,\n                'float64': -64}\n\"\"\"\nMaps Numpy dtype names to FITS BITPIX values (this includes unsigned\nintegers, with the assumption that the pseudo-unsigned integer convention\nwill be used in this case.\n\"\"\"\n\n\nclass InvalidHDUException(Exception):\n    \"\"\"\n    A custom exception class used mainly to signal to _BaseHDU.__new__ that\n    an HDU cannot possibly be considered valid, and must be assumed to be\n    corrupted.\n    \"\"\"", "metadata": {"file_name": "astropy/io/fits/hdu/base.py", "File Name": "astropy/io/fits/hdu/base.py", "Classes": "_Delayed, InvalidHDUException, _BaseHDUMeta, _BaseHDU, _CorruptedHDU, _NonstandardHDU, _ValidHDU, ExtensionHDU, NonstandardExtHDU", "Functions": "_hdu_class_from_header, block_iter, fix, fix, fix, fix, fix"}}, {"code": "_output_checksum = True\n\n    @property\n    def header(self):\n        return self._header\n\n    @header.setter\n    def header(self, value):\n        self._header = value\n\n    @property\n    def name(self):\n        # Convert the value to a string to be flexible in some pathological\n        # cases (see ticket #96)\n        return str(self._header.get('EXTNAME', self._default_name))\n\n    @name.setter\n    def name(self, value):\n        if not isinstance(value, str):\n            raise TypeError(\"'name' attribute must be a string\")\n        if not conf.extension_name_case_sensitive:\n            value = value.upper()\n        if 'EXTNAME' in self._header:\n            self._header['EXTNAME'] = value\n        else:\n            self._header['EXTNAME'] = (value, 'extension name')\n\n    @property\n    def ver(self):\n        return self._header.get('EXTVER', 1)\n\n    @ver.setter\n    def ver(self, value):\n        if not _is_int(value):\n            raise TypeError(\"'ver' attribute must be an integer\")\n        if 'EXTVER' in self._header:\n            self._header['EXTVER'] = value\n        else:\n            self._header['EXTVER'] = (value, 'extension value')\n\n    @property\n    def level(self):\n        return self._header.get('EXTLEVEL', 1)\n\n    @level.setter\n    def level(self, value):\n        if not _is_int(value):\n            raise TypeError(\"'level' attribute must be an integer\")\n        if 'EXTLEVEL' in self._header:\n            self._header['EXTLEVEL'] = value\n        else:\n            self._header['EXTLEVEL'] = (value, 'extension level')\n\n    @property\n    def is_image(self):\n        return (\n            self.name == 'PRIMARY' or\n            ('XTENSION' in self._header and\n             (self.", "metadata": {"file_name": "astropy/io/fits/hdu/base.py", "File Name": "astropy/io/fits/hdu/base.py", "Classes": "_Delayed, InvalidHDUException, _BaseHDUMeta, _BaseHDU, _CorruptedHDU, _NonstandardHDU, _ValidHDU, ExtensionHDU, NonstandardExtHDU", "Functions": "_hdu_class_from_header, block_iter, fix, fix, fix, fix, fix"}}, {"code": ".. versionchanged:: 2.0\n               ``rtol`` replaces the deprecated ``tolerance`` argument.\n\n        atol : float, optional\n            The allowed absolute difference. See also ``rtol`` parameter.\n\n            .. versionadded:: 2.0\n\n        ignore_blanks : bool, optional\n            Ignore extra whitespace at the end of string values either in\n            headers or data. Extra leading whitespace is not ignored\n            (default: True).\n\n        ignore_blank_cards : bool, optional\n            Ignore all cards that are blank, i.e. they only contain\n            whitespace (default: True).\n        \"\"\"\n\n        if isinstance(a, str):\n            try:\n                a = fitsopen(a)\n            except Exception as exc:\n                raise OSError(\"error opening file a ({}): {}: {}\".format(\n                        a, exc.__class__.__name__, exc.args[0]))\n            close_a = True\n        else:\n            close_a = False\n\n        if isinstance(b, str):\n            try:\n                b = fitsopen(b)\n            except Exception as exc:\n                raise OSError(\"error opening file b ({}): {}: {}\".format(\n                        b, exc.__class__.__name__, exc.args[0]))\n            close_b = True\n        else:\n            close_b = False\n\n        # Normalize keywords/fields to ignore to upper case\n        self.ignore_keywords = set(k.upper() for k in ignore_keywords)\n        self.ignore_comments = set(k.upper() for k in ignore_comments)\n        self.ignore_fields = set(k.upper() for k in ignore_fields)\n\n        self.numdiffs = numdiffs\n        self.rtol = rtol\n        self.atol = atol\n\n        if tolerance is not None:  # This should be removed in the next astropy version\n            warnings.warn(\n                '\"tolerance\" was deprecated in version 2.0 and will be removed in '\n                'a future version. Use argument \"rtol\" instead.',\n                AstropyDeprecationWarning)\n            self.", "metadata": {"file_name": "astropy/io/fits/diff.py", "File Name": "astropy/io/fits/diff.py", "Classes": "_BaseDiff, FITSDiff, HDUDiff, HeaderDiff, ImageDataDiff, RawDataDiff, TableDataDiff", "Functions": "diff_values, report_diff_values, report_diff_keyword_attr, where_not_allclose, get_header_values_comments"}}, {"code": "if not fileobj and self._file:\n            root = os.path.splitext(self._file.name)[0]\n            fileobj = root + '.txt'\n\n        close_file = False\n\n        if isinstance(fileobj, str):\n            fileobj = open(fileobj, 'w')\n            close_file = True\n\n        linewriter = csv.writer(fileobj, dialect=FITSTableDumpDialect)\n\n        # Process each row of the table and output one row at a time\n        def format_value(val, format):\n            if format[0] == 'S':\n                itemsize = int(format[1:])\n                return '{:{size}}'.format(val, size=itemsize)\n            elif format in np.typecodes['AllInteger']:\n                # output integer\n                return '{:21d}'.format(val)\n            elif format in np.typecodes['Complex']:\n                return '{:21.15g}+{:.15g}j'.format(val.real, val.imag)\n            elif format in np.typecodes['Float']:\n                # output floating point\n                return '{:#21.15g}'.format(val)\n\n        for row in self.data:\n            line = []   # the line for this row of the table\n\n            # Process each column of the row.", "metadata": {"file_name": "astropy/io/fits/hdu/table.py", "File Name": "astropy/io/fits/hdu/table.py", "Classes": "FITSTableDumpDialect, _TableLikeHDU, _TableBaseHDU, TableHDU, BinTableHDU", "Functions": "_binary_table_byte_swap, format_value, update_recformats, format_value"}}, {"code": "def _extract_number(value, default):\n    \"\"\"\n    Attempts to extract an integer number from the given value. If the\n    extraction fails, the value of the 'default' argument is returned.\n    \"\"\"\n\n    try:\n        # The _str_to_num method converts the value to string/float\n        # so we need to perform one additional conversion to int on top\n        return int(_str_to_num(value))\n    except (TypeError, ValueError):\n        return default\n\n\ndef get_testdata_filepath(filename):\n    \"\"\"\n    Return a string representing the path to the file requested from the\n    io.fits test data set.\n\n    .. versionadded:: 2.0.3\n\n    Parameters\n    ----------\n    filename : str\n        The filename of the test data file.\n\n    Returns\n    -------\n    filepath : str\n        The path to the requested file.\n    \"\"\"\n    return data.get_pkg_data_filename(\n        'io/fits/tests/data/{}'.format(filename), 'astropy')\n\n\ndef _rstrip_inplace(array):\n    \"\"\"\n    Performs an in-place rstrip operation on string arrays. This is necessary\n    since the built-in `np.char.rstrip` in Numpy does not perform an in-place\n    calculation.\n    \"\"\"\n\n    # The following implementation convert the string to unsigned integers of\n    # the right length. Trailing spaces (which are represented as 32) are then\n    # converted to null characters (represented as zeros).", "metadata": {"file_name": "astropy/io/fits/util.py", "File Name": "astropy/io/fits/util.py", "Classes": "NotifierMixin, SigintHandler", "Functions": "first, itersubclasses, ignore_sigint, pairwise, encode_ascii, decode_ascii, isreadable, iswritable, isfile, fileobj_open, fileobj_name, fileobj_closed, fileobj_mode, _fileobj_normalize_mode, fileobj_is_binary, translate, fill, _array_from_file, _array_to_file, _array_to_file_like, _write_string, _convert_array, _unsigned_zero, _is_pseudo_unsigned, _is_int, _str_to_num, _words_group, _tmp_name, _get_array_mmap, _free_space_check, _extract_number, get_testdata_filepath, _rstrip_inplace, wrapped, maybe_fill"}}, {"code": "since they're a detail\n        # specific to FITS binary tables\n        if (any(type(r) in (_FormatP, _FormatQ)\n                for r in columns._recformats) and\n                self._data_size is not None and\n                self._data_size > self._theap):\n            # We have a heap; include it in the raw_data\n            raw_data = self._get_raw_data(self._data_size, np.uint8,\n                                          self._data_offset)\n            data = raw_data[:self._theap].view(dtype=columns.dtype,\n                                               type=np.rec.recarray)\n        else:\n            raw_data = self._get_raw_data(self._nrows, columns.dtype,\n                                          self._data_offset)\n            if raw_data is None:\n                # This can happen when a brand new table HDU is being created\n                # and no data has been assigned to the columns, which case just\n                # return an empty array\n                raw_data = np.array([], dtype=columns.dtype)\n\n            data = raw_data.view(np.rec.recarray)\n\n        self._init_tbdata(data)\n        data = data.view(self._data_type)\n        columns._add_listener(data)\n        return data\n\n    def _init_tbdata(self, data):\n        columns = self.columns\n\n        data.dtype = data.dtype.newbyteorder('>')\n\n        # hack to enable pseudo-uint support\n        data._uint = self._uint\n\n        # pass datLoc, for P format\n        data._heapoffset = self._theap\n        data._heapsize = self._header['PCOUNT']\n        tbsize = self._header['NAXIS1'] * self._header['NAXIS2']\n        data._gap = self._theap - tbsize\n\n        # pass the attributes\n        for idx, col in enumerate(columns):\n            # get the data for each column object from the rec.recarray\n            col.array = data.", "metadata": {"file_name": "astropy/io/fits/hdu/table.py", "File Name": "astropy/io/fits/hdu/table.py", "Classes": "FITSTableDumpDialect, _TableLikeHDU, _TableBaseHDU, TableHDU, BinTableHDU", "Functions": "_binary_table_byte_swap, format_value, update_recformats, format_value"}}, {"code": "The 'BLANK' keyword \"\n                \"is only applicable to integer data, and will be ignored \"\n                \"in this HDU.\")\n            self._blank = None\n\n        for msg in messages:\n            warnings.warn(msg, VerifyWarning)\n\n    def _prewriteto(self, checksum=False, inplace=False):\n        if self._scale_back:\n            self._scale_internal(BITPIX2DTYPE[self._orig_bitpix],\n                                 blank=self._orig_blank)\n\n        self.update_header()\n        if not inplace and self._data_needs_rescale:\n            # Go ahead and load the scaled image data and update the header\n            # with the correct post-rescaling headers\n            _ = self.data\n\n        return super()._prewriteto(checksum, inplace)\n\n    def _writedata_internal(self, fileobj):\n        size = 0\n\n        if self.data is not None:\n            # Based on the system type, determine the byteorders that\n            # would need to be swapped to get to big-endian output\n            if sys.byteorder == 'little':\n                swap_types = ('<', '=')\n            else:\n                swap_types = ('<',)\n            # deal with unsigned integer 16, 32 and 64 data\n            if _is_pseudo_unsigned(self.data.dtype):\n                # Convert the unsigned array to signed\n                output = np.array(\n                    self.data - _unsigned_zero(self.data.dtype),\n                    dtype='>i{}'.format(self.data.dtype.itemsize))\n                should_swap = False\n            else:\n                output = self.data\n                byteorder = output.dtype.str[0]\n                should_swap = (byteorder in swap_types)\n\n            if not fileobj.simulateonly:\n\n                if should_swap:\n                    if output.flags.writeable:\n                        output.byteswap(True)\n                        try:\n                            fileobj.writearray(output)\n                        finally:\n                            output.byteswap(True)\n                    else:\n                        # For read-only arrays, there is no way around making\n                        # a byteswapped copy of the data.", "metadata": {"file_name": "astropy/io/fits/hdu/image.py", "File Name": "astropy/io/fits/hdu/image.py", "Classes": "_ImageBaseHDU, Section, PrimaryHDU, ImageHDU, _IndexInfo"}}, {"code": "zbitpix = self._image_header['BITPIX']\n\n        if zbitpix < 0 and quantize_level != 0.0:\n            # floating point image has 'COMPRESSED_DATA',\n            # 'UNCOMPRESSED_DATA', 'ZSCALE', and 'ZZERO' columns (unless using\n            # lossless compression, per CFITSIO)\n            ncols = 4\n\n            # CFITSIO 3.28 and up automatically use the GZIP_COMPRESSED_DATA\n            # store floating point data that couldn't be quantized, instead\n            # of the UNCOMPRESSED_DATA column.  There's no way to control\n            # this behavior so the only way to determine which behavior will\n            # be employed is via the CFITSIO version\n\n            if CFITSIO_SUPPORTS_GZIPDATA:\n                ttype2 = 'GZIP_COMPRESSED_DATA'\n                # The required format for the GZIP_COMPRESSED_DATA is actually\n                # missing from the standard docs, but CFITSIO suggests it\n                # should be 1PB, which is logical.\n                tform2 = '1QB' if huge_hdu else '1PB'\n            else:\n                # Q format is not supported for UNCOMPRESSED_DATA columns.\n                ttype2 = 'UNCOMPRESSED_DATA'\n                if zbitpix == 8:\n                    tform2 = '1QB' if huge_hdu else '1PB'\n                elif zbitpix == 16:\n                    tform2 = '1QI' if huge_hdu else '1PI'\n                elif zbitpix == 32:\n                    tform2 = '1QJ' if huge_hdu else '1PJ'\n                elif zbitpix == -32:\n                    tform2 = '1QE' if huge_hdu else '1PE'\n                else:\n                    tform2 = '1QD' if huge_hdu else '1PD'\n\n            # Set up the second column for the table that will hold any\n            # uncompressable data.", "metadata": {"file_name": "astropy/io/fits/hdu/compressed.py", "File Name": "astropy/io/fits/hdu/compressed.py", "Classes": "CompImageHeader, CompImageHDU"}}, {"code": "'.format(\n                        filename, backup), AstropyUserWarning)\n                try:\n                    shutil.copy(filename, backup)\n                except OSError as exc:\n                    raise OSError('Failed to save backup to destination {}: '\n                                  '{}'.format(filename, exc))\n\n        self.verify(option=output_verify)\n\n        if self._file.mode in ('append', 'ostream'):\n            for hdu in self:\n                if verbose:\n                    try:\n                        extver = str(hdu._header['extver'])\n                    except KeyError:\n                        extver = ''\n\n                # only append HDU's which are \"new\"\n                if hdu._new:\n                    hdu._prewriteto(checksum=hdu._output_checksum)\n                    with _free_space_check(self):\n                        hdu._writeto(self._file)\n                        if verbose:\n                            print('append HDU', hdu.name, extver)\n                        hdu._new = False\n                    hdu._postwriteto()\n\n        elif self._file.mode == 'update':\n            self._flush_update()\n\n    def update_extend(self):\n        \"\"\"\n        Make sure that if the primary header needs the keyword ``EXTEND`` that\n        it has it and it is correct.\n        \"\"\"", "metadata": {"file_name": "astropy/io/fits/hdu/hdulist.py", "File Name": "astropy/io/fits/hdu/hdulist.py", "Classes": "HDUList", "Functions": "fitsopen, get_first_ext, fix, fix"}}, {"code": "if isinstance(input, HDUList):\n\n        # Parse all table objects\n        tables = OrderedDict()\n        for ihdu, hdu_item in enumerate(input):\n            if isinstance(hdu_item, (TableHDU, BinTableHDU, GroupsHDU)):\n                tables[ihdu] = hdu_item\n\n        if len(tables) > 1:\n            if hdu is None:\n                warnings.warn(\"hdu= was not specified but multiple tables\"\n                              \" are present, reading in first available\"\n                              \" table (hdu={0})\".format(first(tables)),\n                              AstropyUserWarning)\n                hdu = first(tables)\n\n            # hdu might not be an integer, so we first need to convert it\n            # to the correct HDU index\n            hdu = input.index_of(hdu)\n\n            if hdu in tables:\n                table = tables[hdu]\n            else:\n                raise ValueError(\"No table found in hdu={0}\".format(hdu))\n\n        elif len(tables) == 1:\n            table = tables[first(tables)]\n        else:\n            raise ValueError(\"No table found\")\n\n    elif isinstance(input, (TableHDU, BinTableHDU, GroupsHDU)):\n\n        table = input\n\n    else:\n\n        hdulist = fits_open(input, character_as_bytes=character_as_bytes,\n                            memmap=memmap)\n\n        try:\n            return read_table_fits(hdulist, hdu=hdu,\n                                   astropy_native=astropy_native)\n        finally:\n            hdulist.close()\n\n    # Check if table is masked\n    masked = any(col.null is not None for col in table.columns)\n\n    # TODO: in future, it may make more sense to do this column-by-column,\n    # rather than via the structured array.\n\n    # In the loop below we access the data using data[col.name] rather than\n    # col.array to make sure that the data is scaled correctly if needed.", "metadata": {"file_name": "astropy/io/fits/connect.py", "File Name": "astropy/io/fits/connect.py", "Functions": "is_column_keyword, is_fits, _decode_mixins, read_table_fits, _encode_mixins, write_table_fits"}}, {"code": "def _cmp_recformats(f1, f2):\n    \"\"\"\n    Compares two numpy recformats using the ordering given by FORMATORDER.\n    \"\"\"\n\n    if f1[0] == 'a' and f2[0] == 'a':\n        return cmp(int(f1[1:]), int(f2[1:]))\n    else:\n        f1, f2 = NUMPY2FITS[f1], NUMPY2FITS[f2]\n        return cmp(FORMATORDER.index(f1), FORMATORDER.index(f2))", "metadata": {"file_name": "astropy/io/fits/column.py", "File Name": "astropy/io/fits/column.py", "Classes": "Delayed, _BaseColumnFormat, _ColumnFormat, _AsciiColumnFormat, _FormatX, _FormatP, _FormatQ, ColumnAttribute, Column, ColDefs, _AsciiColDefs, _VLF", "Functions": "_get_index, _unwrapx, _wrapx, _makep, _parse_tformat, _parse_ascii_tformat, _parse_tdim, _scalar_to_format, _cmp_recformats, _convert_fits2record, _convert_record2fits, _dtype_to_recformat, _convert_ascii_format, convert_int"}}, {"code": "_file.read(hdrsize))\n            # The header size is unchanged, but the data location may be\n            # different from before depending on if previous HDUs were resized\n            datloc = fileobj.tell()\n\n        if self._data_loaded:\n            if self.data is not None:\n                # Seek through the array's bases for an memmap'd array; we\n                # can't rely on the _File object to give us this info since\n                # the user may have replaced the previous mmap'd array\n                if copy or self._data_replaced:\n                    # Of course, if we're copying the data to a new file\n                    # we don't care about flushing the original mmap;\n                    # instead just read it into the new file\n                    array_mmap = None\n                else:\n                    array_mmap = _get_array_mmap(self.data)\n\n                if array_mmap is not None:\n                    array_mmap.flush()\n                else:\n                    self._file.seek(self._data_offset)\n                    datloc, datsize = self._writedata(fileobj)\n        elif copy:\n            datsize = self._writedata_direct_copy(fileobj)\n\n        self._header_offset = hdrloc\n        self._data_offset = datloc\n        self._data_size = datsize\n        self._data_replaced = False\n\n    def _close(self, closed=True):\n        # If the data was mmap'd, close the underlying mmap (this will\n        # prevent any future access to the .data attribute if there are\n        # not other references to it; if there are other references then\n        # it is up to the user to clean those up\n        if (closed and self._data_loaded and\n                _get_array_mmap(self.data) is not None):\n            del self.data", "metadata": {"file_name": "astropy/io/fits/hdu/base.py", "File Name": "astropy/io/fits/hdu/base.py", "Classes": "_Delayed, InvalidHDUException, _BaseHDUMeta, _BaseHDU, _CorruptedHDU, _NonstandardHDU, _ValidHDU, ExtensionHDU, NonstandardExtHDU", "Functions": "_hdu_class_from_header, block_iter, fix, fix, fix, fix, fix"}}, {"code": "# Public API compatibility imports\n# These need to come after the global config variables, as some of the\n# submodules use them\nfrom . import card\nfrom . import column\nfrom . import convenience\nfrom . import hdu\nfrom .card import *\nfrom .column import *\nfrom .convenience import *\nfrom .diff import *\nfrom .fitsrec import FITS_record, FITS_rec\nfrom .hdu import *\n\nfrom .hdu.groups import GroupData\nfrom .hdu.hdulist import fitsopen as open\nfrom .hdu.image import Section\nfrom .header import Header\nfrom .verify import VerifyError\n\n\n__all__ = (['Conf', 'conf'] + card.__all__ + column.__all__ +\n           convenience.__all__ + hdu.__all__ +\n           ['FITS_record', 'FITS_rec', 'GroupData', 'open', 'Section',\n            'Header', 'VerifyError', 'conf'])", "metadata": {"file_name": "astropy/io/fits/__init__.py", "File Name": "astropy/io/fits/__init__.py", "Classes": "Conf"}}, {"code": "if 'array' in self.__dict__:\n                            del self.__dict__['array']\n                        return\n\n            if getattr(base, 'base', None) is not None:\n                base = base.base\n            else:\n                break\n\n        self.__dict__['array'] = array\n\n    @array.deleter\n    def array(self):\n        try:\n            del self.__dict__['array']\n        except KeyError:\n            pass\n\n        self._parent_fits_rec = None\n\n    @ColumnAttribute('TTYPE')\n    def name(col, name):\n        if name is None:\n            # Allow None to indicate deleting the name, or to just indicate an\n            # unspecified name (when creating a new Column).\n            return\n\n        # Check that the name meets the recommended standard--other column\n        # names are *allowed*, but will be discouraged\n        if isinstance(name, str) and not TTYPE_RE.match(name):\n            warnings.warn(\n                'It is strongly recommended that column names contain only '\n                'upper and lower-case ASCII letters, digits, or underscores '\n                'for maximum compatibility with other software '\n                '(got {0!r}).'.format(name), VerifyWarning)\n\n        # This ensures that the new name can fit into a single FITS card\n        # without any special extension like CONTINUE cards or the like.\n        if (not isinstance(name, str)\n                or len(str(Card('TTYPE', name))) != CARD_LENGTH):\n            raise AssertionError(\n                'Column name must be a string able to fit in a single '\n                'FITS card--typically this means a maximum of 68 '\n                'characters, though it may be fewer if the string '\n                'contains special characters like quotes.')", "metadata": {"file_name": "astropy/io/fits/column.py", "File Name": "astropy/io/fits/column.py", "Classes": "Delayed, _BaseColumnFormat, _ColumnFormat, _AsciiColumnFormat, _FormatX, _FormatP, _FormatQ, ColumnAttribute, Column, ColDefs, _AsciiColDefs, _VLF", "Functions": "_get_index, _unwrapx, _wrapx, _makep, _parse_tformat, _parse_ascii_tformat, _parse_tdim, _scalar_to_format, _cmp_recformats, _convert_fits2record, _convert_record2fits, _dtype_to_recformat, _convert_ascii_format, convert_int"}}, {"code": "', AstropyUserWarning)\n            # Assume that the values are in GPS format\n            global_info['scale'] = 'tai'\n            global_info['format'] = 'gps'\n\n        if global_info['scale'] == 'local':\n            warnings.warn(\n                'Global time scale (TIMESYS) has a FITS recognized time scale '\n                'value \"LOCAL\". However, the standard states that \"LOCAL\" should be '\n                'tied to one of the existing scales because it is intrinsically '\n                'unreliable and/or ill-defined. Astropy will thus use the default '\n                'global time scale \"UTC\" instead of \"LOCAL\".', AstropyUserWarning)\n            # Default scale 'UTC'\n            global_info['scale'] = 'utc'\n            global_info['format'] = None\n\n        else:\n            raise AssertionError(\n                'Global time scale (TIMESYS) should have a FITS recognized '\n                'time scale value (got {!r}). The FITS standard states that '\n                'the use of local time scales should be restricted to alternate '\n                'coordinates.'.format(global_info['TIMESYS']))\n    else:\n        # Scale is already set\n        global_info['format'] = None\n\n    # Check if geocentric global location is specified\n    obs_geo = [global_info[attr] for attr in ('OBSGEO-X', 'OBSGEO-Y', 'OBSGEO-Z')\n               if attr in global_info]\n\n    # Location full specification is (X, Y, Z)\n    if len(obs_geo) == 3:\n        global_info['location'] = EarthLocation.from_geocentric(*obs_geo, unit=u.m)\n    else:\n        # Check if geodetic global location is specified (since geocentric failed)\n\n        # First warn the user if geocentric location is partially specified\n        if obs_geo:\n            warnings.warn(\n                'The geocentric observatory location {} is not completely '\n                'specified (X, Y, Z) and will be ignored.", "metadata": {"file_name": "astropy/io/fits/fitstime.py", "File Name": "astropy/io/fits/fitstime.py", "Functions": "is_time_column_keyword, _verify_global_info, _verify_column_info, _get_info_if_time_column, _convert_global_time, _convert_time_column, fits_to_time, time_to_fits"}}, {"code": "bscale, bzero : int, optional\n            user specified ``BSCALE`` and ``BZERO`` values.\n        \"\"\"\n\n        if self.data is None:\n            return\n\n        # Determine the destination (numpy) data type\n        if type is None:\n            type = BITPIX2DTYPE[self._bitpix]\n        _type = getattr(np, type)\n\n        # Determine how to scale the data\n        # bscale and bzero takes priority\n        if (bscale != 1 or bzero != 0):\n            _scale = bscale\n            _zero = bzero\n        else:\n            if option == 'old':\n                _scale = self._orig_bscale\n                _zero = self._orig_bzero\n            elif option == 'minmax':\n                if isinstance(_type, np.floating):\n                    _scale = 1\n                    _zero = 0\n                else:\n                    _min = np.minimum.reduce(self.data.flat)\n                    _max = np.maximum.reduce(self.data.flat)\n\n                    if _type == np.uint8:  # uint8 case\n                        _zero = _min\n                        _scale = (_max - _min) / (2. ** 8 - 1)\n                    else:\n                        _zero = (_max + _min) / 2.\n\n                        # throw away -2^N\n                        _scale = (_max - _min) / (2. ** (8 * _type.bytes) - 2)\n\n        # Do the scaling\n        if _zero != 0:\n            # We have to explicitly cast self._bzero to prevent numpy from\n            # raising an error when doing self.data -= _zero, and we\n            # do this instead of self.data = self.data - _zero to\n            # avoid doubling memory usage.\n            np.subtract(self.data, _zero, out=self.data, casting='unsafe')\n            self.header['BZERO'] = _zero\n        else:\n            # Delete from both headers\n            for header in (self.header, self._header):\n                with suppress(KeyError):\n                    del header['BZERO']\n\n        if _scale != 1:\n            self.data /= _scale\n            self.header['BSCALE'] = _scale\n        else:\n            for header in (self.header, self.", "metadata": {"file_name": "astropy/io/fits/hdu/compressed.py", "File Name": "astropy/io/fits/hdu/compressed.py", "Classes": "CompImageHeader, CompImageHDU"}}, {"code": "character_as_bytes : bool, optional\n        If `True`, string columns are stored as Numpy byte arrays (dtype ``S``)\n        and are converted on-the-fly to unicode strings when accessing\n        individual elements. If you need to use Numpy unicode arrays (dtype\n        ``U``) internally, you should set this to `False`, but note that this\n        will use more memory. If set to `False`, string columns will not be\n        memory-mapped even if ``memmap`` is `True`.\n    \"\"\"", "metadata": {"file_name": "astropy/io/fits/connect.py", "File Name": "astropy/io/fits/connect.py", "Functions": "is_column_keyword, is_fits, _decode_mixins, read_table_fits, _encode_mixins, write_table_fits"}}, {"code": "``EXTNAME`` values are\n        not case sensitive:\n\n            printdiff('inA.fits', 'inB.fits', 'sci')\n            printdiff('inA.fits', 'inB.fits', extname='sci')  # equivalent\n\n        By combination of ``EXTNAME`` and ``EXTVER`` as separate\n        arguments or as a tuple::\n\n            printdiff('inA.fits', 'inB.fits', 'sci', 2)    # EXTNAME='SCI'\n                                                           # & EXTVER=2\n            printdiff('inA.fits', 'inB.fits', extname='sci', extver=2)\n                                                           # equivalent\n            printdiff('inA.fits', 'inB.fits', ('sci', 2))  # equivalent\n\n        Ambiguous or conflicting specifications will raise an exception::\n\n            printdiff('inA.fits', 'inB.fits',\n                      ext=('sci', 1), extname='err', extver=2)\n\n    kwargs\n        Any additional keyword arguments to be passed to\n        `~astropy.io.fits.FITSDiff`.\n\n    Notes\n    -----\n    The primary use for the `printdiff` function is to allow quick print out\n    of a FITS difference report and will write to ``sys.stdout``.\n    To save the diff report to a file please use `~astropy.io.fits.FITSDiff`\n    directly.\n    \"\"\"", "metadata": {"file_name": "astropy/io/fits/convenience.py", "File Name": "astropy/io/fits/convenience.py", "Functions": "getheader, getdata, getval, setval, delval, writeto, table_to_hdu, append, update, info, printdiff, tabledump, tableload, _getext, _makehdu, _stat_filename_or_fileobj, _get_file_mode"}}, {"code": "errs = errlist\n        fix = None\n\n        try:\n            index = self._header.index(keyword)\n        except ValueError:\n            index = None\n\n        fixable = fix_value is not None\n\n        insert_pos = len(self._header) + 1\n\n        # If pos is an int, insert at the given position (and convert it to a\n        # lambda)\n        if _is_int(pos):\n            insert_pos = pos\n            pos = lambda x: x == insert_pos\n\n        # if the card does not exist\n        if index is None:\n            err_text = \"'{}' card does not exist.\".format(keyword)\n            fix_text = \"Fixed by inserting a new '{}' card.\".format(keyword)\n            if fixable:\n                # use repr to accommodate both string and non-string types\n                # Boolean is also OK in this constructor\n                card = (keyword, fix_value)\n\n                def fix(self=self, insert_pos=insert_pos, card=card):\n                    self._header.insert(insert_pos, card)\n\n            errs.append(self.run_option(option, err_text=err_text,\n                        fix_text=fix_text, fix=fix, fixable=fixable))\n        else:\n            # if the supposed location is specified\n            if pos is not None:\n                if not pos(index):\n                    err_text = (\"'{}' card at the wrong place \"\n                                \"(card {}).\".format(keyword, index))\n                    fix_text = (\"Fixed by moving it to the right place \"\n                                \"(card {}).\".format(insert_pos))\n\n                    def fix(self=self, index=index, insert_pos=insert_pos):\n                        card = self._header.cards[index]\n                        del self._header[index]\n                        self._header.insert(insert_pos, card)\n\n                    errs.append(self.run_option(option, err_text=err_text,\n                                fix_text=fix_text, fix=fix))\n\n            # if value checking is specified\n            if test:\n                val = self._header[keyword]\n                if not test(val):\n                    err_text = (\"'{}' card has invalid value '{}'.", "metadata": {"file_name": "astropy/io/fits/hdu/base.py", "File Name": "astropy/io/fits/hdu/base.py", "Classes": "_Delayed, InvalidHDUException, _BaseHDUMeta, _BaseHDU, _CorruptedHDU, _NonstandardHDU, _ValidHDU, ExtensionHDU, NonstandardExtHDU", "Functions": "_hdu_class_from_header, block_iter, fix, fix, fix, fix, fix"}}, {"code": "data is None or self.b.data is None:\n            # TODO: Perhaps have some means of marking this case\n            pass\n        elif self.a.is_image and self.b.is_image:\n            self.diff_data = ImageDataDiff.fromdiff(self, self.a.data,\n                                                    self.b.data)\n        elif (isinstance(self.a, _TableLikeHDU) and\n              isinstance(self.b, _TableLikeHDU)):\n            # TODO: Replace this if/when _BaseHDU grows a .is_table property\n            self.diff_data = TableDataDiff.fromdiff(self, self.a.data,\n                                                    self.b.data)\n        elif not self.diff_extension_types:\n            # Don't diff the data for unequal extension types that are not\n            # recognized image or table types\n            self.diff_data = RawDataDiff.fromdiff(self, self.a.data,\n                                                  self.b.data)\n\n    def _report(self):\n        if self.identical:\n            self._writeln(\" No differences found.\")", "metadata": {"file_name": "astropy/io/fits/diff.py", "File Name": "astropy/io/fits/diff.py", "Classes": "_BaseDiff, FITSDiff, HDUDiff, HeaderDiff, ImageDataDiff, RawDataDiff, TableDataDiff", "Functions": "diff_values, report_diff_values, report_diff_keyword_attr, where_not_allclose, get_header_values_comments"}}, {"code": "@ColumnAttribute('TCTYP')\n    def coord_type(col, coord_type):\n        if coord_type is None:\n            return\n\n        if (not isinstance(coord_type, str)\n                or len(coord_type) > 8):\n            raise AssertionError(\n                'Coordinate/axis type must be a string of atmost 8 '\n                'characters.')\n\n    @ColumnAttribute('TCUNI')\n    def coord_unit(col, coord_unit):\n        if (coord_unit is not None\n                and not isinstance(coord_unit, str)):\n            raise AssertionError(\n                'Coordinate/axis unit must be a string.')\n\n    @ColumnAttribute('TCRPX')\n    def coord_ref_point(col, coord_ref_point):\n        if (coord_ref_point is not None\n                and not isinstance(coord_ref_point, numbers.Real)):\n            raise AssertionError(\n                'Pixel coordinate of the reference point must be '\n                'real floating type.')\n\n    @ColumnAttribute('TCRVL')\n    def coord_ref_value(col, coord_ref_value):\n        if (coord_ref_value is not None\n                and not isinstance(coord_ref_value, numbers.Real)):\n            raise AssertionError(\n                'Coordinate value at reference point must be real '\n                'floating type.')\n\n    @ColumnAttribute('TCDLT')\n    def coord_inc(col, coord_inc):\n        if (coord_inc is not None\n                and not isinstance(coord_inc, numbers.Real)):\n            raise AssertionError(\n                'Coordinate increment must be real floating type.')\n\n    @ColumnAttribute('TRPOS')\n    def time_ref_pos(col, time_ref_pos):\n        if (time_ref_pos is not None\n                and not isinstance(time_ref_pos, str)):\n            raise AssertionError(\n                'Time reference position must be a string.')", "metadata": {"file_name": "astropy/io/fits/column.py", "File Name": "astropy/io/fits/column.py", "Classes": "Delayed, _BaseColumnFormat, _ColumnFormat, _AsciiColumnFormat, _FormatX, _FormatP, _FormatQ, ColumnAttribute, Column, ColDefs, _AsciiColDefs, _VLF", "Functions": "_get_index, _unwrapx, _wrapx, _makep, _parse_tformat, _parse_ascii_tformat, _parse_tdim, _scalar_to_format, _cmp_recformats, _convert_fits2record, _convert_record2fits, _dtype_to_recformat, _convert_ascii_format, convert_int"}}, {"code": "io_registry.register_reader('fits', Table, read_table_fits)\nio_registry.register_writer('fits', Table, write_table_fits)\nio_registry.register_identifier('fits', Table, is_fits)", "metadata": {"file_name": "astropy/io/fits/connect.py", "File Name": "astropy/io/fits/connect.py", "Functions": "is_column_keyword, is_fits, _decode_mixins, read_table_fits, _encode_mixins, write_table_fits"}}, {"code": "Parameters\n    ----------\n    array\n        input object array\n\n    descr_output\n        output \"descriptor\" array of data type int32 (for P format arrays) or\n        int64 (for Q format arrays)--must be nrows long in its first dimension\n\n    format\n        the _FormatP object representing the format of the variable array\n\n    nrows : int, optional\n        number of rows to create in the column; defaults to the number of rows\n        in the input array\n    \"\"\"\n\n    # TODO: A great deal of this is redundant with FITS_rec._convert_p; see if\n    # we can merge the two somehow.\n\n    _offset = 0\n\n    if not nrows:\n        nrows = len(array)\n\n    data_output = _VLF([None] * nrows, dtype=format.dtype)\n\n    if format.dtype == 'a':\n        _nbytes = 1\n    else:\n        _nbytes = np.array([], dtype=format.dtype).itemsize\n\n    for idx in range(nrows):\n        if idx < len(array):\n            rowval = array[idx]\n        else:\n            if format.dtype == 'a':\n                rowval = ' ' * data_output.max\n            else:\n                rowval = [0] * data_output.max\n        if format.dtype == 'a':\n            data_output[idx] = chararray.array(encode_ascii(rowval),\n                                               itemsize=1)\n        else:\n            data_output[idx] = np.array(rowval, dtype=format.dtype)\n\n        descr_output[idx, 0] = len(data_output[idx])\n        descr_output[idx, 1] = _offset\n        _offset += len(data_output[idx]) * _nbytes\n\n    return data_output", "metadata": {"file_name": "astropy/io/fits/column.py", "File Name": "astropy/io/fits/column.py", "Classes": "Delayed, _BaseColumnFormat, _ColumnFormat, _AsciiColumnFormat, _FormatX, _FormatP, _FormatQ, ColumnAttribute, Column, ColDefs, _AsciiColDefs, _VLF", "Functions": "_get_index, _unwrapx, _wrapx, _makep, _parse_tformat, _parse_ascii_tformat, _parse_tdim, _scalar_to_format, _cmp_recformats, _convert_fits2record, _convert_record2fits, _dtype_to_recformat, _convert_ascii_format, convert_int"}}, {"code": "format, recformat = cls._guess_format(format, start, dim)\n        elif not ascii and not isinstance(format, _BaseColumnFormat):\n            format, recformat = cls._convert_format(format, _ColumnFormat)\n        elif ascii and not isinstance(format, _AsciiColumnFormat):\n            format, recformat = cls._convert_format(format,\n                                                    _AsciiColumnFormat)\n        else:\n            # The format is already acceptable and unambiguous\n            recformat = format.recformat\n\n        return format, recformat\n\n    @classmethod\n    def _guess_format(cls, format, start, dim):\n        if start and dim:\n            # This is impossible; this can't be a valid FITS column\n            raise ValueError(\n                'Columns cannot have both a start (TCOLn) and dim '\n                '(TDIMn) option, since the former is only applies to '\n                'ASCII tables, and the latter is only valid for binary '\n                'tables.')\n        elif start:\n            # Only ASCII table columns can have a 'start' option\n            guess_format = _AsciiColumnFormat\n        elif dim:\n            # Only binary tables can have a dim option\n            guess_format = _ColumnFormat\n        else:\n            # If the format is *technically* a valid binary column format\n            # (i.e. it has a valid format code followed by arbitrary\n            # \"optional\" codes), but it is also strictly a valid ASCII\n            # table format, then assume an ASCII table column was being\n            # requested (the more likely case, after all).\n            with suppress(VerifyError):\n                format = _AsciiColumnFormat(format, strict=True)\n\n            # A safe guess which reflects the existing behavior of previous\n            # Astropy versions\n            guess_format = _ColumnFormat\n\n        try:\n            format,", "metadata": {"file_name": "astropy/io/fits/column.py", "File Name": "astropy/io/fits/column.py", "Classes": "Delayed, _BaseColumnFormat, _ColumnFormat, _AsciiColumnFormat, _FormatX, _FormatP, _FormatQ, ColumnAttribute, Column, ColDefs, _AsciiColDefs, _VLF", "Functions": "_get_index, _unwrapx, _wrapx, _makep, _parse_tformat, _parse_ascii_tformat, _parse_tdim, _scalar_to_format, _cmp_recformats, _convert_fits2record, _convert_record2fits, _dtype_to_recformat, _convert_ascii_format, convert_int"}}], "traj": {"usage": {"embedding_tokens": 0}}}
{"instance_id": "astropy__astropy-14365", "found_files": ["astropy/io/ascii/qdp.py", "astropy/io/ascii/ui.py", "astropy/io/ascii/core.py", "astropy/io/ascii/__init__.py", "astropy/io/ascii/docs.py", "astropy/table/table.py", "astropy/table/scripts/showtable.py", "astropy/table/pprint.py", "astropy/table/__init__.py", "astropy/io/ascii/cds.py", "astropy/table/pandas.py", "astropy/io/ascii/connect.py", "astropy/io/ascii/basic.py", "astropy/io/ascii/rst.py", "astropy/io/ascii/ipac.py", "astropy/io/ascii/daophot.py", "astropy/table/connect.py", "astropy/io/ascii/fixedwidth.py", "astropy/table/meta.py", "astropy/table/scripts/__init__.py"], "node_info": [{"code": "When writing tables to this format, users can pass an ``err_specs`` keyword\n    passing a dictionary ``{'serr': [3], 'terr': [1, 2]}``, meaning that data\n    columns 1 and two will have two additional columns each with their positive\n    and negative errors, and data column 3 will have an additional column with\n    a symmetric error (just like the ``READ SERR`` and ``READ TERR`` commands\n    above)\n\n    Headers are just comments, and tables distributed by various missions\n    can differ greatly in their use of conventions. For example, light curves\n    distributed by the Swift-Gehrels mission have an extra space in one header\n    entry that makes the number of labels inconsistent with the number of cols.\n    For this reason, we ignore the comments that might encode the column names\n    and leave the name specification to the user.\n\n    Example::\n\n        >               Extra space\n        >                   |\n        >                   v\n        >!     MJD       Err (pos)       Err(neg)        Rate            Error\n        >53000.123456   2.378e-05     -2.378472e-05     NO             0.212439\n\n    These readers and writer classes will strive to understand which of the\n    comments belong to all the tables, and which ones to each single table.\n    General comments will be stored in the ``initial_comments`` meta of each\n    table. The comments of each table will be stored in the ``comments`` meta.\n\n    Example::\n\n        t = Table.read(example_qdp, format='ascii.qdp', table_id=1, names=['a', 'b', 'c', 'd'])\n\n    reads the second table (``table_id=1``) in file ``example.qdp`` containing\n    the table above. There are four column names but seven data columns, why?\n    Because the ``READ SERR`` and ``READ TERR`` commands say that there are\n    three error columns.", "metadata": {"file_name": "astropy/io/ascii/qdp.py", "File Name": "astropy/io/ascii/qdp.py", "Classes": "QDPSplitter, QDPHeader, QDPData, QDP", "Functions": "_line_type, _get_type_from_list_of_lines, _get_lines_from_file, _interpret_err_lines, _get_tables_from_qdp_file, _understand_err_col, _read_table_qdp, _write_table_qdp"}}, {"code": "Table 1 comment\n        !a a(pos) a(neg) b be c d\n        54000.5   2.25  -2.5   NO  3.5  5.5 5\n        55000.5   3.25  -3.5   4  4.5  6.5 nan\n\n    The input table above contains some initial comments, the error commands,\n    then two tables.\n    This file format can contain multiple tables, separated by a line full\n    of ``NO``s. Comments are exclamation marks, and missing values are single\n    ``NO`` entries. The delimiter is usually whitespace, more rarely a comma.\n    The QDP format differentiates between data and error columns. The table\n    above has commands::\n\n        READ TERR 1\n        READ SERR 3\n\n    which mean that after data column 1 there will be two error columns\n    containing its positive and engative error bars, then data column 2 without\n    error bars, then column 3, then a column with the symmetric error of column\n    3, then the remaining data columns.\n\n    As explained below, table headers are highly inconsistent. Possible\n    comments containing column names will be ignored and columns will be called\n    ``col1``, ``col2``, etc. unless the user specifies their names with the\n    ``names=`` keyword argument,\n    When passing column names, pass **only the names of the data columns, not\n    the error columns.**\n    Error information will be encoded in the names of the table columns.\n    (e.g. ``a_perr`` and ``a_nerr`` for the positive and negative error of\n    column ``a``, ``b_err`` the symmetric error of column ``b``.)", "metadata": {"file_name": "astropy/io/ascii/qdp.py", "File Name": "astropy/io/ascii/qdp.py", "Classes": "QDPSplitter, QDPHeader, QDPData, QDP", "Functions": "_line_type, _get_type_from_list_of_lines, _get_lines_from_file, _interpret_err_lines, _get_tables_from_qdp_file, _understand_err_col, _read_table_qdp, _write_table_qdp"}}, {"code": "Parameters\n    ----------\n    table : :class:`~astropy.table.Table`\n        Input table to be written\n    filename : str\n        Output QDP file name\n\n    Other Parameters\n    ----------------\n    err_specs : dict\n        Dictionary of the format {'serr': [1], 'terr': [2, 3]}, specifying\n        which columns have symmetric and two-sided errors (see QDP format\n        specification)\n    \"\"\"\n    import io\n\n    fobj = io.StringIO()\n\n    if \"initial_comments\" in table.meta and table.meta[\"initial_comments\"] != []:\n        for line in table.meta[\"initial_comments\"]:\n            line = line.strip()\n            if not line.startswith(\"!\"):\n                line = \"!\" + line\n            print(line, file=fobj)\n\n    if err_specs is None:\n        serr_cols, terr_cols = _understand_err_col(table.colnames)\n    else:\n        serr_cols = err_specs.pop(\"serr\", [])\n        terr_cols = err_specs.pop(\"terr\", [])\n    if serr_cols != []:\n        col_string = \" \".join([str(val) for val in serr_cols])\n        print(f\"READ SERR {col_string}\", file=fobj)\n    if terr_cols != []:\n        col_string = \" \".join([str(val) for val in terr_cols])\n        print(f\"READ TERR {col_string}\", file=fobj)\n\n    if \"comments\" in table.meta and table.meta[\"comments\"] != []:\n        for line in table.meta[\"comments\"]:\n            line = line.strip()\n            if not line.startswith(\"!\"):\n                line = \"!\" + line\n            print(line, file=fobj)\n\n    colnames = table.colnames\n    print(\"!\"", "metadata": {"file_name": "astropy/io/ascii/qdp.py", "File Name": "astropy/io/ascii/qdp.py", "Classes": "QDPSplitter, QDPHeader, QDPData, QDP", "Functions": "_line_type, _get_type_from_list_of_lines, _get_lines_from_file, _interpret_err_lines, _get_tables_from_qdp_file, _understand_err_col, _read_table_qdp, _write_table_qdp"}}, {"code": "if len(command) < 3:\n                        continue\n                    err_specs[command[1].lower()] = [int(c) for c in command[2:]]\n            if colnames is None:\n                colnames = _interpret_err_lines(err_specs, ncol, names=input_colnames)\n\n            if current_rows is None:\n                current_rows = []\n\n            values = []\n            for v in line.split(delimiter):\n                if v == \"NO\":\n                    values.append(np.ma.masked)\n                else:\n                    # Understand if number is int or float\n                    try:\n                        values.append(int(v))\n                    except ValueError:\n                        values.append(float(v))\n            current_rows.append(values)\n            continue\n\n        if datatype == \"new\":\n            # Save table to table_list and reset\n            if current_rows is not None:\n                new_table = Table(names=colnames, rows=current_rows)\n                new_table.meta[\"initial_comments\"] = initial_comments.strip().split(\n                    \"\\n\"\n                )\n                new_table.meta[\"comments\"] = comment_text.strip().split(\"\\n\")\n                # Reset comments\n                comment_text = \"\"\n                table_list.append(new_table)\n                current_rows = None\n            continue\n\n    # At the very end, if there is still a table being written, let's save\n    # it to the table_list\n    if current_rows is not None:\n        new_table = Table(names=colnames, rows=current_rows)\n        new_table.meta[\"initial_comments\"] = initial_comments.strip().split(\"\\n\")\n        new_table.meta[\"comments\"] = comment_text.strip().split(\"\\n\")\n        table_list.append(new_table)\n\n    return table_list", "metadata": {"file_name": "astropy/io/ascii/qdp.py", "File Name": "astropy/io/ascii/qdp.py", "Classes": "QDPSplitter, QDPHeader, QDPData, QDP", "Functions": "_line_type, _get_type_from_list_of_lines, _get_lines_from_file, _interpret_err_lines, _get_tables_from_qdp_file, _understand_err_col, _read_table_qdp, _write_table_qdp"}}, {"code": "a comment to disturb\")\n    'data,1'\n    >>> _line_type(\"NO NO NO NO NO\")\n    'new'\n    >>> _line_type(\"NO,NO,NO,NO,NO\", delimiter=',')\n    'new'\n    >>> _line_type(\"N O N NOON OON O\")\n    Traceback (most recent call last):\n        ...\n    ValueError: Unrecognized QDP line...\n    >>> _line_type(\" some non-comment gibberish\")\n    Traceback (most recent call last):\n        ...\n    ValueError: Unrecognized QDP line...\n    \"\"\"\n    _decimal_re = r\"[+-]?(\\d+(\\.\\d*)?|\\.\\d+)([eE][+-]?\\d+)?\"\n    _command_re = r\"READ [TS]ERR(\\s+[0-9]+)+\"\n\n    sep = delimiter\n    if delimiter is None:\n        sep = r\"\\s+\"\n    _new_re = rf\"NO({sep}NO)+\"\n    _data_re = rf\"({_decimal_re}|NO|[-+]?nan)({sep}({_decimal_re}|NO|[-+]?nan))*)\"\n    _type_re = rf\"^\\s*((?P<command>{_command_re})|(?P<new>{_new_re})|(?P<data>{_data_re})?\\s*(\\!(?P<comment>.*))?\\s*$\"\n    _line_type_re = re.compile(_type_re)\n    line = line.strip()\n    if not line:\n        return \"comment\"\n    match = _line_type_re.match(line)\n\n    if match is None:\n        raise ValueError(f\"Unrecognized QDP line: {line}\")\n    for type_, val in match.groupdict().items():\n        if val is None:\n            continue\n        if type_ == \"data\":\n            return f\"data,{len(val.split(sep=delimiter))}\"\n        else:\n            return type_", "metadata": {"file_name": "astropy/io/ascii/qdp.py", "File Name": "astropy/io/ascii/qdp.py", "Classes": "QDPSplitter, QDPHeader, QDPData, QDP", "Functions": "_line_type, _get_type_from_list_of_lines, _get_lines_from_file, _interpret_err_lines, _get_tables_from_qdp_file, _understand_err_col, _read_table_qdp, _write_table_qdp"}}, {"code": "Returns\n    -------\n    list of `~astropy.table.Table`\n        List containing all the tables present inside the QDP file\n    \"\"\"\n    lines = _get_lines_from_file(qdp_file)\n    contents, ncol = _get_type_from_list_of_lines(lines, delimiter=delimiter)\n\n    table_list = []\n    err_specs = {}\n    colnames = None\n\n    comment_text = \"\"\n    initial_comments = \"\"\n    command_lines = \"\"\n    current_rows = None\n\n    for line, datatype in zip(lines, contents):\n        line = line.strip().lstrip(\"!\")\n        # Is this a comment?\n        if datatype == \"comment\":\n            comment_text += line + \"\\n\"\n            continue\n\n        if datatype == \"command\":\n            # The first time I find commands, I save whatever comments into\n            # The initial comments.\n            if command_lines == \"\":\n                initial_comments = comment_text\n                comment_text = \"\"\n\n            if err_specs != {}:\n                warnings.warn(\n                    \"This file contains multiple command blocks. Please verify\",\n                    AstropyUserWarning,\n                )\n            command_lines += line + \"\\n\"\n            continue\n\n        if datatype.startswith(\"data\"):\n            # The first time I find data, I define err_specs\n            if err_specs == {} and command_lines != \"\":\n                for cline in command_lines.strip().split(\"\\n\"):\n                    command = cline.strip().split()\n                    # This should never happen, but just in case.", "metadata": {"file_name": "astropy/io/ascii/qdp.py", "File Name": "astropy/io/ascii/qdp.py", "Classes": "QDPSplitter, QDPHeader, QDPData, QDP", "Functions": "_line_type, _get_type_from_list_of_lines, _get_lines_from_file, _interpret_err_lines, _get_tables_from_qdp_file, _understand_err_col, _read_table_qdp, _write_table_qdp"}}, {"code": "``t.meta['initial_comments']`` will contain the initial two comment lines\n    in the file, while ``t.meta['comments']`` will contain ``Table 1 comment``\n\n    The table can be written to another file, preserving the same information,\n    as::\n\n        t.write(test_file, err_specs={'terr': [1], 'serr': [3]})\n\n    Note how the ``terr`` and ``serr`` commands are passed to the writer.\n\n    \"\"\"\n\n    _format_name = \"qdp\"\n    _io_registry_can_write = True\n    _io_registry_suffix = \".qdp\"\n    _description = \"Quick and Dandy Plotter\"\n\n    header_class = QDPHeader\n    data_class = QDPData\n\n    def __init__(self, table_id=None, names=None, err_specs=None, sep=None):\n        super().__init__()\n        self.table_id = table_id\n        self.names = names\n        self.err_specs = err_specs\n        self.delimiter = sep\n\n    def read(self, table):\n        self.lines = self.inputter.get_lines(table, newline=\"\\n\")\n        return _read_table_qdp(\n            self.lines,\n            table_id=self.table_id,\n            names=self.names,\n            delimiter=self.delimiter,\n        )\n\n    def write(self, table):\n        self._check_multidim_table(table)\n        lines = _write_table_qdp(table, err_specs=self.err_specs)\n        return lines", "metadata": {"file_name": "astropy/io/ascii/qdp.py", "File Name": "astropy/io/ascii/qdp.py", "Classes": "QDPSplitter, QDPHeader, QDPData, QDP", "Functions": "_line_type, _get_type_from_list_of_lines, _get_lines_from_file, _interpret_err_lines, _get_tables_from_qdp_file, _understand_err_col, _read_table_qdp, _write_table_qdp"}}, {"code": "+ \" \".join(colnames), file=fobj)\n    for row in table:\n        values = []\n        for val in row:\n            if not np.ma.is_masked(val):\n                rep = str(val)\n            else:\n                rep = \"NO\"\n            values.append(rep)\n        print(\" \".join(values), file=fobj)\n\n    full_string = fobj.getvalue()\n    fobj.close()\n\n    if filename is not None:\n        with open(filename, \"w\") as fobj:\n            print(full_string, file=fobj)\n\n    return full_string.split(\"\\n\")\n\n\nclass QDPSplitter(core.DefaultSplitter):\n    \"\"\"\n    Split on space for QDP tables.\n    \"\"\"\n\n    delimiter = \" \"\n\n\nclass QDPHeader(basic.CommentedHeaderHeader):\n    \"\"\"\n    Header that uses the :class:`astropy.io.ascii.basic.QDPSplitter`.\n    \"\"\"\n\n    splitter_class = QDPSplitter\n    comment = \"!\"\n    write_comment = \"!\"\n\n\nclass QDPData(basic.BasicData):\n    \"\"\"\n    Data that uses the :class:`astropy.io.ascii.basic.CsvSplitter`.\n    \"\"\"\n\n    splitter_class = QDPSplitter\n    fill_values = [(core.masked, \"NO\")]\n    comment = \"!\"\n    write_comment = None\n\n\nclass QDP(basic.Basic):\n    \"\"\"Quick and Dandy Plot table.\n\n    Example::\n\n        ! Initial comment line 1\n        ! Initial comment line 2\n        READ TERR 1\n        READ SERR 3\n        ! Table 0 comment\n        !a a(pos) a(neg) b be c d\n        53000.5   0.25  -0.5   1  1.5  3.5 2\n        54000.5   1.25  -1.5   2  2.5  4.5 3\n        NO NO NO NO NO\n        !", "metadata": {"file_name": "astropy/io/ascii/qdp.py", "File Name": "astropy/io/ascii/qdp.py", "Classes": "QDPSplitter, QDPHeader, QDPData, QDP", "Functions": "_line_type, _get_type_from_list_of_lines, _get_lines_from_file, _interpret_err_lines, _get_tables_from_qdp_file, _understand_err_col, _read_table_qdp, _write_table_qdp"}}, {"code": "def _read_table_qdp(qdp_file, names=None, table_id=None, delimiter=None):\n    \"\"\"Read a table from a QDP file.\n\n    Parameters\n    ----------\n    qdp_file : str\n        Input QDP file name\n\n    Other Parameters\n    ----------------\n    names : list of str\n        Name of data columns (defaults to ['col1', 'col2', ...]), _not_\n        including error columns.\n\n    table_id : int, default 0\n        Number of the table to be read from the QDP file. This is useful\n        when multiple tables present in the file. By default, the first is read.\n\n    delimiter : str\n        Any delimiter accepted by the `sep` argument of str.split()\n\n    Returns\n    -------\n    tables : list of `~astropy.table.Table`\n        List containing all the tables present inside the QDP file\n    \"\"\"\n    if table_id is None:\n        warnings.warn(\n            \"table_id not specified. Reading the first available table\",\n            AstropyUserWarning,\n        )\n        table_id = 0\n\n    tables = _get_tables_from_qdp_file(\n        qdp_file, input_colnames=names, delimiter=delimiter\n    )\n\n    return tables[table_id]\n\n\ndef _write_table_qdp(table, filename=None, err_specs=None):\n    \"\"\"Write a table to a QDP file.", "metadata": {"file_name": "astropy/io/ascii/qdp.py", "File Name": "astropy/io/ascii/qdp.py", "Classes": "QDPSplitter, QDPHeader, QDPData, QDP", "Functions": "_line_type, _get_type_from_list_of_lines, _get_lines_from_file, _interpret_err_lines, _get_tables_from_qdp_file, _understand_err_col, _read_table_qdp, _write_table_qdp"}}, {"code": "# Licensed under a 3-clause BSD style license - see LICENSE.rst\n\"\"\"\nThis package contains functions for reading and writing QDP tables that are\nnot meant to be used directly, but instead are available as readers/writers in\n`astropy.table`. See :ref:`astropy:table_io` for more details.\n\"\"\"\nimport copy\nimport re\nimport warnings\nfrom collections.abc import Iterable\n\nimport numpy as np\n\nfrom astropy.table import Table\nfrom astropy.utils.exceptions import AstropyUserWarning\n\nfrom . import basic, core\n\n\ndef _line_type(line, delimiter=None):\n    \"\"\"Interpret a QDP file line.\n\n    Parameters\n    ----------\n    line : str\n        a single line of the file\n\n    Returns\n    -------\n    type : str\n        Line type: \"comment\", \"command\", or \"data\"\n\n    Examples\n    --------\n    >>> _line_type(\"READ SERR 3\")\n    'command'\n    >>> _line_type(\" \\\\n    !some gibberish\")\n    'comment'\n    >>> _line_type(\"   \")\n    'comment'\n    >>> _line_type(\" 21345.45\")\n    'data,1'\n    >>> _line_type(\" 21345.45 1.53e-3 1e-3 .04 NO nan\")\n    'data,6'\n    >>> _line_type(\" 21345.45,1.53e-3,1e-3,.04,NO,nan\", delimiter=',')\n    'data,6'\n    >>> _line_type(\" 21345.45 !", "metadata": {"file_name": "astropy/io/ascii/qdp.py", "File Name": "astropy/io/ascii/qdp.py", "Classes": "QDPSplitter, QDPHeader, QDPData, QDP", "Functions": "_line_type, _get_type_from_list_of_lines, _get_lines_from_file, _interpret_err_lines, _get_tables_from_qdp_file, _understand_err_col, _read_table_qdp, _write_table_qdp"}}, {"code": "pop(\"serr\", [])\n        terr_cols = err_specs.pop(\"terr\", [])\n\n    if names is not None:\n        all_error_cols = len(serr_cols) + len(terr_cols) * 2\n        if all_error_cols + len(names) != ncols:\n            raise ValueError(\"Inconsistent number of input colnames\")\n\n    shift = 0\n    for i in range(ncols):\n        col_num = i + 1 - shift\n        if colnames[i] != \"\":\n            continue\n\n        colname_root = f\"col{col_num}\"\n\n        if names is not None:\n            colname_root = names[col_num - 1]\n\n        colnames[i] = f\"{colname_root}\"\n        if col_num in serr_cols:\n            colnames[i + 1] = f\"{colname_root}_err\"\n            shift += 1\n            continue\n\n        if col_num in terr_cols:\n            colnames[i + 1] = f\"{colname_root}_perr\"\n            colnames[i + 2] = f\"{colname_root}_nerr\"\n            shift += 2\n            continue\n\n    assert not np.any([c == \"\" for c in colnames])\n\n    return colnames\n\n\ndef _get_tables_from_qdp_file(qdp_file, input_colnames=None, delimiter=None):\n    \"\"\"Get all tables from a QDP file.\n\n    Parameters\n    ----------\n    qdp_file : str\n        Input QDP file name\n\n    Other Parameters\n    ----------------\n    input_colnames : list of str\n        Name of data columns (defaults to ['col1', 'col2', ...]), _not_\n        including error columns.\n    delimiter : str\n        Delimiter for the values in the table.", "metadata": {"file_name": "astropy/io/ascii/qdp.py", "File Name": "astropy/io/ascii/qdp.py", "Classes": "QDPSplitter, QDPHeader, QDPData, QDP", "Functions": "_line_type, _get_type_from_list_of_lines, _get_lines_from_file, _interpret_err_lines, _get_tables_from_qdp_file, _understand_err_col, _read_table_qdp, _write_table_qdp"}}, {"code": "def _interpret_err_lines(err_specs, ncols, names=None):\n    \"\"\"Give list of column names from the READ SERR and TERR commands.\n\n    Parameters\n    ----------\n    err_specs : dict\n        ``{'serr': [n0, n1, ...], 'terr': [n2, n3, ...]}``\n        Error specifications for symmetric and two-sided errors\n    ncols : int\n        Number of data columns\n\n    Other Parameters\n    ----------------\n    names : list of str\n        Name of data columns (defaults to ['col1', 'col2', ...]), _not_\n        including error columns.\n\n    Returns\n    -------\n    colnames : list\n        List containing the column names. Error columns will have the name\n        of the main column plus ``_err`` for symmetric errors, and ``_perr``\n        and ``_nerr`` for positive and negative errors respectively\n\n    Examples\n    --------\n    >>> col_in = ['MJD', 'Rate']\n    >>> cols = _interpret_err_lines(None, 2, names=col_in)\n    >>> cols[0]\n    'MJD'\n    >>> err_specs = {'terr': [1], 'serr': [2]}\n    >>> ncols = 5\n    >>> cols = _interpret_err_lines(err_specs, ncols, names=col_in)\n    >>> cols[0]\n    'MJD'\n    >>> cols[2]\n    'MJD_nerr'\n    >>> cols[4]\n    'Rate_err'\n    >>> _interpret_err_lines(err_specs, 6, names=col_in)\n    Traceback (most recent call last):\n        ...\n    ValueError: Inconsistent number of input colnames\n    \"\"\"\n    colnames = [\"\" for i in range(ncols)]\n    if err_specs is None:\n        serr_cols = terr_cols = []\n\n    else:\n        # I don't want to empty the original one when using `pop` below\n        err_specs = copy.deepcopy(err_specs)\n\n        serr_cols = err_specs.", "metadata": {"file_name": "astropy/io/ascii/qdp.py", "File Name": "astropy/io/ascii/qdp.py", "Classes": "QDPSplitter, QDPHeader, QDPData, QDP", "Functions": "_line_type, _get_type_from_list_of_lines, _get_lines_from_file, _interpret_err_lines, _get_tables_from_qdp_file, _understand_err_col, _read_table_qdp, _write_table_qdp"}}, {"code": "See #3132, #3013, #3109,\n        # #2001.  If a `readme` arg was passed that implies CDS format, in\n        # which case the original `table` as the data filename must be left\n        # intact.\n        if \"readme\" not in new_kwargs:\n            encoding = kwargs.get(\"encoding\")\n            try:\n                table = _expand_user_if_path(table)\n                with get_readable_fileobj(table, encoding=encoding) as fileobj:\n                    table = fileobj.read()\n            except ValueError:  # unreadable or invalid binary file\n                raise\n            except Exception:\n                pass\n            else:\n                # Ensure that `table` has at least one \\r or \\n in it\n                # so that the core.BaseInputter test of\n                # ('\\n' not in table and '\\r' not in table)\n                # will fail and so `table` cannot be interpreted there\n                # as a filename.  See #4160.\n                if not re.search(r\"[\\r\\n]\", table):\n                    table = table + os.linesep\n\n                # If the table got successfully read then look at the content\n                # to see if is probably HTML, but only if it wasn't already\n                # identified as HTML based on the filename.\n                if not new_kwargs[\"guess_html\"]:\n                    new_kwargs[\"guess_html\"] = _probably_html(table)\n\n        # Get the table from guess in ``dat``.  If ``dat`` comes back as None\n        # then there was just one set of kwargs in the guess list so fall\n        # through below to the non-guess way so that any problems result in a\n        # more useful traceback.", "metadata": {"file_name": "astropy/io/ascii/ui.py", "File Name": "astropy/io/ascii/ui.py", "Functions": "_probably_html, set_guess, get_reader, _get_format_class, _get_fast_reader_dict, _validate_read_write_kwargs, _expand_user_if_path, read, _guess, _get_guess_kwargs_list, _read_in_chunks, _read_in_chunks_generator, get_writer, write, get_read_trace, is_ducktype, passthrough_fileobj"}}, {"code": "def _get_type_from_list_of_lines(lines, delimiter=None):\n    \"\"\"Read through the list of QDP file lines and label each line by type.\n\n    Parameters\n    ----------\n    lines : list\n        List containing one file line in each entry\n\n    Returns\n    -------\n    contents : list\n        List containing the type for each line (see `line_type_and_data`)\n    ncol : int\n        The number of columns in the data lines. Must be the same throughout\n        the file\n\n    Examples\n    --------\n    >>> line0 = \"! A comment\"\n    >>> line1 = \"543 12 456.0\"\n    >>> lines = [line0, line1]\n    >>> types, ncol = _get_type_from_list_of_lines(lines)\n    >>> types[0]\n    'comment'\n    >>> types[1]\n    'data,3'\n    >>> ncol\n    3\n    >>> lines.append(\"23\")\n    >>> _get_type_from_list_of_lines(lines)\n    Traceback (most recent call last):\n        ...\n    ValueError: Inconsistent number of columns\n    \"\"\"\n    types = [_line_type(line, delimiter=delimiter) for line in lines]\n    current_ncol = None\n    for type_ in types:\n        if type_.startswith(\"data,\"):\n            ncol = int(type_[5:])\n            if current_ncol is None:\n                current_ncol = ncol\n            elif ncol != current_ncol:\n                raise ValueError(\"Inconsistent number of columns\")\n\n    return types, current_ncol\n\n\ndef _get_lines_from_file(qdp_file):\n    if \"\\n\" in qdp_file:\n        lines = qdp_file.split(\"\\n\")\n    elif isinstance(qdp_file, str):\n        with open(qdp_file) as fobj:\n            lines = [line.strip() for line in fobj.readlines()]\n    elif isinstance(qdp_file, Iterable):\n        lines = qdp_file\n    else:\n        raise ValueError(\"invalid value of qdb_file\")\n\n    return lines", "metadata": {"file_name": "astropy/io/ascii/qdp.py", "File Name": "astropy/io/ascii/qdp.py", "Classes": "QDPSplitter, QDPHeader, QDPData, QDP", "Functions": "_line_type, _get_type_from_list_of_lines, _get_lines_from_file, _interpret_err_lines, _get_tables_from_qdp_file, _understand_err_col, _read_table_qdp, _write_table_qdp"}}, {"code": "\"dt\": f\"{(time.time() - t0) * 1000:.3f} ms\",\n                }\n            )\n            failed_kwargs.append(guess_kwargs)\n    else:\n        # Failed all guesses, try the original read_kwargs without column requirements\n        try:\n            reader = get_reader(**read_kwargs)\n            dat = reader.read(table)\n            _read_trace.append(\n                {\n                    \"kwargs\": copy.deepcopy(read_kwargs),\n                    \"Reader\": reader.__class__,\n                    \"status\": (\n                        \"Success with original kwargs without strict_names (guessing)\"\n                    ),\n                }\n            )\n            return dat\n\n        except guess_exception_classes as err:\n            _read_trace.append(\n                {\n                    \"kwargs\": copy.deepcopy(read_kwargs),\n                    \"status\": f\"{err.__class__.__name__}: {str(err)}\",\n                }\n            )\n            failed_kwargs.append(read_kwargs)\n            lines = [\n                \"\\nERROR: Unable to guess table format with the guesses listed below:\"\n            ]\n            for kwargs in failed_kwargs:\n                sorted_keys = sorted(\n                    x for x in sorted(kwargs) if x not in (\"Reader\", \"Outputter\")\n                )\n                reader_repr = repr(kwargs.get(\"Reader\", basic.Basic))\n                keys_vals = [\"Reader:\" + re.search(r\"\\.(\\w+)'>\", reader_repr).group(1)]\n                kwargs_sorted = ((key, kwargs[key]) for key in sorted_keys)\n                keys_vals.extend([f\"{key}: {val!r}\" for key, val in kwargs_sorted])\n                lines.append(\" \".join(keys_vals))\n\n            msg = [\n                \"\",\n                \"************************************************************************\",\n                \"** ERROR: Unable to guess table format with the guesses listed above. **\",\n                \"**                                                                    **\",\n                \"** To figure out why the table did not read, use guess=False and      **\",\n                \"** fast_reader=False, along with any appropriate arguments to read().", "metadata": {"file_name": "astropy/io/ascii/ui.py", "File Name": "astropy/io/ascii/ui.py", "Functions": "_probably_html, set_guess, get_reader, _get_format_class, _get_fast_reader_dict, _validate_read_write_kwargs, _expand_user_if_path, read, _guess, _get_guess_kwargs_list, _read_in_chunks, _read_in_chunks_generator, get_writer, write, get_read_trace, is_ducktype, passthrough_fileobj"}}, {"code": "# Licensed under a 3-clause BSD style license - see LICENSE.rst\n\"\"\"An extensible ASCII table reader and writer.\n\nui.py:\n  Provides the main user functions for reading and writing tables.\n\n:Copyright: Smithsonian Astrophysical Observatory (2010)\n:Author: Tom Aldcroft (aldcroft@head.cfa.harvard.edu)\n\"\"\"\n\n\nimport collections\nimport contextlib\nimport copy\nimport os\nimport re\nimport sys\nimport time\nimport warnings\nfrom io import StringIO\n\nimport numpy as np\n\nfrom astropy.table import Table\nfrom astropy.utils.data import get_readable_fileobj\nfrom astropy.utils.exceptions import AstropyWarning\nfrom astropy.utils.misc import NOT_OVERWRITING_MSG\n\nfrom . import (\n    basic,\n    cds,\n    core,\n    cparser,\n    daophot,\n    ecsv,\n    fastbasic,\n    fixedwidth,\n    html,\n    ipac,\n    latex,\n    mrt,\n    rst,\n    sextractor,\n)\nfrom .docs import READ_KWARG_TYPES, WRITE_KWARG_TYPES\n\n_read_trace = []\n\n# Default setting for guess parameter in read()\n_GUESS = True", "metadata": {"file_name": "astropy/io/ascii/ui.py", "File Name": "astropy/io/ascii/ui.py", "Functions": "_probably_html, set_guess, get_reader, _get_format_class, _get_fast_reader_dict, _validate_read_write_kwargs, _expand_user_if_path, read, _guess, _get_guess_kwargs_list, _read_in_chunks, _read_in_chunks_generator, get_writer, write, get_read_trace, is_ducktype, passthrough_fileobj"}}, {"code": "names = kwargs.get(\"names\")\n    if isinstance(table, Table):\n        # While we are only going to read data from columns, we may need to\n        # to adjust info attributes such as format, so we make a shallow copy.\n        table = table.__class__(table, names=names, copy=False)\n    else:\n        # Otherwise, create a table from the input.\n        table = Table(table, names=names, copy=False)\n\n    table0 = table[:0].copy()\n    core._apply_include_exclude_names(\n        table0,\n        kwargs.get(\"names\"),\n        kwargs.get(\"include_names\"),\n        kwargs.get(\"exclude_names\"),\n    )\n    diff_format_with_names = set(kwargs.get(\"formats\", [])) - set(table0.colnames)\n\n    if diff_format_with_names:\n        warnings.warn(\n            \"The key(s) {} specified in the formats argument do not match a column\"\n            \" name.\".format(diff_format_with_names),\n            AstropyWarning,\n        )\n\n    if table.has_mixin_columns:\n        fast_writer = False\n\n    Writer = _get_format_class(format, Writer, \"Writer\")\n    writer = get_writer(Writer=Writer, fast_writer=fast_writer, **kwargs)\n    if writer._format_name in core.FAST_CLASSES:\n        writer.write(table, output)\n        return\n\n    lines = writer.write(table)\n\n    # Write the lines to output\n    outstr = os.linesep.join(lines)\n    if not hasattr(output, \"write\"):\n        # NOTE: we need to specify newline='', otherwise the default\n        # behavior is for Python to translate \\r\\n (which we write because\n        # of os.linesep) into \\r\\r\\n. Specifying newline='' disables any\n        # auto-translation.\n        output = open(output, \"w\", newline=\"\")\n        output.write(outstr)\n        output.write(os.linesep)\n        output.close()\n    else:\n        output.write(outstr)\n        output.write(os.linesep)\n\n\nwrite.__doc__ = core.WRITE_DOCSTRING", "metadata": {"file_name": "astropy/io/ascii/ui.py", "File Name": "astropy/io/ascii/ui.py", "Functions": "_probably_html, set_guess, get_reader, _get_format_class, _get_fast_reader_dict, _validate_read_write_kwargs, _expand_user_if_path, read, _guess, _get_guess_kwargs_list, _read_in_chunks, _read_in_chunks_generator, get_writer, write, get_read_trace, is_ducktype, passthrough_fileobj"}}, {"code": "# Licensed under a 3-clause BSD style license - see LICENSE.rst\n\"\"\"An extensible ASCII table reader and writer.\n\ncore.py:\n  Core base classes and functions for reading and writing tables.\n\n:Copyright: Smithsonian Astrophysical Observatory (2010)\n:Author: Tom Aldcroft (aldcroft@head.cfa.harvard.edu)\n\"\"\"\n\n\nimport copy\nimport csv\nimport fnmatch\nimport functools\nimport inspect\nimport itertools\nimport operator\nimport os\nimport re\nimport warnings\nfrom collections import OrderedDict\nfrom contextlib import suppress\nfrom io import StringIO\n\nimport numpy\n\nfrom astropy.table import Table\nfrom astropy.utils.data import get_readable_fileobj\nfrom astropy.utils.exceptions import AstropyWarning\n\nfrom . import connect\nfrom .docs import READ_DOCSTRING, WRITE_DOCSTRING\n\n# Global dictionary mapping format arg to the corresponding Reader class\nFORMAT_CLASSES = {}\n\n# Similar dictionary for fast readers\nFAST_CLASSES = {}", "metadata": {"file_name": "astropy/io/ascii/core.py", "File Name": "astropy/io/ascii/core.py", "Classes": "CsvWriter, MaskedConstant, InconsistentTableError, OptionalTableImportError, ParameterError, FastOptionsError, NoType, StrType, NumType, FloatType, BoolType, IntType, AllType, Column, BaseInputter, BaseSplitter, DefaultSplitter, BaseHeader, BaseData, BaseOutputter, TableOutputter, MetaBaseReader, BaseReader, ContinuationLinesInputter, WhitespaceSplitter", "Functions": "_check_multidim_table, _replace_tab_with_space, _get_line_index, convert_numpy, _deduplicate_names, _is_number, _apply_include_exclude_names, _get_reader, _get_writer, bool_converter, generic_converter, rename_columns"}}, {"code": "def _understand_err_col(colnames):\n    \"\"\"Get which column names are error columns.\n\n    Examples\n    --------\n    >>> colnames = ['a', 'a_err', 'b', 'b_perr', 'b_nerr']\n    >>> serr, terr = _understand_err_col(colnames)\n    >>> np.allclose(serr, [1])\n    True\n    >>> np.allclose(terr, [2])\n    True\n    >>> serr, terr = _understand_err_col(['a', 'a_nerr'])\n    Traceback (most recent call last):\n    ...\n    ValueError: Missing positive error...\n    >>> serr, terr = _understand_err_col(['a', 'a_perr'])\n    Traceback (most recent call last):\n    ...\n    ValueError: Missing negative error...\n    \"\"\"\n    shift = 0\n    serr = []\n    terr = []\n\n    for i, col in enumerate(colnames):\n        if col.endswith(\"_err\"):\n            # The previous column, but they're numbered from 1!\n            # Plus, take shift into account\n            serr.append(i - shift)\n            shift += 1\n        elif col.endswith(\"_perr\"):\n            terr.append(i - shift)\n            if len(colnames) == i + 1 or not colnames[i + 1].endswith(\"_nerr\"):\n                raise ValueError(\"Missing negative error\")\n            shift += 2\n        elif col.endswith(\"_nerr\") and not colnames[i - 1].endswith(\"_perr\"):\n            raise ValueError(\"Missing positive error\")\n    return serr, terr", "metadata": {"file_name": "astropy/io/ascii/qdp.py", "File Name": "astropy/io/ascii/qdp.py", "Classes": "QDPSplitter, QDPHeader, QDPData, QDP", "Functions": "_line_type, _get_type_from_list_of_lines, _get_lines_from_file, _interpret_err_lines, _get_tables_from_qdp_file, _understand_err_col, _read_table_qdp, _write_table_qdp"}}, {"code": "if \"delimiter\" in kwargs:\n        if kwargs[\"delimiter\"] in (\"\\n\", \"\\r\", \"\\r\\n\"):\n            reader.header.splitter = BaseSplitter()\n            reader.data.splitter = BaseSplitter()\n        reader.header.splitter.delimiter = kwargs[\"delimiter\"]\n        reader.data.splitter.delimiter = kwargs[\"delimiter\"]\n    if \"comment\" in kwargs:\n        reader.header.comment = kwargs[\"comment\"]\n        reader.data.comment = kwargs[\"comment\"]\n    if \"quotechar\" in kwargs:\n        reader.header.splitter.quotechar = kwargs[\"quotechar\"]\n        reader.data.splitter.quotechar = kwargs[\"quotechar\"]\n    if \"data_start\" in kwargs:\n        reader.data.start_line = kwargs[\"data_start\"]\n    if \"data_end\" in kwargs:\n        reader.data.end_line = kwargs[\"data_end\"]\n    if \"header_start\" in kwargs:\n        if reader.header.start_line is not None:\n            reader.header.start_line = kwargs[\"header_start\"]\n            # For FixedWidthTwoLine the data_start is calculated relative to the position line.\n            # However, position_line is given as absolute number and not relative to header_start.\n            # So, ignore this Reader here.", "metadata": {"file_name": "astropy/io/ascii/core.py", "File Name": "astropy/io/ascii/core.py", "Classes": "CsvWriter, MaskedConstant, InconsistentTableError, OptionalTableImportError, ParameterError, FastOptionsError, NoType, StrType, NumType, FloatType, BoolType, IntType, AllType, Column, BaseInputter, BaseSplitter, DefaultSplitter, BaseHeader, BaseData, BaseOutputter, TableOutputter, MetaBaseReader, BaseReader, ContinuationLinesInputter, WhitespaceSplitter", "Functions": "_check_multidim_table, _replace_tab_with_space, _get_line_index, convert_numpy, _deduplicate_names, _is_number, _apply_include_exclude_names, _get_reader, _get_writer, bool_converter, generic_converter, rename_columns"}}, {"code": "# Licensed under a 3-clause BSD style license - see LICENSE.rst\n\"\"\" An extensible ASCII table reader and writer.\n\n\"\"\"\n# flake8: noqa\n\nfrom . import connect\nfrom .basic import (\n    Basic,\n    BasicData,\n    BasicHeader,\n    CommentedHeader,\n    Csv,\n    NoHeader,\n    Rdb,\n    Tab,\n)\nfrom .cds import Cds\nfrom .core import (\n    AllType,\n    BaseData,\n    BaseHeader,\n    BaseInputter,\n    BaseOutputter,\n    BaseReader,\n    BaseSplitter,\n    Column,\n    ContinuationLinesInputter,\n    DefaultSplitter,\n    FloatType,\n    InconsistentTableError,\n    IntType,\n    NoType,\n    NumType,\n    ParameterError,\n    StrType,\n    TableOutputter,\n    WhitespaceSplitter,\n    convert_numpy,\n    masked,\n)\nfrom .daophot import Daophot\nfrom .ecsv import Ecsv\nfrom .fastbasic import (\n    FastBasic,\n    FastCommentedHeader,\n    FastCsv,\n    FastNoHeader,\n    FastRdb,\n    FastTab,\n)\nfrom .fixedwidth import (\n    FixedWidth,\n    FixedWidthData,\n    FixedWidthHeader,\n    FixedWidthNoHeader,\n    FixedWidthSplitter,\n    FixedWidthTwoLine,\n)\nfrom .html import HTML\nfrom .ipac import Ipac\nfrom .latex import AASTex, Latex, latexdicts\nfrom .mrt import Mrt\nfrom .qdp import QDP\nfrom .rst import RST\nfrom .sextractor import SExtractor\nfrom .ui import get_read_trace, get_reader, get_writer, read, set_guess, write", "metadata": {"file_name": "astropy/io/ascii/__init__.py", "File Name": "astropy/io/ascii/__init__.py"}}, {"code": "int) or a\n# list of types.  These get used in io.ascii.ui._validate_read_write_kwargs().\n# -  The commented-out kwargs are too flexible for a useful check\n# -  'list-list' is a special case for an iterable that is not a string.\nREAD_KWARG_TYPES = {\n    # 'table'\n    \"guess\": bool,\n    # 'format'\n    # 'Reader'\n    # 'Inputter'\n    # 'Outputter'\n    \"delimiter\": str,\n    \"comment\": str,\n    \"quotechar\": str,\n    \"header_start\": int,\n    \"data_start\": (int, str),  # CDS allows 'guess'\n    \"data_end\": int,\n    \"converters\": dict,\n    # 'data_Splitter'\n    # 'header_Splitter'\n    \"names\": \"list-like\",\n    \"include_names\": \"list-like\",\n    \"exclude_names\": \"list-like\",\n    \"fill_values\": \"list-like\",\n    \"fill_include_names\": \"list-like\",\n    \"fill_exclude_names\": \"list-like\",\n    \"fast_reader\": (bool, str, dict),\n    \"encoding\": str,\n}\n\n\nWRITE_DOCSTRING = \"\"\"\n    Write the input ``table`` to ``filename``.  Most of the default behavior\n    for various parameters is determined by the Writer class.\n\n    See also:\n\n    - https://docs.astropy.org/en/stable/io/ascii/\n    - https://docs.astropy.org/en/stable/io/ascii/write.html\n\n    Parameters\n    ----------\n    table : `~astropy.io.ascii.BaseReader`, array-like, str, file-like, list\n        Input table as a Reader object, Numpy struct array, file name,\n        file-like object, list of strings, or single newline-separated string.\n    output : str, file-like\n        Output [filename, file-like object]. Defaults to``sys.stdout``.\n    format : str\n        Output table format. Defaults to 'basic'.\n    delimiter : str\n        Column delimiter string\n    comment : str, bool\n        String defining a comment line in table.  If `False` then comments\n        are not written out.", "metadata": {"file_name": "astropy/io/ascii/docs.py", "File Name": "astropy/io/ascii/docs.py"}}, {"code": "if (\n                (\"data_start\" not in kwargs)\n                and (default_header_length is not None)\n                and reader._format_name\n                not in [\"fixed_width_two_line\", \"commented_header\"]\n            ):\n                reader.data.start_line = (\n                    reader.header.start_line + default_header_length\n                )\n        elif kwargs[\"header_start\"] is not None:\n            # User trying to set a None header start to some value other than None\n            raise ValueError(\"header_start cannot be modified for this Reader\")\n    if \"converters\" in kwargs:\n        reader.outputter.converters = kwargs[\"converters\"]\n    if \"data_Splitter\" in kwargs:\n        reader.data.splitter = kwargs[\"data_Splitter\"]()\n    if \"header_Splitter\" in kwargs:\n        reader.header.splitter = kwargs[\"header_Splitter\"]()\n    if \"names\" in kwargs:\n        reader.names = kwargs[\"names\"]\n        if None in reader.names:\n            raise TypeError(\"Cannot have None for column name\")\n        if len(set(reader.names)) != len(reader.names):\n            raise ValueError(\"Duplicate column names\")\n    if \"include_names\" in kwargs:\n        reader.include_names = kwargs[\"include_names\"]\n    if \"exclude_names\" in kwargs:\n        reader.exclude_names = kwargs[\"exclude_names\"]\n    # Strict names is normally set only within the guessing process to\n    # indicate that column names cannot be numeric or have certain\n    # characters at the beginning or end.  It gets used in\n    # BaseHeader.check_column_names().", "metadata": {"file_name": "astropy/io/ascii/core.py", "File Name": "astropy/io/ascii/core.py", "Classes": "CsvWriter, MaskedConstant, InconsistentTableError, OptionalTableImportError, ParameterError, FastOptionsError, NoType, StrType, NumType, FloatType, BoolType, IntType, AllType, Column, BaseInputter, BaseSplitter, DefaultSplitter, BaseHeader, BaseData, BaseOutputter, TableOutputter, MetaBaseReader, BaseReader, ContinuationLinesInputter, WhitespaceSplitter", "Functions": "_check_multidim_table, _replace_tab_with_space, _get_line_index, convert_numpy, _deduplicate_names, _is_number, _apply_include_exclude_names, _get_reader, _get_writer, bool_converter, generic_converter, rename_columns"}}, {"code": "with suppress(TypeError):\n            # Strings only\n            if os.linesep not in table + \"\":\n                self.data.table_name = os.path.basename(table)\n\n        # If one of the newline chars is set as field delimiter, only\n        # accept the other one as line splitter\n        if self.header.splitter.delimiter == \"\\n\":\n            newline = \"\\r\"\n        elif self.header.splitter.delimiter == \"\\r\":\n            newline = \"\\n\"\n        else:\n            newline = None\n\n        # Get a list of the lines (rows) in the table\n        self.lines = self.inputter.get_lines(table, newline=newline)\n\n        # Set self.data.data_lines to a slice of lines contain the data rows\n        self.data.get_data_lines(self.lines)\n\n        # Extract table meta values (e.g. keywords, comments, etc).  Updates self.meta.\n        self.header.update_meta(self.lines, self.meta)\n\n        # Get the table column definitions\n        self.header.get_cols(self.lines)\n\n        # Make sure columns are valid\n        self.header.check_column_names(self.names, self.strict_names, self.guessing)\n\n        self.cols = cols = self.header.cols\n        self.data.splitter.cols = cols\n        n_cols = len(cols)\n\n        for i, str_vals in enumerate(self.data.get_str_vals()):\n            if len(str_vals) != n_cols:\n                str_vals = self.inconsistent_handler(str_vals, n_cols)\n\n                # if str_vals is None, we skip this row\n                if str_vals is None:\n                    continue\n\n                # otherwise,", "metadata": {"file_name": "astropy/io/ascii/core.py", "File Name": "astropy/io/ascii/core.py", "Classes": "CsvWriter, MaskedConstant, InconsistentTableError, OptionalTableImportError, ParameterError, FastOptionsError, NoType, StrType, NumType, FloatType, BoolType, IntType, AllType, Column, BaseInputter, BaseSplitter, DefaultSplitter, BaseHeader, BaseData, BaseOutputter, TableOutputter, MetaBaseReader, BaseReader, ContinuationLinesInputter, WhitespaceSplitter", "Functions": "_check_multidim_table, _replace_tab_with_space, _get_line_index, convert_numpy, _deduplicate_names, _is_number, _apply_include_exclude_names, _get_reader, _get_writer, bool_converter, generic_converter, rename_columns"}}, {"code": "if arg not in kwarg_types or val is None:\n            continue\n\n        # Single type or tuple of types for this arg (like isinstance())\n        types = kwarg_types[arg]\n        err_msg = (\n            f\"{read_write}() argument '{arg}' must be a \"\n            f\"{types} object, got {type(val)} instead\"\n        )\n\n        # Force `types` to be a tuple for the any() check below\n        if not isinstance(types, tuple):\n            types = (types,)\n\n        if not any(is_ducktype(val, cls) for cls in types):\n            raise TypeError(err_msg)\n\n\ndef _expand_user_if_path(argument):\n    if isinstance(argument, (str, bytes, os.PathLike)):\n        # For the `read()` method, a `str` input can be either a file path or\n        # the table data itself. File names for io.ascii cannot have newlines\n        # in them and io.ascii does not accept table data as `bytes`, so we can\n        # attempt to detect data strings like this.\n        is_str_data = isinstance(argument, str) and (\n            \"\\n\" in argument or \"\\r\" in argument\n        )\n        if not is_str_data:\n            # Remain conservative in expanding the presumed-path\n            ex_user = os.path.expanduser(argument)\n            if os.path.exists(ex_user):\n                argument = ex_user\n    return argument\n\n\ndef read(table, guess=None, **kwargs):\n    # This the final output from reading. Static analysis indicates the reading\n    # logic (which is indeed complex) might not define `dat`, thus do so here.\n    dat = None\n\n    # Docstring defined below\n    del _read_trace[:]\n\n    # Downstream readers might munge kwargs\n    kwargs = copy.deepcopy(kwargs)\n\n    _validate_read_write_kwargs(\"read\", **kwargs)\n\n    # Convert 'fast_reader' key in kwargs into a dict if not already and make sure\n    # 'enable' key is available.", "metadata": {"file_name": "astropy/io/ascii/ui.py", "File Name": "astropy/io/ascii/ui.py", "Functions": "_probably_html, set_guess, get_reader, _get_format_class, _get_fast_reader_dict, _validate_read_write_kwargs, _expand_user_if_path, read, _guess, _get_guess_kwargs_list, _read_in_chunks, _read_in_chunks_generator, get_writer, write, get_read_trace, is_ducktype, passthrough_fileobj"}}, {"code": "# Licensed under a 3-clause BSD style license - see LICENSE.rst\nimport itertools\nimport sys\nimport types\nimport warnings\nimport weakref\nfrom collections import OrderedDict, defaultdict\nfrom collections.abc import Mapping\nfrom copy import deepcopy\n\nimport numpy as np\nfrom numpy import ma\n\nfrom astropy import log\nfrom astropy.io.registry import UnifiedReadWriteMethod\nfrom astropy.units import Quantity, QuantityInfo\nfrom astropy.utils import ShapedLikeNDArray, isiterable\nfrom astropy.utils.console import color_print\nfrom astropy.utils.data_info import BaseColumnInfo, DataInfo, MixinInfo\nfrom astropy.utils.decorators import format_doc\nfrom astropy.utils.exceptions import AstropyUserWarning\nfrom astropy.utils.masked import Masked\nfrom astropy.utils.metadata import MetaAttribute, MetaData\n\nfrom . import conf, groups\nfrom .column import (\n    BaseColumn,\n    Column,\n    FalseArray,\n    MaskedColumn,\n    _auto_names,\n    _convert_sequence_data_to_array,\n    col_copy,\n)\nfrom .connect import TableRead, TableWrite\nfrom .index import (\n    Index,\n    SlicedIndex,\n    TableILoc,\n    TableIndices,\n    TableLoc,\n    TableLocIndices,\n    _IndexModeContext,\n    get_index,\n)\nfrom .info import TableInfo\nfrom .mixins.registry import get_mixin_handler\nfrom .ndarray_mixin import NdarrayMixin  # noqa: F401\nfrom .pprint import TableFormatter\nfrom .row import Row\n\n_implementation_notes = \"\"\"\nThis string has informal notes concerning Table implementation for developers.\n\nThings to remember:\n\n- Table has customizable attributes ColumnClass, Column, MaskedColumn.\n  Table.Column is normally just column.Column (same w/ MaskedColumn)\n  but in theory they can be different.  Table.ColumnClass is the default\n  class used to create new non-mixin columns, and this is a function of\n  the Table.masked attribute.  Column creation / manipulation in a Table\n  needs to respect these.\n\n- Column objects that get inserted into the Table.columns attribute must\n  have the info.parent_table attribute set correctly.", "metadata": {"file_name": "astropy/table/table.py", "File Name": "astropy/table/table.py", "Classes": "TableReplaceWarning, TableColumns, TableAttribute, PprintIncludeExclude, Table, QTable, _Context", "Functions": "descr, has_info_class, _get_names_from_list_of_dict, _encode_mixins"}}, {"code": "if guess_kwargs not in filtered_guess_kwargs:\n            filtered_guess_kwargs.append(guess_kwargs)\n\n    # If there are not at least two formats to guess then return no table\n    # (None) to indicate that guessing did not occur.  In that case the\n    # non-guess read() will occur and any problems will result in a more useful\n    # traceback.\n    if len(filtered_guess_kwargs) <= 1:\n        return None\n\n    # Define whitelist of exceptions that are expected from readers when\n    # processing invalid inputs.  Note that OSError must fall through here\n    # so one cannot simply catch any exception.\n    guess_exception_classes = (\n        core.InconsistentTableError,\n        ValueError,\n        TypeError,\n        AttributeError,\n        core.OptionalTableImportError,\n        core.ParameterError,\n        cparser.CParserError,\n    )\n\n    # Now cycle through each possible reader and associated keyword arguments.\n    # Try to read the table using those args, and if an exception occurs then\n    # keep track of the failed guess and move on.\n    for guess_kwargs in filtered_guess_kwargs:\n        t0 = time.time()\n        try:\n            # If guessing will try all Readers then use strict req'ts on column names\n            if \"Reader\" not in read_kwargs:\n                guess_kwargs[\"strict_names\"] = True\n\n            reader = get_reader(**guess_kwargs)\n\n            reader.guessing = True\n            dat = reader.read(table)\n            _read_trace.append(\n                {\n                    \"kwargs\": copy.deepcopy(guess_kwargs),\n                    \"Reader\": reader.__class__,\n                    \"status\": \"Success (guessing)\",\n                    \"dt\": f\"{(time.time() - t0) * 1000:.3f} ms\",\n                }\n            )\n            return dat\n\n        except guess_exception_classes as err:\n            _read_trace.append(\n                {\n                    \"kwargs\": copy.deepcopy(guess_kwargs),\n                    \"status\": f\"{err.__class__.__name__}: {str(err)}\",", "metadata": {"file_name": "astropy/io/ascii/ui.py", "File Name": "astropy/io/ascii/ui.py", "Functions": "_probably_html, set_guess, get_reader, _get_format_class, _get_fast_reader_dict, _validate_read_write_kwargs, _expand_user_if_path, read, _guess, _get_guess_kwargs_list, _read_in_chunks, _read_in_chunks_generator, get_writer, write, get_read_trace, is_ducktype, passthrough_fileobj"}}, {"code": "READ_DOCSTRING = \"\"\"\n    Read the input ``table`` and return the table.  Most of\n    the default behavior for various parameters is determined by the Reader\n    class.\n\n    See also:\n\n    - https://docs.astropy.org/en/stable/io/ascii/\n    - https://docs.astropy.org/en/stable/io/ascii/read.html\n\n    Parameters\n    ----------\n    table : str, file-like, list, `pathlib.Path` object\n        Input table as a file name, file-like object, list of string[s],\n        single newline-separated string or `pathlib.Path` object.\n    guess : bool\n        Try to guess the table format. Defaults to None.\n    format : str, `~astropy.io.ascii.BaseReader`\n        Input table format\n    Inputter : `~astropy.io.ascii.BaseInputter`\n        Inputter class\n    Outputter : `~astropy.io.ascii.BaseOutputter`\n        Outputter class\n    delimiter : str\n        Column delimiter string\n    comment : str\n        Regular expression defining a comment line in table\n    quotechar : str\n        One-character string to quote fields containing special characters\n    header_start : int\n        Line index for the header line not counting comment or blank lines.\n        A line with only whitespace is considered blank.\n    data_start : int\n        Line index for the start of data not counting comment or blank lines.\n        A line with only whitespace is considered blank.\n    data_end : int\n        Line index for the end of data not counting comment or blank lines.\n        This value can be negative to count from the end.\n    converters : dict\n        Dictionary of converters to specify output column dtypes. Each key in\n        the dictionary is a column name or else a name matching pattern\n        including wildcards. The value is either a data type such as ``int`` or\n        ``np.float32``; a list of such types which is tried in order until a\n        successful conversion is achieved; or a list of converter tuples (see\n        the `~astropy.io.ascii.convert_numpy` function for details).", "metadata": {"file_name": "astropy/io/ascii/docs.py", "File Name": "astropy/io/ascii/docs.py"}}, {"code": "q_cls = Masked(Quantity) if isinstance(col, MaskedColumn) else Quantity\n            try:\n                qcol = q_cls(col.data, col.unit, copy=False, subok=True)\n            except Exception as exc:\n                warnings.warn(\n                    f\"column {col.info.name} has a unit but is kept as \"\n                    f\"a {col.__class__.__name__} as an attempt to \"\n                    f\"convert it to Quantity failed with:\\n{exc!r}\",\n                    AstropyUserWarning,\n                )\n            else:\n                qcol.info = col.info\n                qcol.info.indices = col.info.indices\n                col = qcol\n        else:\n            col = super()._convert_col_for_table(col)\n\n        return col", "metadata": {"file_name": "astropy/table/table.py", "File Name": "astropy/table/table.py", "Classes": "TableReplaceWarning, TableColumns, TableAttribute, PprintIncludeExclude, Table, QTable, _Context", "Functions": "descr, has_info_class, _get_names_from_list_of_dict, _encode_mixins"}}, {"code": "html#built-in-table-readers-writers\n        \"\"\"\n        )\n    )\n\n    addarg = parser.add_argument\n    addarg(\"filename\", nargs=\"+\", help=\"path to one or more files\")\n\n    addarg(\n        \"--format\",\n        help=(\n            \"input table format, should be specified if it \"\n            \"cannot be automatically detected\"\n        ),\n    )\n    addarg(\"--more\", action=\"store_true\", help=\"use the pager mode from Table.more\")\n    addarg(\n        \"--info\", action=\"store_true\", help=\"show information about the table columns\"\n    )\n    addarg(\n        \"--stats\", action=\"store_true\", help=\"show statistics about the table columns\"\n    )\n\n    # pprint arguments\n    pprint_args = parser.add_argument_group(\"pprint arguments\")\n    addarg = pprint_args.add_argument\n    addarg(\n        \"--max-lines\",\n        type=int,\n        help=(\n            \"maximum number of lines in table output (default=screen \"\n            \"length, -1 for no limit)\"\n        ),\n    )\n    addarg(\n        \"--max-width\",\n        type=int,\n        help=\"maximum width in table output (default=screen width, -1 for no limit)\",\n    )\n    addarg(\n        \"--hide-unit\",\n        action=\"store_true\",\n        help=(\n            \"hide the header row for unit (which is shown \"\n            \"only if one or more columns has a unit)\"\n        ),\n    )\n    addarg(\n        \"--show-dtype\",\n        action=\"store_true\",\n        help=(\n            \"always include a header row for column dtypes \"\n            \"(otherwise shown only if any column is multidimensional)\"\n        ),\n    )\n\n    # ASCII-specific arguments\n    ascii_args = parser.add_argument_group(\"ASCII arguments\")\n    addarg = ascii_args.add_argument\n    addarg(\"--delimiter\", help=\"column delimiter string\")\n\n    # FITS-specific arguments\n    fits_args = parser.add_argument_group(\"FITS arguments\")\n    addarg = fits_args.add_argument\n    addarg(\"--hdu\", help=\"name of the HDU to show\")\n\n    # HDF5-specific arguments\n    hdf5_args = parser.add_argument_group(\"HDF5 arguments\")\n    addarg = hdf5_args.add_argument\n    addarg(\"--path\",", "metadata": {"file_name": "astropy/table/scripts/showtable.py", "File Name": "astropy/table/scripts/showtable.py", "Functions": "showtable, main"}}, {"code": "Rdb and Tab)\n            writer.data.splitter.process_val = operator.methodcaller(\"strip\", \" \\t\")\n        else:\n            writer.data.splitter.process_val = None\n    if \"names\" in kwargs:\n        writer.header.names = kwargs[\"names\"]\n    if \"include_names\" in kwargs:\n        writer.include_names = kwargs[\"include_names\"]\n    if \"exclude_names\" in kwargs:\n        writer.exclude_names = kwargs[\"exclude_names\"]\n    if \"fill_values\" in kwargs:\n        # Prepend user-specified values to the class default.\n        with suppress(TypeError, IndexError):\n            # Test if it looks like (match, replace_string, optional_colname),\n            # in which case make it a list\n            kwargs[\"fill_values\"][1] + \"\"\n            kwargs[\"fill_values\"] = [kwargs[\"fill_values\"]]\n        writer.data.fill_values = kwargs[\"fill_values\"] + writer.data.fill_values\n    if \"fill_include_names\" in kwargs:\n        writer.data.fill_include_names = kwargs[\"fill_include_names\"]\n    if \"fill_exclude_names\" in kwargs:\n        writer.data.fill_exclude_names = kwargs[\"fill_exclude_names\"]\n    return writer", "metadata": {"file_name": "astropy/io/ascii/core.py", "File Name": "astropy/io/ascii/core.py", "Classes": "CsvWriter, MaskedConstant, InconsistentTableError, OptionalTableImportError, ParameterError, FastOptionsError, NoType, StrType, NumType, FloatType, BoolType, IntType, AllType, Column, BaseInputter, BaseSplitter, DefaultSplitter, BaseHeader, BaseData, BaseOutputter, TableOutputter, MetaBaseReader, BaseReader, ContinuationLinesInputter, WhitespaceSplitter", "Functions": "_check_multidim_table, _replace_tab_with_space, _get_line_index, convert_numpy, _deduplicate_names, _is_number, _apply_include_exclude_names, _get_reader, _get_writer, bool_converter, generic_converter, rename_columns"}}, {"code": ")\n                    return\n                if key in allowed_keys:\n                    break\n            print(key)\n\n            if key.lower() == \"q\":\n                break\n            elif key == \" \" or key == \"f\":\n                i0 += delta_lines\n            elif key == \"b\":\n                i0 = i0 - delta_lines\n            elif key == \"r\":\n                pass\n            elif key == \"<\":\n                i0 = 0\n            elif key == \">\":\n                i0 = len(tabcol)\n            elif key == \"p\":\n                i0 -= 1\n            elif key == \"n\":\n                i0 += 1\n            elif key == \"h\":\n                showlines = False\n                print(\n                    \"\"\"\n    Browsing keys:\n       f, <space> : forward one page\n       b : back one page\n       r : refresh same page\n       n : next row\n       p : previous row\n       < : go to beginning\n       > : go to end\n       q : quit browsing\n       h : print this help\"\"\",\n                    end=\" \",\n                )\n            if i0 < 0:\n                i0 = 0\n            if i0 >= len(tabcol) - delta_lines:\n                i0 = len(tabcol) - delta_lines\n            print(\"\\n\")", "metadata": {"file_name": "astropy/table/pprint.py", "File Name": "astropy/table/pprint.py", "Classes": "TableFormatter", "Functions": "default_format_func, _use_str_for_masked_values, _possible_string_format_functions, get_auto_format_func, _get_pprint_include_names, _auto_format_func, get_matches, format_col_str, outwidth"}}, {"code": "dtype.kind == \"O\":\n                # If all elements of an object array are string-like or np.nan\n                # then coerce back to a native numpy str/unicode array.\n                string_types = (str, bytes)\n                nan = np.nan\n                if all(isinstance(x, string_types) or x is nan for x in data):\n                    # Force any missing (null) values to b''.  Numpy will\n                    # upcast to str/unicode as needed.\n                    data[mask] = b\"\"\n\n                    # When the numpy object array is represented as a list then\n                    # numpy initializes to the correct string or unicode type.\n                    data = np.array([x for x in data])\n\n            # Numpy datetime64\n            if data.dtype.kind == \"M\":\n                from astropy.time import Time\n\n                out[name] = Time(data, format=\"datetime64\")\n                if np.any(mask):\n                    out[name][mask] = np.ma.masked\n                out[name].format = \"isot\"\n\n            # Numpy timedelta64\n            elif data.dtype.kind == \"m\":\n                from astropy.time import TimeDelta\n\n                data_sec = data.astype(\"timedelta64[ns]\").astype(np.float64) / 1e9\n                out[name] = TimeDelta(data_sec, format=\"sec\")\n                if np.any(mask):\n                    out[name][mask] = np.ma.masked\n\n            else:\n                if np.any(mask):\n                    out[name] = MaskedColumn(data=data, name=name, mask=mask, unit=unit)\n                else:\n                    out[name] = Column(data=data, name=name, unit=unit)\n\n        return cls(out)\n\n    info = TableInfo()\n\n\nclass QTable(Table):\n    \"\"\"A class to represent tables of heterogeneous data.\n\n    `~astropy.table.QTable` provides a class for heterogeneous tabular data\n    which can be easily modified, for instance adding columns or new rows.\n\n    The `~astropy.table.QTable` class is identical to `~astropy.table.Table`\n    except that columns with an associated ``unit`` attribute are converted to\n    `~astropy.units.Quantity` objects.", "metadata": {"file_name": "astropy/table/table.py", "File Name": "astropy/table/table.py", "Classes": "TableReplaceWarning, TableColumns, TableAttribute, PprintIncludeExclude, Table, QTable, _Context", "Functions": "descr, has_info_class, _get_names_from_list_of_dict, _encode_mixins"}}, {"code": "last_err = \"no converters defined\"\n\n            while not hasattr(col, \"data\"):\n                # Try converters, popping the unsuccessful ones from the list.\n                # If there are no converters left here then fail.\n                if not col.converters:\n                    raise ValueError(f\"Column {col.name} failed to convert: {last_err}\")\n\n                converter_func, converter_type = col.converters[0]\n                if not issubclass(converter_type, col.type):\n                    raise TypeError(\"converter type does not match column type\")\n\n                try:\n                    col.data = converter_func(col.str_vals)\n                    col.type = converter_type\n                except (OverflowError, TypeError, ValueError) as err:\n                    # Overflow during conversion (most likely an int that\n                    # doesn't fit in native C long). Put string at the top of\n                    # the converters list for the next while iteration.\n                    # With python/cpython#95778 this has been supplemented with a\n                    # \"ValueError: Exceeds the limit (4300) for integer string conversion\"\n                    # so need to catch that as well.\n                    if isinstance(err, OverflowError) or (\n                        isinstance(err, ValueError)\n                        and str(err).startswith(\"Exceeds the limit\")\n                    ):\n                        warnings.warn(\n                            f\"OverflowError converting to {converter_type.__name__} in\"\n                            f\" column {col.name}, reverting to String.\",\n                            AstropyWarning,\n                        )\n                        col.converters.insert(0, convert_numpy(str))\n                    else:\n                        col.converters.pop(0)\n                    last_err = err", "metadata": {"file_name": "astropy/io/ascii/core.py", "File Name": "astropy/io/ascii/core.py", "Classes": "CsvWriter, MaskedConstant, InconsistentTableError, OptionalTableImportError, ParameterError, FastOptionsError, NoType, StrType, NumType, FloatType, BoolType, IntType, AllType, Column, BaseInputter, BaseSplitter, DefaultSplitter, BaseHeader, BaseData, BaseOutputter, TableOutputter, MetaBaseReader, BaseReader, ContinuationLinesInputter, WhitespaceSplitter", "Functions": "_check_multidim_table, _replace_tab_with_space, _get_line_index, convert_numpy, _deduplicate_names, _is_number, _apply_include_exclude_names, _get_reader, _get_writer, bool_converter, generic_converter, rename_columns"}}, {"code": "if fast_reader[\"enable\"] and f\"fast_{format}\" in core.FAST_CLASSES:\n            fast_kwargs = copy.deepcopy(new_kwargs)\n            fast_kwargs[\"Reader\"] = core.FAST_CLASSES[f\"fast_{format}\"]\n            fast_reader_rdr = get_reader(**fast_kwargs)\n            try:\n                dat = fast_reader_rdr.read(table)\n                _read_trace.append(\n                    {\n                        \"kwargs\": copy.deepcopy(fast_kwargs),\n                        \"Reader\": fast_reader_rdr.__class__,\n                        \"status\": \"Success with fast reader (no guessing)\",\n                    }\n                )\n            except (\n                core.ParameterError,\n                cparser.CParserError,\n                UnicodeEncodeError,\n            ) as err:\n                # special testing value to avoid falling back on the slow reader\n                if fast_reader[\"enable\"] == \"force\":\n                    raise core.InconsistentTableError(\n                        f\"fast reader {fast_reader_rdr.__class__} exception: {err}\"\n                    )\n                # If the fast reader doesn't work, try the slow version\n                reader = get_reader(**new_kwargs)\n                dat = reader.read(table)\n                _read_trace.append(\n                    {\n                        \"kwargs\": copy.deepcopy(new_kwargs),\n                        \"Reader\": reader.__class__,\n                        \"status\": (\n                            \"Success with slow reader after failing\"\n                            \" with fast (no guessing)\"\n                        ),\n                    }\n                )\n        else:\n            reader = get_reader(**new_kwargs)\n            dat = reader.read(table)\n            _read_trace.append(\n                {\n                    \"kwargs\": copy.deepcopy(new_kwargs),\n                    \"Reader\": reader.__class__,\n                    \"status\": \"Success with specified Reader class (no guessing)\",\n                }\n            )\n\n    # Static analysis (pyright) indicates `dat` might be left undefined, so just\n    # to be sure define it at the beginning and check here.\n    if dat is None:\n        raise RuntimeError(\n            \"read() function failed due to code logic error, \"\n            \"please report this bug on github\"\n        )\n\n    return dat\n\n\nread.__doc__ = core.READ_DOCSTRING", "metadata": {"file_name": "astropy/io/ascii/ui.py", "File Name": "astropy/io/ascii/ui.py", "Functions": "_probably_html, set_guess, get_reader, _get_format_class, _get_fast_reader_dict, _validate_read_write_kwargs, _expand_user_if_path, read, _guess, _get_guess_kwargs_list, _read_in_chunks, _read_in_chunks_generator, get_writer, write, get_read_trace, is_ducktype, passthrough_fileobj"}}, {"code": "Parameters\n        ----------\n        table : `~astropy.table.Table`\n            Input table data.\n\n        Returns\n        -------\n        lines : list\n            List of strings corresponding to ASCII table\n\n        \"\"\"\n        # Check column names before altering\n        self.header.cols = list(table.columns.values())\n        self.header.check_column_names(self.names, self.strict_names, False)\n\n        # In-place update of columns in input ``table`` to reflect column\n        # filtering.  Note that ``table`` is guaranteed to be a copy of the\n        # original user-supplied table.\n        _apply_include_exclude_names(\n            table, self.names, self.include_names, self.exclude_names\n        )\n\n        # This is a hook to allow updating the table columns after name\n        # filtering but before setting up to write the data.  This is currently\n        # only used by ECSV and is otherwise just a pass-through.\n        table = self.update_table_data(table)\n\n        # Check that table column dimensions are supported by this format class.\n        # Most formats support only 1-d columns, but some like ECSV support N-d.\n        self._check_multidim_table(table)\n\n        # Now use altered columns\n        new_cols = list(table.columns.values())\n        # link information about the columns to the writer object (i.e. self)\n        self.header.cols = new_cols\n        self.data.cols = new_cols\n        self.header.table_meta = table.meta\n\n        # Write header and data to lines list\n        lines = []\n        self.write_header(lines, table.meta)\n        self.data.write(lines)\n\n        return lines", "metadata": {"file_name": "astropy/io/ascii/core.py", "File Name": "astropy/io/ascii/core.py", "Classes": "CsvWriter, MaskedConstant, InconsistentTableError, OptionalTableImportError, ParameterError, FastOptionsError, NoType, StrType, NumType, FloatType, BoolType, IntType, AllType, Column, BaseInputter, BaseSplitter, DefaultSplitter, BaseHeader, BaseData, BaseOutputter, TableOutputter, MetaBaseReader, BaseReader, ContinuationLinesInputter, WhitespaceSplitter", "Functions": "_check_multidim_table, _replace_tab_with_space, _get_line_index, convert_numpy, _deduplicate_names, _is_number, _apply_include_exclude_names, _get_reader, _get_writer, bool_converter, generic_converter, rename_columns"}}, {"code": "# Licensed under a 3-clause BSD style license - see LICENSE.rst\n\nimport astropy.config as _config\nfrom astropy.utils.compat import optional_deps\n\nfrom .column import Column, ColumnInfo, MaskedColumn, StringTruncateWarning\n\n__all__ = [\n    \"BST\",\n    \"Column\",\n    \"ColumnGroups\",\n    \"ColumnInfo\",\n    \"Conf\",\n    \"JSViewer\",\n    \"MaskedColumn\",\n    \"NdarrayMixin\",\n    \"QTable\",\n    \"Row\",\n    \"SCEngine\",\n    \"SerializedColumn\",\n    \"SortedArray\",\n    \"StringTruncateWarning\",\n    \"Table\",\n    \"TableAttribute\",\n    \"TableColumns\",\n    \"TableFormatter\",\n    \"TableGroups\",\n    \"TableMergeError\",\n    \"TableReplaceWarning\",\n    \"conf\",\n    \"connect\",\n    \"hstack\",\n    \"join\",\n    \"registry\",\n    \"represent_mixins_as_columns\",\n    \"setdiff\",\n    \"unique\",\n    \"vstack\",\n    \"dstack\",\n    \"conf\",\n    \"join_skycoord\",\n    \"join_distance\",\n    \"PprintIncludeExclude\",\n]", "metadata": {"file_name": "astropy/table/__init__.py", "File Name": "astropy/table/__init__.py", "Classes": "Conf"}}, {"code": "Parameters\n        ----------\n        names : list\n            User-supplied list of column names\n        strict_names : bool\n            Whether to impose extra requirements on names\n        guessing : bool\n            True if this method is being called while guessing the table format\n        \"\"\"\n        if strict_names:\n            # Impose strict requirements on column names (normally used in guessing)\n            bads = [\" \", \",\", \"|\", \"\\t\", \"'\", '\"']\n            for name in self.colnames:\n                if (\n                    _is_number(name)\n                    or len(name) == 0\n                    or name[0] in bads\n                    or name[-1] in bads\n                ):\n                    raise InconsistentTableError(\n                        f\"Column name {name!r} does not meet strict name requirements\"\n                    )\n        # When guessing require at least two columns, except for ECSV which can\n        # reliably be guessed from the header requirements.\n        if (\n            guessing\n            and len(self.colnames) <= 1\n            and self.__class__.__name__ != \"EcsvHeader\"\n        ):\n            raise ValueError(\n                \"Table format guessing requires at least two columns, got {}\".format(\n                    list(self.colnames)\n                )\n            )\n\n        if names is not None and len(names) != len(self.colnames):\n            raise InconsistentTableError(\n                \"Length of names argument ({}) does not match number\"\n                \" of table columns ({})\".format(len(names), len(self.colnames))\n            )\n\n\nclass BaseData:\n    \"\"\"\n    Base table data reader.\n    \"\"\"", "metadata": {"file_name": "astropy/io/ascii/core.py", "File Name": "astropy/io/ascii/core.py", "Classes": "CsvWriter, MaskedConstant, InconsistentTableError, OptionalTableImportError, ParameterError, FastOptionsError, NoType, StrType, NumType, FloatType, BoolType, IntType, AllType, Column, BaseInputter, BaseSplitter, DefaultSplitter, BaseHeader, BaseData, BaseOutputter, TableOutputter, MetaBaseReader, BaseReader, ContinuationLinesInputter, WhitespaceSplitter", "Functions": "_check_multidim_table, _replace_tab_with_space, _get_line_index, convert_numpy, _deduplicate_names, _is_number, _apply_include_exclude_names, _get_reader, _get_writer, bool_converter, generic_converter, rename_columns"}}, {"code": "with suppress(TypeError):\n                # For strings only\n                if os.linesep not in table + \"\":\n                    self.data.table_name = os.path.basename(table)\n\n            self.data.header = self.header\n            self.header.data = self.data\n\n            # Get a list of the lines (rows) in the table\n            lines = self.inputter.get_lines(table)\n\n            # Now try increasing data.start_line by one until the table reads successfully.\n            # For efficiency use the in-memory list of lines instead of `table`, which\n            # could be a file.\n            for data_start in range(len(lines)):\n                self.data.start_line = data_start\n                with suppress(Exception):\n                    table = super().read(lines)\n                    return table\n        else:\n            return super().read(table)", "metadata": {"file_name": "astropy/io/ascii/cds.py", "File Name": "astropy/io/ascii/cds.py", "Classes": "CdsHeader, CdsData, Cds"}}, {"code": "Examples::\n\n      >>> from astropy.io import ascii\n      >>> table = ascii.read(\"data/cds.dat\")\n      >>> table = ascii.read(\"data/vizier/table1.dat\", readme=\"data/vizier/ReadMe\")\n      >>> table = ascii.read(\"data/cds/multi/lhs2065.dat\", readme=\"data/cds/multi/ReadMe\")\n      >>> table = ascii.read(\"data/cds/glob/lmxbrefs.dat\", readme=\"data/cds/glob/ReadMe\")\n\n    The table name and the CDS ReadMe file can be entered as URLs.  This can be used\n    to directly load tables from the Internet.  For example, Vizier tables from the\n    CDS::\n\n      >>> table = ascii.read(\"ftp://cdsarc.u-strasbg.fr/pub/cats/VII/253/snrs.dat\",\n      ...             readme=\"ftp://cdsarc.u-strasbg.fr/pub/cats/VII/253/ReadMe\")\n\n    If the header (ReadMe) and data are stored in a single file and there\n    is content between the header and the data (for instance Notes), then the\n    parsing process may fail.  In this case you can instruct the reader to\n    guess the actual start of the data by supplying ``data_start='guess'`` in the\n    call to the ``ascii.read()`` function.  You should verify that the output\n    data table matches expectation based on the input CDS file.\n\n    **Using a reader object**\n\n    When ``Cds`` reader object is created with a ``readme`` parameter\n    passed to it at initialization, then when the ``read`` method is\n    executed with a table filename, the header information for the\n    specified table is taken from the ``readme`` file.  An\n    ``InconsistentTableError`` is raised if the ``readme`` file does not\n    have header information for the given table.", "metadata": {"file_name": "astropy/io/ascii/cds.py", "File Name": "astropy/io/ascii/cds.py", "Classes": "CdsHeader, CdsData, Cds"}}, {"code": "The input table can be one of:\n\n        * File name\n        * String (newline separated) with all header and data lines (must have at least 2 lines)\n        * File-like object with read() method\n        * List of strings\n\n        Parameters\n        ----------\n        table : str, file-like, list\n            Can be either a file name, string (newline separated) with all header and data\n            lines (must have at least 2 lines), a file-like object with a\n            ``read()`` method, or a list of strings.\n        newline :\n            Line separator. If `None` use OS default from ``splitlines()``.", "metadata": {"file_name": "astropy/io/ascii/core.py", "File Name": "astropy/io/ascii/core.py", "Classes": "CsvWriter, MaskedConstant, InconsistentTableError, OptionalTableImportError, ParameterError, FastOptionsError, NoType, StrType, NumType, FloatType, BoolType, IntType, AllType, Column, BaseInputter, BaseSplitter, DefaultSplitter, BaseHeader, BaseData, BaseOutputter, TableOutputter, MetaBaseReader, BaseReader, ContinuationLinesInputter, WhitespaceSplitter", "Functions": "_check_multidim_table, _replace_tab_with_space, _get_line_index, convert_numpy, _deduplicate_names, _is_number, _apply_include_exclude_names, _get_reader, _get_writer, bool_converter, generic_converter, rename_columns"}}, {"code": "dat = _guess(table, new_kwargs, format, fast_reader)\n        if dat is None:\n            guess = False\n\n    if not guess:\n        if format is None:\n            reader = get_reader(**new_kwargs)\n            format = reader._format_name\n\n        table = _expand_user_if_path(table)\n\n        # Try the fast reader version of `format` first if applicable.  Note that\n        # if user specified a fast format (e.g. format='fast_basic') this test\n        # will fail and the else-clause below will be used.", "metadata": {"file_name": "astropy/io/ascii/ui.py", "File Name": "astropy/io/ascii/ui.py", "Functions": "_probably_html, set_guess, get_reader, _get_format_class, _get_fast_reader_dict, _validate_read_write_kwargs, _expand_user_if_path, read, _guess, _get_guess_kwargs_list, _read_in_chunks, _read_in_chunks_generator, get_writer, write, get_read_trace, is_ducktype, passthrough_fileobj"}}, {"code": "after help)\n                try:\n                    os.system(\"cls\" if os.name == \"nt\" else \"clear\")\n                except Exception:\n                    pass  # No worries if clear screen call fails\n                lines = tabcol[i0:i1].pformat(**kwargs)\n                colors = (\n                    \"red\" if i < n_header else \"default\" for i in range(len(lines))\n                )\n                for color, line in zip(colors, lines):\n                    color_print(line, color)\n            showlines = True\n            print()\n            print(\"-- f, <space>, b, r, p, n, <, >, q h (help) --\", end=\" \")\n            # Get a valid key\n            while True:\n                try:\n                    key = inkey().lower()\n                except Exception:\n                    print(\"\\n\")\n                    log.error(\n                        \"Console does not support getting a character\"\n                        \" as required by more().  Use pprint() instead.\"", "metadata": {"file_name": "astropy/table/pprint.py", "File Name": "astropy/table/pprint.py", "Classes": "TableFormatter", "Functions": "default_format_func, _use_str_for_masked_values, _possible_string_format_functions, get_auto_format_func, _get_pprint_include_names, _auto_format_func, get_matches, format_col_str, outwidth"}}, {"code": "conf = Conf()\n\n# Finally import the formats for the read and write method but delay building\n# the documentation until all are loaded. (#5275)\nfrom astropy.io import registry\n\nfrom . import connect\nfrom .bst import BST\nfrom .groups import ColumnGroups, TableGroups\nfrom .operations import (\n    TableMergeError,\n    dstack,\n    hstack,\n    join,\n    join_distance,\n    join_skycoord,\n    setdiff,\n    unique,\n    vstack,\n)\nfrom .serialize import SerializedColumn, represent_mixins_as_columns\nfrom .soco import SCEngine\nfrom .sorted_array import SortedArray\nfrom .table import (\n    NdarrayMixin,\n    PprintIncludeExclude,\n    QTable,\n    Row,\n    Table,\n    TableAttribute,\n    TableColumns,\n    TableFormatter,\n    TableReplaceWarning,\n)\n\nwith registry.delay_doc_updates(Table):\n    # Import routines that connect readers/writers to astropy.table\n    import astropy.io.ascii.connect\n    import astropy.io.fits.connect\n    import astropy.io.misc.connect\n    import astropy.io.misc.pandas.connect\n    import astropy.io.votable.connect\n\n    from .jsviewer import JSViewer\n\n    if optional_deps.HAS_ASDF_ASTROPY:\n        import asdf_astropy.io.connect\n    else:\n        import astropy.io.misc.asdf.connect", "metadata": {"file_name": "astropy/table/__init__.py", "File Name": "astropy/table/__init__.py", "Classes": "Conf"}}, {"code": "Supported keys::\n\n          f, <space> : forward one page\n          b : back one page\n          r : refresh same page\n          n : next row\n          p : previous row\n          < : go to beginning\n          > : go to end\n          q : quit browsing\n          h : print this help\n\n        Parameters\n        ----------\n        max_lines : int\n            Maximum number of lines in table output\n\n        max_width : int or None\n            Maximum character width of output\n\n        show_name : bool\n            Include a header row for column names. Default is True.\n\n        show_unit : bool\n            Include a header row for unit.  Default is to show a row\n            for units only if one or more columns has a defined value\n            for the unit.\n\n        show_dtype : bool\n            Include a header row for column dtypes. Default is False.\n        \"\"\"", "metadata": {"file_name": "astropy/table/table.py", "File Name": "astropy/table/table.py", "Classes": "TableReplaceWarning, TableColumns, TableAttribute, PprintIncludeExclude, Table, QTable, _Context", "Functions": "descr, has_info_class, _get_names_from_list_of_dict, _encode_mixins"}}, {"code": "ascii_coded = (\n    \"\u00d2\u2659\u2659\u2659\u2659\u2659\u2659\u2659\u2659\u264c\u2650\u2650\u264c\u2659\u2659\u2659\u2659\u2659\u2659\u264c\u264c\u2659\u2659\u00d2\u2659\u2659\u2659\u2659\u2659\u2659\u2659\u2658\u2650\u2650\u2650\u2648\u2659\u2659\u2659\u2659\u2659\u264c\u2650\u2650\u2650\u2654\u00d2\u2659\u2659\u264c\u2648\u2659\u2659\u264c\u2650\u2648\u2648\u2659\u2659\u2659\u2659\u2659\u2659\u2659\u2659\u2648\u2650\u2650\u2659\u00d2\u2659\u2650\u2659\u2659\u2659\u2650\u2650\u2659\u2659\u2659\"\n    \"\u2659\u2659\u2659\u2659\u2659\u2659\u2659\u2659\u2659\u2659\u2659\u2659\u00d2\u2650\u2654\u2659\u2659\u2658\u2650\u2650\u2659\u2659\u264c\u2650\u2650\u2654\u2659\u2659\u264c\u264c\u264c\u2659\u2659\u2659\u264c\u00d2\u2650\u2650\u2659\u2659\u2658\u2650\u2650\u264c\u2659\u2648\u2650\u2648\u2659\u2659\u2659\u2648\u2650\u2650\u2659\u2659\u2658\u2654\u00d2\u2650\u2650\u264c\u2659\u2658\u2650\u2650\u2650\u264c\u264c\u2659\u2659\u264c\u264c\u264c\u2659\u2648\u2648\u2659\u264c\u2650\"", "metadata": {"file_name": "astropy/table/pandas.py", "File Name": "astropy/table/pandas.py", "Classes": "HTMLWithBackup"}}, {"code": "names = None\n    include_names = None\n    exclude_names = None\n    strict_names = False\n    guessing = False\n    encoding = None\n\n    header_class = BaseHeader\n    data_class = BaseData\n    inputter_class = BaseInputter\n    outputter_class = TableOutputter\n\n    # Max column dimension that writer supports for this format. Exceptions\n    # include ECSV (no limit) and HTML (max_ndim=2).\n    max_ndim = 1\n\n    def __init__(self):\n        self.header = self.header_class()\n        self.data = self.data_class()\n        self.inputter = self.inputter_class()\n        self.outputter = self.outputter_class()\n        # Data and Header instances benefit from a little cross-coupling.  Header may need to\n        # know about number of data columns for auto-column name generation and Data may\n        # need to know about header (e.g. for fixed-width tables where widths are spec'd in header.\n        self.data.header = self.header\n        self.header.data = self.data\n\n        # Metadata, consisting of table-level meta and column-level meta.  The latter\n        # could include information about column type, description, formatting, etc,\n        # depending on the table meta format.\n        self.meta = OrderedDict(table=OrderedDict(), cols=OrderedDict())\n\n    def _check_multidim_table(self, table):\n        \"\"\"Check that the dimensions of columns in ``table`` are acceptable.\n\n        The reader class attribute ``max_ndim`` defines the maximum dimension of\n        columns that can be written using this format.", "metadata": {"file_name": "astropy/io/ascii/core.py", "File Name": "astropy/io/ascii/core.py", "Classes": "CsvWriter, MaskedConstant, InconsistentTableError, OptionalTableImportError, ParameterError, FastOptionsError, NoType, StrType, NumType, FloatType, BoolType, IntType, AllType, Column, BaseInputter, BaseSplitter, DefaultSplitter, BaseHeader, BaseData, BaseOutputter, TableOutputter, MetaBaseReader, BaseReader, ContinuationLinesInputter, WhitespaceSplitter", "Functions": "_check_multidim_table, _replace_tab_with_space, _get_line_index, convert_numpy, _deduplicate_names, _is_number, _apply_include_exclude_names, _get_reader, _get_writer, bool_converter, generic_converter, rename_columns"}}, {"code": "# Licensed under a 3-clause BSD style license - see LICENSE.rst\n# This file connects the readers/writers to the astropy.table.Table class\n\n\nimport re\n\nfrom astropy.io import registry as io_registry  # noqa: F401\nfrom astropy.table import Table\n\n__all__ = []\n\n\ndef io_read(format, filename, **kwargs):\n    from .ui import read\n\n    if format != \"ascii\":\n        format = re.sub(r\"^ascii\\.\", \"\", format)\n        kwargs[\"format\"] = format\n    return read(filename, **kwargs)\n\n\ndef io_write(format, table, filename, **kwargs):\n    from .ui import write\n\n    if format != \"ascii\":\n        format = re.sub(r\"^ascii\\.\", \"\", format)\n        kwargs[\"format\"] = format\n    return write(table, filename, **kwargs)\n\n\ndef io_identify(suffix, origin, filepath, fileobj, *args, **kwargs):\n    return filepath is not None and filepath.endswith(suffix)\n\n\ndef _get_connectors_table():\n    from .core import FORMAT_CLASSES\n\n    rows = []\n    rows.append(\n        (\"ascii\", \"\", \"Yes\", \"ASCII table in any supported format (uses guessing)\")\n    )\n    for format in sorted(FORMAT_CLASSES):\n        cls = FORMAT_CLASSES[format]\n\n        io_format = \"ascii.\" + cls._format_name\n        description = getattr(cls, \"_description\", \"\")\n        class_link = f\":class:`~{cls.__module__}.{cls.__name__}`\"\n        suffix = getattr(cls, \"_io_registry_suffix\", \"\")\n        can_write = \"Yes\" if getattr(cls, \"_io_registry_can_write\", True) else \"\"\n\n        rows.append((io_format, suffix, can_write, f\"{class_link}: {description}\"))\n    out = Table(list(zip(*rows)), names=(\"Format\", \"Suffix\", \"Write\", \"Description\"))\n    for colname in (\"Format\", \"Description\"):\n        width = max(len(x) for x in out[colname])\n        out[colname].format = f\"%-{width}s\"\n\n    return out", "metadata": {"file_name": "astropy/io/ascii/connect.py", "File Name": "astropy/io/ascii/connect.py", "Functions": "io_read, io_write, io_identify, _get_connectors_table"}}, {"code": "Beware just dropping\n  an object into the columns dict since an existing column may\n  be part of another Table and have parent_table set to point at that\n  table.  Dropping that column into `columns` of this Table will cause\n  a problem for the old one so the column object needs to be copied (but\n  not necessarily the data).\n\n  Currently replace_column is always making a copy of both object and\n  data if parent_table is set.  This could be improved but requires a\n  generic way to copy a mixin object but not the data.\n\n- Be aware of column objects that have indices set.\n\n- `cls.ColumnClass` is a property that effectively uses the `masked` attribute\n  to choose either `cls.Column` or `cls.MaskedColumn`.\n\"\"\"\n\n__doctest_skip__ = [\n    \"Table.read\",\n    \"Table.write\",\n    \"Table._read\",\n    \"Table.convert_bytestring_to_unicode\",\n    \"Table.convert_unicode_to_bytestring\",\n]\n\n__doctest_requires__ = {\"*pandas\": [\"pandas>=1.1\"]}\n\n_pprint_docs = \"\"\"\n    {__doc__}\n\n    Parameters\n    ----------\n    max_lines : int or None\n        Maximum number of lines in table output.\n\n    max_width : int or None\n        Maximum character width of output.\n\n    show_name : bool\n        Include a header row for column names. Default is True.\n\n    show_unit : bool\n        Include a header row for unit.  Default is to show a row\n        for units only if one or more columns has a defined value\n        for the unit.\n\n    show_dtype : bool\n        Include a header row for column dtypes. Default is False.\n\n    align : str or list or tuple or None\n        Left/right alignment of columns. Default is right (None) for all\n        columns. Other allowed values are '>', '<', '^', and '0=' for\n        right, left, centered, and 0-padded, respectively. A list of\n        strings can be provided for alignment of tables with multiple\n        columns.\n    \"\"\"", "metadata": {"file_name": "astropy/table/table.py", "File Name": "astropy/table/table.py", "Classes": "TableReplaceWarning, TableColumns, TableAttribute, PprintIncludeExclude, Table, QTable, _Context", "Functions": "descr, has_info_class, _get_names_from_list_of_dict, _encode_mixins"}}, {"code": "class MetaBaseReader(type):\n    def __init__(cls, name, bases, dct):\n        super().__init__(name, bases, dct)\n\n        format = dct.get(\"_format_name\")\n        if format is None:\n            return\n\n        fast = dct.get(\"_fast\")\n        if fast is not None:\n            FAST_CLASSES[format] = cls\n\n        FORMAT_CLASSES[format] = cls\n\n        io_formats = [\"ascii.\" + format] + dct.get(\"_io_registry_format_aliases\", [])\n\n        if dct.get(\"_io_registry_suffix\"):\n            func = functools.partial(connect.io_identify, dct[\"_io_registry_suffix\"])\n            connect.io_registry.register_identifier(io_formats[0], Table, func)\n\n        for io_format in io_formats:\n            func = functools.partial(connect.io_read, io_format)\n            header = f\"ASCII reader '{io_format}' details\\n\"\n            func.__doc__ = (\n                inspect.cleandoc(READ_DOCSTRING).strip()\n                + \"\\n\\n\"\n                + header\n                + re.sub(\".\", \"=\", header)\n                + \"\\n\"\n            )\n            func.__doc__ += inspect.cleandoc(cls.__doc__).strip()\n            connect.io_registry.register_reader(io_format, Table, func)\n\n            if dct.get(\"_io_registry_can_write\", True):\n                func = functools.partial(connect.io_write, io_format)\n                header = f\"ASCII writer '{io_format}' details\\n\"\n                func.__doc__ = (\n                    inspect.cleandoc(WRITE_DOCSTRING).strip()\n                    + \"\\n\\n\"\n                    + header\n                    + re.sub(\".", "metadata": {"file_name": "astropy/io/ascii/core.py", "File Name": "astropy/io/ascii/core.py", "Classes": "CsvWriter, MaskedConstant, InconsistentTableError, OptionalTableImportError, ParameterError, FastOptionsError, NoType, StrType, NumType, FloatType, BoolType, IntType, AllType, Column, BaseInputter, BaseSplitter, DefaultSplitter, BaseHeader, BaseData, BaseOutputter, TableOutputter, MetaBaseReader, BaseReader, ContinuationLinesInputter, WhitespaceSplitter", "Functions": "_check_multidim_table, _replace_tab_with_space, _get_line_index, convert_numpy, _deduplicate_names, _is_number, _apply_include_exclude_names, _get_reader, _get_writer, bool_converter, generic_converter, rename_columns"}}, {"code": "Offending columns are: {badcols}\\n'\n                f'One can filter out such columns using:\\n'\n                f'names = [name for name in tbl.colnames if len(tbl[name].shape) <= 1]\\n'\n                f'tbl[names].to_pandas(...)'\n            )\n            # fmt: on\n\n        out = OrderedDict()\n\n        for name, column in tbl.columns.items():\n            if getattr(column.dtype, \"isnative\", True):\n                out[name] = column\n            else:\n                out[name] = column.data.byteswap().newbyteorder(\"=\")\n\n            if isinstance(column, MaskedColumn) and np.any(column.mask):\n                if column.dtype.kind in [\"i\", \"u\"]:\n                    pd_dtype = column.dtype.name\n                    if use_nullable_int:\n                        # Convert int64 to Int64, uint32 to UInt32, etc for nullable types\n                        pd_dtype = pd_dtype.replace(\"i\", \"I\").replace(\"u\", \"U\")\n                    out[name] = Series(out[name], dtype=pd_dtype)\n\n                    # If pandas is older than 0.24 the type may have turned to float\n                    if column.dtype.kind != out[name].dtype.kind:\n                        warnings.warn(\n                            f\"converted column '{name}' from {column.dtype} to\"\n                            f\" {out[name].dtype}\",\n                            TableReplaceWarning,\n                            stacklevel=3,\n                        )\n                elif column.dtype.kind not in [\"f\", \"c\"]:\n                    out[name] = column.astype(object).filled(np.nan)\n\n        kwargs = {}\n\n        if index:\n            idx = out.pop(index)\n\n            kwargs[\"index\"] = idx\n\n            # We add the table index to Series inputs (MaskedColumn with int values) to override\n            # its default RangeIndex, see #11432\n            for v in out.values():\n                if isinstance(v, Series):\n                    v.index = idx\n\n        df = DataFrame(out, **kwargs)\n        if index:\n            # Explicitly set the pandas DataFrame index to the original table\n            # index name.", "metadata": {"file_name": "astropy/table/table.py", "File Name": "astropy/table/table.py", "Classes": "TableReplaceWarning, TableColumns, TableAttribute, PprintIncludeExclude, Table, QTable, _Context", "Functions": "descr, has_info_class, _get_names_from_list_of_dict, _encode_mixins"}}, {"code": "fast_reader = _get_fast_reader_dict(kwargs)\n    kwargs[\"fast_reader\"] = fast_reader\n\n    if fast_reader[\"enable\"] and fast_reader.get(\"chunk_size\"):\n        return _read_in_chunks(table, **kwargs)\n\n    if \"fill_values\" not in kwargs:\n        kwargs[\"fill_values\"] = [(\"\", \"0\")]\n\n    # If an Outputter is supplied in kwargs that will take precedence.\n    if (\n        \"Outputter\" in kwargs\n    ):  # user specified Outputter, not supported for fast reading\n        fast_reader[\"enable\"] = False\n\n    format = kwargs.get(\"format\")\n    # Dictionary arguments are passed by reference per default and thus need\n    # special protection:\n    new_kwargs = copy.deepcopy(kwargs)\n    kwargs[\"fast_reader\"] = copy.deepcopy(fast_reader)\n\n    # Get the Reader class based on possible format and Reader kwarg inputs.\n    Reader = _get_format_class(format, kwargs.get(\"Reader\"), \"Reader\")\n    if Reader is not None:\n        new_kwargs[\"Reader\"] = Reader\n        format = Reader._format_name\n\n    # Remove format keyword if there, this is only allowed in read() not get_reader()\n    if \"format\" in new_kwargs:\n        del new_kwargs[\"format\"]\n\n    if guess is None:\n        guess = _GUESS\n\n    if guess:\n        # If ``table`` is probably an HTML file then tell guess function to add\n        # the HTML reader at the top of the guess list.  This is in response to\n        # issue #3691 (and others) where libxml can segfault on a long non-HTML\n        # file, thus prompting removal of the HTML reader from the default\n        # guess list.\n        new_kwargs[\"guess_html\"] = _probably_html(table)\n\n        # If `table` is a filename or readable file object then read in the\n        # file now.  This prevents problems in Python 3 with the file object\n        # getting closed or left at the file end.", "metadata": {"file_name": "astropy/io/ascii/ui.py", "File Name": "astropy/io/ascii/ui.py", "Functions": "_probably_html, set_guess, get_reader, _get_format_class, _get_fast_reader_dict, _validate_read_write_kwargs, _expand_user_if_path, read, _guess, _get_guess_kwargs_list, _read_in_chunks, _read_in_chunks_generator, get_writer, write, get_read_trace, is_ducktype, passthrough_fileobj"}}, {"code": "for reader in (\n        fixedwidth.FixedWidthTwoLine,\n        rst.RST,\n        fastbasic.FastBasic,\n        basic.Basic,\n        fastbasic.FastRdb,\n        basic.Rdb,\n        fastbasic.FastTab,\n        basic.Tab,\n        cds.Cds,\n        mrt.Mrt,\n        daophot.Daophot,\n        sextractor.SExtractor,\n        ipac.Ipac,\n        latex.Latex,\n        latex.AASTex,\n    ):\n        guess_kwargs_list.append(dict(Reader=reader))\n\n    # Cycle through the basic-style readers using all combinations of delimiter\n    # and quotechar.\n    for Reader in (\n        fastbasic.FastCommentedHeader,\n        basic.CommentedHeader,\n        fastbasic.FastBasic,\n        basic.Basic,\n        fastbasic.FastNoHeader,\n        basic.NoHeader,\n    ):\n        for delimiter in (\"|\", \",\", \" \", r\"\\s\"):\n            for quotechar in ('\"', \"'\"):\n                guess_kwargs_list.append(\n                    dict(Reader=Reader, delimiter=delimiter, quotechar=quotechar)\n                )\n\n    return guess_kwargs_list\n\n\ndef _read_in_chunks(table, **kwargs):\n    \"\"\"\n    For fast_reader read the ``table`` in chunks and vstack to create\n    a single table, OR return a generator of chunk tables.\n    \"\"\"\n    fast_reader = kwargs[\"fast_reader\"]\n    chunk_size = fast_reader.pop(\"chunk_size\")\n    chunk_generator = fast_reader.pop(\"chunk_generator\", False)\n    fast_reader[\"parallel\"] = False  # No parallel with chunks\n\n    tbl_chunks = _read_in_chunks_generator(table, chunk_size, **kwargs)\n    if chunk_generator:\n        return tbl_chunks\n\n    tbl0 = next(tbl_chunks)\n    masked = tbl0.masked\n\n    # Numpy won't allow resizing the original so make a copy here.", "metadata": {"file_name": "astropy/io/ascii/ui.py", "File Name": "astropy/io/ascii/ui.py", "Functions": "_probably_html, set_guess, get_reader, _get_format_class, _get_fast_reader_dict, _validate_read_write_kwargs, _expand_user_if_path, read, _guess, _get_guess_kwargs_list, _read_in_chunks, _read_in_chunks_generator, get_writer, write, get_read_trace, is_ducktype, passthrough_fileobj"}}, {"code": "Some objects may not\n        # have an info attribute. Also avoid creating info as a side effect.\n        if not name:\n            if isinstance(data, Column):\n                name = data.name or default_name\n            elif \"info\" in getattr(data, \"__dict__\", ()):\n                name = data.info.name or default_name\n            else:\n                name = default_name\n\n        if isinstance(data, Column):\n            # If self.ColumnClass is a subclass of col, then \"upgrade\" to ColumnClass,\n            # otherwise just use the original class.  The most common case is a\n            # table with masked=True and ColumnClass=MaskedColumn.  Then a Column\n            # gets upgraded to MaskedColumn, but the converse (pre-4.0) behavior\n            # of downgrading from MaskedColumn to Column (for non-masked table)\n            # does not happen.\n            col_cls = self._get_col_cls_for_table(data)\n\n        elif data_is_mixin:\n            # Copy the mixin column attributes if they exist since the copy below\n            # may not get this attribute. If not copying, take a slice\n            # to ensure we get a new instance and we do not share metadata\n            # like info.\n            col = col_copy(data, copy_indices=self._init_indices) if copy else data[:]\n            col.info.name = name\n            return col\n\n        elif data0_is_mixin:\n            # Handle case of a sequence of a mixin, e.g. [1*u.m, 2*u.m].\n            try:\n                col = data[0].__class__(data)\n                col.info.name = name\n                return col\n            except Exception:\n                # If that didn't work for some reason, just turn it into np.array of object\n                data = np.array(data, dtype=object)\n                col_cls = self.ColumnClass\n\n        elif isinstance(data, (np.ma.MaskedArray, Masked)):\n            # Require that col_cls be a subclass of MaskedColumn, remembering\n            # that ColumnClass could be a user-defined subclass (though more-likely\n            # could be MaskedColumn).", "metadata": {"file_name": "astropy/table/table.py", "File Name": "astropy/table/table.py", "Classes": "TableReplaceWarning, TableColumns, TableAttribute, PprintIncludeExclude, Table, QTable, _Context", "Functions": "descr, has_info_class, _get_names_from_list_of_dict, _encode_mixins"}}, {"code": "# Licensed under a 3-clause BSD style license - see LICENSE.rst\n\"\"\"An extensible ASCII table reader and writer.\n\nbasic.py:\n  Basic table read / write functionality for simple character\n  delimited files with various options for column header definition.\n\n:Copyright: Smithsonian Astrophysical Observatory (2011)\n:Author: Tom Aldcroft (aldcroft@head.cfa.harvard.edu)\n\"\"\"\n\n\nimport re\n\nfrom . import core\n\n\nclass BasicHeader(core.BaseHeader):\n    \"\"\"\n    Basic table Header Reader.\n\n    Set a few defaults for common ascii table formats\n    (start at line 0, comments begin with ``#`` and possibly white space)\n    \"\"\"\n\n    start_line = 0\n    comment = r\"\\s*#\"\n    write_comment = \"# \"\n\n\nclass BasicData(core.BaseData):\n    \"\"\"\n    Basic table Data Reader.\n\n    Set a few defaults for common ascii table formats\n    (start at line 1, comments begin with ``#`` and possibly white space)\n    \"\"\"\n\n    start_line = 1\n    comment = r\"\\s*#\"\n    write_comment = \"# \"\n\n\nclass Basic(core.BaseReader):\n    r\"\"\"Character-delimited table with a single header line at the top.\n\n    Lines beginning with a comment character (default='#') as the first\n    non-whitespace character are comments.\n\n    Example table::\n\n      # Column definition is the first uncommented line\n      # Default delimiter is the space character.\n      apples oranges pears\n\n      # Data starts after the header column definition, blank lines ignored\n      1 2 3\n      4 5 6\n    \"\"\"\n\n    _format_name = \"basic\"\n    _description = \"Basic table with custom delimiters\"\n    _io_registry_format_aliases = [\"ascii\"]\n\n    header_class = BasicHeader\n    data_class = BasicData", "metadata": {"file_name": "astropy/io/ascii/basic.py", "File Name": "astropy/io/ascii/basic.py", "Classes": "BasicHeader, BasicData, Basic, NoHeaderHeader, NoHeaderData, NoHeader, CommentedHeaderHeader, CommentedHeader, TabHeaderSplitter, TabDataSplitter, TabHeader, TabData, Tab, CsvSplitter, CsvHeader, CsvData, Csv, RdbHeader, RdbData, Rdb"}}, {"code": "Examples\n        --------\n        Create three columns with different types:\n\n            >>> t = Table([[1, 4, 5], [-25.55, 12.123, 85],\n            ...     ['a', 'b', 'c']], names=('a', 'b', 'c'))\n            >>> print(t)\n             a    b     c\n            --- ------ ---\n              1 -25.55   a\n              4 12.123   b\n              5   85.0   c\n\n        Round them all to 0:\n\n            >>> t.round(0)\n            >>> print(t)\n             a    b    c\n            --- ----- ---\n              1 -26.0   a\n              4  12.0   b\n              5  85.0   c\n\n        Round column 'a' to -1 decimal:\n\n            >>> t.round({'a':-1})\n            >>> print(t)\n             a    b    c\n            --- ----- ---\n              0 -26.0   a\n              0  12.0   b\n              0  85.0   c\n\n        Parameters\n        ----------\n        decimals: int, dict\n            Number of decimals to round the columns to. If a dict is given,\n            the columns will be rounded to the number specified as the value.\n            If a certain column is not in the dict given, it will remain the\n            same.\n        \"\"\"", "metadata": {"file_name": "astropy/table/table.py", "File Name": "astropy/table/table.py", "Classes": "TableReplaceWarning, TableColumns, TableAttribute, PprintIncludeExclude, Table, QTable, _Context", "Functions": "descr, has_info_class, _get_names_from_list_of_dict, _encode_mixins"}}, {"code": "class RST(FixedWidth):\n    \"\"\"reStructuredText simple format table.\n\n    See: https://docutils.sourceforge.io/docs/ref/rst/restructuredtext.html#simple-tables\n\n    Example::\n\n      >>> from astropy.table import QTable\n      >>> import astropy.units as u\n      >>> import sys\n      >>> tbl = QTable({\"wave\": [350, 950] * u.nm, \"response\": [0.7, 1.2] * u.count})\n      >>> tbl.write(sys.stdout,  format=\"ascii.rst\")\n      ===== ========\n       wave response\n      ===== ========\n      350.0      0.7\n      950.0      1.2\n      ===== ========\n\n    Like other fixed-width formats, when writing a table you can provide ``header_rows``\n    to specify a list of table rows to output as the header.  For example::\n\n      >>> tbl.write(sys.stdout,  format=\"ascii.rst\", header_rows=['name', 'unit'])\n      ===== ========\n       wave response\n         nm       ct\n      ===== ========\n      350.0      0.7\n      950.0      1.2\n      ===== ========\n\n    Currently there is no support for reading tables which utilize continuation lines,\n    or for ones which define column spans through the use of an additional\n    line of dashes in the header.\n\n    \"\"\"\n\n    _format_name = \"rst\"\n    _description = \"reStructuredText simple table\"\n    data_class = SimpleRSTData\n    header_class = SimpleRSTHeader\n\n    def __init__(self, header_rows=None):\n        super().__init__(delimiter_pad=None, bookend=False, header_rows=header_rows)\n\n    def write(self, lines):\n        lines = super().write(lines)\n        idx = len(self.header.header_rows)\n        lines = [lines[idx]] + lines + [lines[idx]]\n        return lines\n\n    def read(self, table):\n        self.data.start_line = 2 + len(self.header.header_rows)\n        return super().read(table)", "metadata": {"file_name": "astropy/io/ascii/rst.py", "File Name": "astropy/io/ascii/rst.py", "Classes": "SimpleRSTHeader, SimpleRSTData, RST"}}, {"code": "from .fastbasic import FastBasic\n\n    if issubclass(Reader, FastBasic):  # Fast readers handle args separately\n        if Inputter is not None:\n            kwargs[\"Inputter\"] = Inputter\n        return Reader(**kwargs)\n\n    # If user explicitly passed a fast reader with enable='force'\n    # (e.g. by passing non-default options), raise an error for slow readers\n    if \"fast_reader\" in kwargs:\n        if kwargs[\"fast_reader\"][\"enable\"] == \"force\":\n            raise ParameterError(\n                \"fast_reader required with \"\n                \"{}, but this is not a fast C reader: {}\".format(\n                    kwargs[\"fast_reader\"], Reader\n                )\n            )\n        else:\n            del kwargs[\"fast_reader\"]  # Otherwise ignore fast_reader parameter\n\n    reader_kwargs = {k: v for k, v in kwargs.items() if k not in extra_reader_pars}\n    reader = Reader(**reader_kwargs)\n\n    if Inputter is not None:\n        reader.inputter = Inputter()\n\n    if Outputter is not None:\n        reader.outputter = Outputter()\n\n    # Issue #855 suggested to set data_start to header_start + default_header_length\n    # Thus, we need to retrieve this from the class definition before resetting these numbers.\n    try:\n        default_header_length = reader.data.start_line - reader.header.start_line\n    except TypeError:  # Start line could be None or an instancemethod\n        default_header_length = None\n\n    # csv.reader is hard-coded to recognise either '\\r' or '\\n' as end-of-line,\n    # therefore DefaultSplitter cannot handle these as delimiters.", "metadata": {"file_name": "astropy/io/ascii/core.py", "File Name": "astropy/io/ascii/core.py", "Classes": "CsvWriter, MaskedConstant, InconsistentTableError, OptionalTableImportError, ParameterError, FastOptionsError, NoType, StrType, NumType, FloatType, BoolType, IntType, AllType, Column, BaseInputter, BaseSplitter, DefaultSplitter, BaseHeader, BaseData, BaseOutputter, TableOutputter, MetaBaseReader, BaseReader, ContinuationLinesInputter, WhitespaceSplitter", "Functions": "_check_multidim_table, _replace_tab_with_space, _get_line_index, convert_numpy, _deduplicate_names, _is_number, _apply_include_exclude_names, _get_reader, _get_writer, bool_converter, generic_converter, rename_columns"}}, {"code": "allowed_keys = \"f br<>qhpn\"\n\n        # Count the header lines\n        n_header = 0\n        if show_name:\n            n_header += 1\n        if show_unit:\n            n_header += 1\n        if show_dtype:\n            n_header += 1\n        if show_name or show_unit or show_dtype:\n            n_header += 1\n\n        # Set up kwargs for pformat call.  Only Table gets max_width.\n        kwargs = dict(\n            max_lines=-1,\n            show_name=show_name,\n            show_unit=show_unit,\n            show_dtype=show_dtype,\n        )\n        if hasattr(tabcol, \"columns\"):  # tabcol is a table\n            kwargs[\"max_width\"] = max_width\n\n        # If max_lines is None (=> query screen size) then increase by 2.\n        # This is because get_pprint_size leaves 6 extra lines so that in\n        # ipython you normally see the last input line.\n        max_lines1, max_width = self._get_pprint_size(max_lines, max_width)\n        if max_lines is None:\n            max_lines1 += 2\n        delta_lines = max_lines1 - n_header\n\n        # Set up a function to get a single character on any platform\n        inkey = Getch()\n\n        i0 = 0  # First table/column row to show\n        showlines = True\n        while True:\n            i1 = i0 + delta_lines  # Last table/col row to show\n            if showlines:  # Don't always show the table (e.g.", "metadata": {"file_name": "astropy/table/pprint.py", "File Name": "astropy/table/pprint.py", "Classes": "TableFormatter", "Functions": "default_format_func, _use_str_for_masked_values, _possible_string_format_functions, get_auto_format_func, _get_pprint_include_names, _auto_format_func, get_matches, format_col_str, outwidth"}}, {"code": "if col.shape == () and len(self) == 0:\n            raise TypeError(\"Empty table cannot have column set to scalar value\")\n        # Make col data shape correct for scalars.  The second test is to allow\n        # broadcasting an N-d element to a column, e.g. t['new'] = [[1, 2]].\n        elif (col.shape == () or col.shape[0] == 1) and len(self) > 0:\n            new_shape = (len(self),) + getattr(col, \"shape\", ())[1:]\n            if isinstance(col, np.ndarray):\n                col = np.broadcast_to(col, shape=new_shape, subok=True)\n            elif isinstance(col, ShapedLikeNDArray):\n                col = col._apply(np.broadcast_to, shape=new_shape, subok=True)\n\n            # broadcast_to() results in a read-only array.  Apparently it only changes\n            # the view to look like the broadcasted array.  So copy.", "metadata": {"file_name": "astropy/table/table.py", "File Name": "astropy/table/table.py", "Classes": "TableReplaceWarning, TableColumns, TableAttribute, PprintIncludeExclude, Table, QTable, _Context", "Functions": "descr, has_info_class, _get_names_from_list_of_dict, _encode_mixins"}}, {"code": "names_from_data = set()\n        for row in data:\n            names_from_data.update(row)\n\n        if set(data[0].keys()) == names_from_data:\n            names_from_data = list(data[0].keys())\n        else:\n            names_from_data = sorted(names_from_data)\n\n        # Note: if set(data[0].keys()) != names_from_data, this will give an\n        # exception later, so NO need to catch here.\n\n        # Convert list of dict into dict of list (cols), keep track of missing\n        # indexes and put in MISSING placeholders in the `cols` lists.\n        cols = {}\n        missing_indexes = defaultdict(list)\n        for name in names_from_data:\n            cols[name] = []\n            for ii, row in enumerate(data):\n                try:\n                    val = row[name]\n                except KeyError:\n                    missing_indexes[name].append(ii)\n                    val = MISSING\n                cols[name].append(val)\n\n        # Fill the missing entries with first values\n        if missing_indexes:\n            for name, indexes in missing_indexes.items():\n                col = cols[name]\n                first_val = next(val for val in col if val is not MISSING)\n                for index in indexes:\n                    col[index] = first_val\n\n        # prepare initialization\n        if all(name is None for name in names):\n            names = names_from_data\n\n        self._init_from_dict(cols, names, dtype, n_cols, copy)\n\n        # Mask the missing values if necessary, converting columns to MaskedColumn\n        # as needed.\n        if missing_indexes:\n            for name, indexes in missing_indexes.items():\n                col = self[name]\n                # Ensure that any Column subclasses with MISSING values can support\n                # setting masked values. As of astropy 4.0 the test condition below is\n                # always True since _init_from_dict cannot result in mixin columns.\n                if isinstance(col, Column) and not isinstance(col, MaskedColumn):\n                    self[name] = self.MaskedColumn(col, copy=False)\n\n                # Finally do the masking in a mixin-safe way.", "metadata": {"file_name": "astropy/table/table.py", "File Name": "astropy/table/table.py", "Classes": "TableReplaceWarning, TableColumns, TableAttribute, PprintIncludeExclude, Table, QTable, _Context", "Functions": "descr, has_info_class, _get_names_from_list_of_dict, _encode_mixins"}}, {"code": ">>> import pandas as pd\n          >>> from astropy.table import QTable\n          >>> import astropy.units as u\n          >>> from astropy.time import Time, TimeDelta\n          >>> from astropy.coordinates import SkyCoord\n\n          >>> q = [1, 2] * u.m\n          >>> tm = Time([1998, 2002], format='jyear')\n          >>> sc = SkyCoord([5, 6], [7, 8], unit='deg')\n          >>> dt = TimeDelta([3, 200] * u.s)\n\n          >>> t = QTable([q, tm, sc, dt], names=['q', 'tm', 'sc', 'dt'])\n\n          >>> df = t.to_pandas(index='tm')\n          >>> with pd.option_context('display.max_columns', 20):\n          ...     print(df)\n                        q  sc.ra  sc.dec              dt\n          tm\n          1998-01-01  1.0    5.0     7.0 0 days 00:00:03\n          2002-01-01  2.0    6.0     8.0 0 days 00:03:20\n\n        \"\"\"\n        from pandas import DataFrame, Series\n\n        if index is not False:\n            if index in (None, True):\n                # Default is to use the table primary key if available and a single column\n                if self.primary_key and len(self.primary_key) == 1:\n                    index = self.primary_key[0]\n                else:\n                    index = False\n            else:\n                if index not in self.colnames:\n                    raise ValueError(\n                        \"index must be None, False, True or a table column name\"\n                    )\n\n        def _encode_mixins(tbl):\n            \"\"\"Encode a Table ``tbl`` that may have mixin columns to a Table with only\n            astropy Columns + appropriate meta-data to allow subsequent decoding.\n            \"\"\"\n            from astropy.time import TimeBase, TimeDelta\n\n            from .", "metadata": {"file_name": "astropy/table/table.py", "File Name": "astropy/table/table.py", "Classes": "TableReplaceWarning, TableColumns, TableAttribute, PprintIncludeExclude, Table, QTable, _Context", "Functions": "descr, has_info_class, _get_names_from_list_of_dict, _encode_mixins"}}, {"code": "if self.process_line:\n            lines = [self.process_line(x) for x in lines]\n\n        delimiter = \" \" if self.delimiter == r\"\\s\" else self.delimiter\n\n        csv_reader = csv.reader(\n            lines,\n            delimiter=delimiter,\n            doublequote=self.doublequote,\n            escapechar=self.escapechar,\n            quotechar=self.quotechar,\n            quoting=self.quoting,\n            skipinitialspace=self.skipinitialspace,\n        )\n        for vals in csv_reader:\n            if self.process_val:\n                yield [self.process_val(x) for x in vals]\n            else:\n                yield vals\n\n    def join(self, vals):\n        delimiter = \" \" if self.delimiter is None else str(self.delimiter)\n\n        if self.csv_writer is None:\n            self.csv_writer = CsvWriter(\n                delimiter=delimiter,\n                doublequote=self.doublequote,\n                escapechar=self.escapechar,\n                quotechar=self.quotechar,\n                quoting=self.quoting,\n            )\n        if self.process_val:\n            vals = [self.process_val(x) for x in vals]\n        out = self.csv_writer.writerow(vals).rstrip(\"\\r\\n\")\n\n        return out", "metadata": {"file_name": "astropy/io/ascii/core.py", "File Name": "astropy/io/ascii/core.py", "Classes": "CsvWriter, MaskedConstant, InconsistentTableError, OptionalTableImportError, ParameterError, FastOptionsError, NoType, StrType, NumType, FloatType, BoolType, IntType, AllType, Column, BaseInputter, BaseSplitter, DefaultSplitter, BaseHeader, BaseData, BaseOutputter, TableOutputter, MetaBaseReader, BaseReader, ContinuationLinesInputter, WhitespaceSplitter", "Functions": "_check_multidim_table, _replace_tab_with_space, _get_line_index, convert_numpy, _deduplicate_names, _is_number, _apply_include_exclude_names, _get_reader, _get_writer, bool_converter, generic_converter, rename_columns"}}, {"code": "def _guess(table, read_kwargs, format, fast_reader):\n    \"\"\"\n    Try to read the table using various sets of keyword args.  Start with the\n    standard guess list and filter to make it unique and consistent with\n    user-supplied read keyword args.  Finally, if none of those work then\n    try the original user-supplied keyword args.\n\n    Parameters\n    ----------\n    table : str, file-like, list\n        Input table as a file name, file-like object, list of strings, or\n        single newline-separated string.\n    read_kwargs : dict\n        Keyword arguments from user to be supplied to reader\n    format : str\n        Table format\n    fast_reader : dict\n        Options for the C engine fast reader.  See read() function for details.\n\n    Returns\n    -------\n    dat : `~astropy.table.Table` or None\n        Output table or None if only one guess format was available\n    \"\"\"\n    # Keep a trace of all failed guesses kwarg\n    failed_kwargs = []\n\n    # Get an ordered list of read() keyword arg dicts that will be cycled\n    # through in order to guess the format.\n    full_list_guess = _get_guess_kwargs_list(read_kwargs)\n\n    # If a fast version of the reader is available, try that before the slow version\n    if (\n        fast_reader[\"enable\"]\n        and format is not None\n        and f\"fast_{format}\" in core.FAST_CLASSES\n    ):\n        fast_kwargs = copy.deepcopy(read_kwargs)\n        fast_kwargs[\"Reader\"] = core.FAST_CLASSES[f\"fast_{format}\"]\n        full_list_guess = [fast_kwargs] + full_list_guess\n    else:\n        fast_kwargs = None\n\n    # Filter the full guess list so that each entry is consistent with user kwarg inputs.\n    # This also removes any duplicates from the list.", "metadata": {"file_name": "astropy/io/ascii/ui.py", "File Name": "astropy/io/ascii/ui.py", "Functions": "_probably_html, set_guess, get_reader, _get_format_class, _get_fast_reader_dict, _validate_read_write_kwargs, _expand_user_if_path, read, _guess, _get_guess_kwargs_list, _read_in_chunks, _read_in_chunks_generator, get_writer, write, get_read_trace, is_ducktype, passthrough_fileobj"}}, {"code": "Parameters\n    ----------\n    definition : str, optional\n        Specify the convention for characters in the data table that occur\n        directly below the pipe (``|``) symbol in the header column definition:\n\n          * 'ignore' - Any character beneath a pipe symbol is ignored (default)\n          * 'right' - Character is associated with the column to the right\n          * 'left' - Character is associated with the column to the left\n\n    DBMS : bool, optional\n        If true, this verifies that written tables adhere (semantically)\n        to the `IPAC/DBMS\n        <https://irsa.ipac.caltech.edu/applications/DDGEN/Doc/DBMSrestriction.html>`_\n        definition of IPAC tables. If 'False' it only checks for the (less strict)\n        `IPAC <https://irsa.ipac.caltech.edu/applications/DDGEN/Doc/ipac_tbl.html>`_\n        definition.\n    \"\"\"\n\n    _format_name = \"ipac\"\n    _io_registry_format_aliases = [\"ipac\"]\n    _io_registry_can_write = True\n    _description = \"IPAC format table\"\n\n    data_class = IpacData\n    header_class = IpacHeader\n\n    def __init__(self, definition=\"ignore\", DBMS=False):\n        super().__init__()\n        # Usually the header is not defined in __init__, but here it need a keyword\n        if definition in [\"ignore\", \"left\", \"right\"]:\n            self.header.ipac_definition = definition\n        else:\n            raise ValueError(\"definition should be one of ignore/left/right\")\n        self.header.DBMS = DBMS\n\n    def write(self, table):\n        \"\"\"\n        Write ``table`` as list of strings.\n\n        Parameters\n        ----------\n        table : `~astropy.table.Table`\n            Input table data\n\n        Returns\n        -------\n        lines : list\n            List of strings corresponding to ASCII table\n\n        \"\"\"\n        # Set a default null value for all columns by adding at the end, which\n        # is the position with the lowest priority.", "metadata": {"file_name": "astropy/io/ascii/ipac.py", "File Name": "astropy/io/ascii/ipac.py", "Classes": "IpacFormatErrorDBMS, IpacFormatError, IpacHeaderSplitter, IpacHeader, IpacDataSplitter, IpacData, Ipac", "Functions": "process_keyword_value"}}, {"code": "try:\n            # Don't allow list-like things that dtype accepts\n            assert type(converters) is type\n            converters = [numpy.dtype(converters)]\n        except (AssertionError, TypeError):\n            pass\n\n        converters_out = []\n        try:\n            for converter in converters:\n                try:\n                    converter_func, converter_type = converter\n                except TypeError as err:\n                    if str(err).startswith(\"cannot unpack\"):\n                        converter_func, converter_type = convert_numpy(converter)\n                    else:\n                        raise\n                if not issubclass(converter_type, NoType):\n                    raise ValueError(\"converter_type must be a subclass of NoType\")\n                if issubclass(converter_type, col.type):\n                    converters_out.append((converter_func, converter_type))\n\n        except (ValueError, TypeError) as err:\n            raise ValueError(\n                \"Error: invalid format for converters, see \"\n                f\"documentation\\n{converters}: {err}\"\n            )\n        return converters_out\n\n    def _convert_vals(self, cols):\n        for col in cols:\n            for key, converters in self.converters.items():\n                if fnmatch.fnmatch(col.name, key):\n                    break\n            else:\n                if col.dtype is not None:\n                    converters = [convert_numpy(col.dtype)]\n                else:\n                    converters = self.default_converters\n\n            col.converters = self._validate_and_copy(col, converters)\n\n            # Catch the last error in order to provide additional information\n            # in case all attempts at column conversion fail.  The initial\n            # value of of last_error will apply if no converters are defined\n            # and the first col.converters[0] access raises IndexError.", "metadata": {"file_name": "astropy/io/ascii/core.py", "File Name": "astropy/io/ascii/core.py", "Classes": "CsvWriter, MaskedConstant, InconsistentTableError, OptionalTableImportError, ParameterError, FastOptionsError, NoType, StrType, NumType, FloatType, BoolType, IntType, AllType, Column, BaseInputter, BaseSplitter, DefaultSplitter, BaseHeader, BaseData, BaseOutputter, TableOutputter, MetaBaseReader, BaseReader, ContinuationLinesInputter, WhitespaceSplitter", "Functions": "_check_multidim_table, _replace_tab_with_space, _get_line_index, convert_numpy, _deduplicate_names, _is_number, _apply_include_exclude_names, _get_reader, _get_writer, bool_converter, generic_converter, rename_columns"}}, {"code": "See: http://vizier.u-strasbg.fr/doc/catstd.htx\n\n    Example::\n\n      Table: Table name here\n      = ==============================================================================\n      Catalog reference paper\n          Bibliography info here\n      ================================================================================\n      ADC_Keywords: Keyword ; Another keyword ; etc\n\n      Description:\n          Catalog description here.\n      ================================================================================\n      Byte-by-byte Description of file: datafile3.txt\n      --------------------------------------------------------------------------------\n         Bytes Format Units  Label  Explanations\n      --------------------------------------------------------------------------------\n         1-  3 I3     ---    Index  Running identification number\n         5-  6 I2     h      RAh    Hour of Right Ascension (J2000)\n         8-  9 I2     min    RAm    Minute of Right Ascension (J2000)\n        11- 15 F5.2   s      RAs    Second of Right Ascension (J2000)\n      --------------------------------------------------------------------------------\n      Note (1): A CDS file can contain sections with various metadata.\n                Notes can be multiple lines.\n      Note (2): Another note.\n      --------------------------------------------------------------------------------\n        1 03 28 39.09\n        2 04 18 24.11\n\n    **About parsing the CDS format**\n\n    The CDS format consists of a table description and the table data.  These\n    can be in separate files as a ``ReadMe`` file plus data file(s), or\n    combined in a single file.  Different subsections within the description\n    are separated by lines of dashes or equal signs (\"------\" or \"======\").\n    The table which specifies the column information must be preceded by a line\n    starting with \"Byte-by-byte Description of file:\".\n\n    In the case where the table description is combined with the data values,\n    the data must be in the last section and must be preceded by a section\n    delimiter line (dashes or equal signs only).\n\n    **Basic usage**\n\n    Use the ``ascii.read()`` function as normal, with an optional ``readme``\n    parameter indicating the CDS ReadMe file.  If not supplied it is assumed that\n    the header information is at the top of the given table.", "metadata": {"file_name": "astropy/io/ascii/cds.py", "File Name": "astropy/io/ascii/cds.py", "Classes": "CdsHeader, CdsData, Cds"}}, {"code": "from_pandas(df)\n          <QTable length=2>\n                    time              dt       x\n                    Time          TimeDelta float64\n          ----------------------- --------- -------\n          1998-01-01T00:00:00.000       1.0     3.0\n          2002-01-01T00:00:00.000     300.0     4.0\n\n        \"\"\"\n        out = OrderedDict()\n\n        names = list(dataframe.columns)\n        columns = [dataframe[name] for name in names]\n        datas = [np.array(column) for column in columns]\n        masks = [np.array(column.isnull()) for column in columns]\n\n        if index:\n            index_name = dataframe.index.name or \"index\"\n            while index_name in names:\n                index_name = \"_\" + index_name + \"_\"\n            names.insert(0, index_name)\n            columns.insert(0, dataframe.index)\n            datas.insert(0, np.array(dataframe.index))\n            masks.insert(0, np.zeros(len(dataframe), dtype=bool))\n\n        if units is None:\n            units = [None] * len(names)\n        else:\n            if not isinstance(units, Mapping):\n                raise TypeError('Expected a Mapping \"column-name\" -> \"unit\"')\n\n            not_found = set(units.keys()) - set(names)\n            if not_found:\n                warnings.warn(f\"`units` contains additional columns: {not_found}\")\n\n            units = [units.get(name) for name in names]\n\n        for name, column, data, mask, unit in zip(names, columns, datas, masks, units):\n            if column.dtype.kind in [\"u\", \"i\"] and np.any(mask):\n                # Special-case support for pandas nullable int\n                np_dtype = str(column.dtype).lower()\n                data = np.zeros(shape=column.shape, dtype=np_dtype)\n                data[~mask] = column[~mask]\n                out[name] = MaskedColumn(\n                    data=data, name=name, mask=mask, unit=unit, copy=False\n                )\n                continue\n\n            if data.", "metadata": {"file_name": "astropy/table/table.py", "File Name": "astropy/table/table.py", "Classes": "TableReplaceWarning, TableColumns, TableAttribute, PprintIncludeExclude, Table, QTable, _Context", "Functions": "descr, has_info_class, _get_names_from_list_of_dict, _encode_mixins"}}, {"code": "Parameters\n        ----------\n        row_specifier : slice or int or array of int\n            Specification for rows to remove\n\n        Examples\n        --------\n        Create a table with three columns 'a', 'b' and 'c'::\n\n            >>> t = Table([[1, 2, 3], [0.1, 0.2, 0.3], ['x', 'y', 'z']],\n            ...           names=('a', 'b', 'c'))\n            >>> print(t)\n             a   b   c\n            --- --- ---\n              1 0.1   x\n              2 0.2   y\n              3 0.3   z\n\n        Remove rows 0 and 2 from the table::\n\n            >>> t.remove_rows([0, 2])\n            >>> print(t)\n             a   b   c\n            --- --- ---\n              2 0.2   y", "metadata": {"file_name": "astropy/table/table.py", "File Name": "astropy/table/table.py", "Classes": "TableReplaceWarning, TableColumns, TableAttribute, PprintIncludeExclude, Table, QTable, _Context", "Functions": "descr, has_info_class, _get_names_from_list_of_dict, _encode_mixins"}}, {"code": "def showtable(filename, args):\n    \"\"\"\n    Read a table and print to the standard output.\n\n    Parameters\n    ----------\n    filename : str\n        The path to a FITS file.\n\n    \"\"\"\n    if args.info and args.stats:\n        warnings.warn(\"--info and --stats cannot be used together\", AstropyUserWarning)\n    if any((args.max_lines, args.max_width, args.hide_unit, args.show_dtype)) and (\n        args.info or args.stats\n    ):\n        warnings.warn(\n            \"print parameters are ignored if --info or --stats is used\",\n            AstropyUserWarning,\n        )\n\n    # these parameters are passed to Table.read if they are specified in the\n    # command-line\n    read_kwargs = (\"hdu\", \"format\", \"table_id\", \"delimiter\")\n    kwargs = {k: v for k, v in vars(args).items() if k in read_kwargs and v is not None}\n    try:\n        table = Table.read(filename, **kwargs)\n        if args.info:\n            table.info(\"attributes\")\n        elif args.stats:\n            table.info(\"stats\")\n        else:\n            formatter = table.more if args.more else table.pprint\n            formatter(\n                max_lines=args.max_lines,\n                max_width=args.max_width,\n                show_unit=(False if args.hide_unit else None),\n                show_dtype=(True if args.show_dtype else None),\n            )\n    except OSError as e:\n        log.error(str(e))\n\n\ndef main(args=None):\n    \"\"\"The main function called by the `showtable` script.\"\"\"\n    parser = argparse.ArgumentParser(\n        description=textwrap.dedent(\n            \"\"\"\n            Print tables from ASCII, FITS, HDF5, VOTable file(s).  The tables\n            are read with 'astropy.table.Table.read' and are printed with\n            'astropy.table.Table.pprint'. The default behavior is to make the\n            table output fit onto a single screen page.  For a long and wide\n            table this will mean cutting out inner rows and columns.  To print\n            **all** the rows or columns use ``--max-lines=-1`` or\n            ``max-width=-1``, respectively. The complete list of supported\n            formats can be found at\n            http://astropy.readthedocs.io/en/latest/io/unified.", "metadata": {"file_name": "astropy/table/scripts/showtable.py", "File Name": "astropy/table/scripts/showtable.py", "Functions": "showtable, main"}}, {"code": "lines = []\n            comment_lines = 0\n            for line in f:\n                line = line.strip()\n                if in_header:\n                    lines.append(line)\n                    if line.startswith((\"------\", \"=======\")):\n                        comment_lines += 1\n                        if comment_lines == 3:\n                            break\n                else:\n                    match = re.match(\n                        r\"Byte-by-byte Description of file: (?P<name>.+)$\",\n                        line,\n                        re.IGNORECASE,\n                    )\n                    if match:\n                        # Split 'name' in case in contains multiple files\n                        names = [s for s in re.split(\"[, ]+\", match.group(\"name\")) if s]\n                        # Iterate on names to find if one matches the tablename\n                        # including wildcards.\n                        for pattern in names:\n                            if fnmatch.fnmatch(self.data.table_name, pattern):\n                                in_header = True\n                                lines.append(line)\n                                break\n\n            else:\n                raise core.InconsistentTableError(\n                    f\"Can't find table {self.data.table_name} in {self.readme}\"\n                )\n\n        found_line = False\n\n        for i_col_def, line in enumerate(lines):\n            if re.match(r\"Byte-by-byte Description\", line, re.IGNORECASE):\n                found_line = True\n            elif found_line:  # First line after list of file descriptions\n                i_col_def -= 1  # Set i_col_def to last description line\n                break\n        else:\n            raise ValueError('no line with \"Byte-by-byte Description\" found')\n\n        re_col_def = re.compile(\n            r\"\"\"\\s*\n                (?P<start> \\d+ \\s* -)? \\s*\n                (?P<end>   \\d+)        \\s+\n                (?P<format> [\\w.]+)     \\s+\n                (?P<units> \\S+)        \\s+\n                (?P<name>  \\S+)\n                (\\s+ (?P<descr> \\S.*))?", "metadata": {"file_name": "astropy/io/ascii/cds.py", "File Name": "astropy/io/ascii/cds.py", "Classes": "CdsHeader, CdsData, Cds"}}, {"code": "xxxs = \"x\" * max(len(name) for name in list(names) + list(table.colnames))\n        for ii, colname in enumerate(table.colnames):\n            table.rename_column(colname, xxxs + str(ii))\n\n        for ii, name in enumerate(names):\n            table.rename_column(xxxs + str(ii), name)\n\n    if names is not None:\n        rename_columns(table, names)\n    else:\n        colnames_uniq = _deduplicate_names(table.colnames)\n        if colnames_uniq != list(table.colnames):\n            rename_columns(table, colnames_uniq)\n\n    names_set = set(table.colnames)\n\n    if include_names is not None:\n        names_set.intersection_update(include_names)\n    if exclude_names is not None:\n        names_set.difference_update(exclude_names)\n    if names_set != set(table.colnames):\n        remove_names = set(table.colnames) - names_set\n        table.remove_columns(remove_names)\n\n\nclass BaseReader(metaclass=MetaBaseReader):\n    \"\"\"Class providing methods to read and write an ASCII table using the specified\n    header, data, inputter, and outputter instances.\n\n    Typical usage is to instantiate a Reader() object and customize the\n    ``header``, ``data``, ``inputter``, and ``outputter`` attributes.  Each\n    of these is an object of the corresponding class.\n\n    There is one method ``inconsistent_handler`` that can be used to customize the\n    behavior of ``read()`` in the event that a data row doesn't match the header.\n    The default behavior is to raise an InconsistentTableError.\n\n    \"\"\"", "metadata": {"file_name": "astropy/io/ascii/core.py", "File Name": "astropy/io/ascii/core.py", "Classes": "CsvWriter, MaskedConstant, InconsistentTableError, OptionalTableImportError, ParameterError, FastOptionsError, NoType, StrType, NumType, FloatType, BoolType, IntType, AllType, Column, BaseInputter, BaseSplitter, DefaultSplitter, BaseHeader, BaseData, BaseOutputter, TableOutputter, MetaBaseReader, BaseReader, ContinuationLinesInputter, WhitespaceSplitter", "Functions": "_check_multidim_table, _replace_tab_with_space, _get_line_index, convert_numpy, _deduplicate_names, _is_number, _apply_include_exclude_names, _get_reader, _get_writer, bool_converter, generic_converter, rename_columns"}}, {"code": "Default is True.\n        rename_duplicate : bool\n            Uniquify new column names if they duplicate the existing ones.\n            Default is False.\n\n        See Also\n        --------\n        astropy.table.hstack, update, replace_column\n\n        Examples\n        --------\n        Create a table with two columns 'a' and 'b', then create columns 'c' and 'd'\n        and append them to the end of the table::\n\n            >>> t = Table([[1, 2], [0.1, 0.2]], names=('a', 'b'))\n            >>> col_c = Column(name='c', data=['x', 'y'])\n            >>> col_d = Column(name='d', data=['u', 'v'])\n            >>> t.add_columns([col_c, col_d])\n            >>> print(t)\n             a   b   c   d\n            --- --- --- ---\n              1 0.1   x   u\n              2 0.2   y   v\n\n        Add column 'c' at position 0 and column 'd' at position 1.", "metadata": {"file_name": "astropy/table/table.py", "File Name": "astropy/table/table.py", "Classes": "TableReplaceWarning, TableColumns, TableAttribute, PprintIncludeExclude, Table, QTable, _Context", "Functions": "descr, has_info_class, _get_names_from_list_of_dict, _encode_mixins"}}, {"code": "# Licensed under a 3-clause BSD style license - see LICENSE.rst\n\"\"\"\n``showtable`` is a command-line script based on ``astropy.io`` and\n``astropy.table`` for printing ASCII, FITS, HDF5 or VOTable files(s) to the\nstandard output.\n\nExample usage of ``showtable``:\n\n1. FITS::\n\n    $ showtable astropy/io/fits/tests/data/table.fits\n\n     target V_mag\n    ------- -----\n    NGC1001  11.1\n    NGC1002  12.3\n    NGC1003  15.2\n\n2. ASCII::\n\n    $ showtable astropy/io/ascii/tests/t/simple_csv.csv\n\n     a   b   c\n    --- --- ---\n      1   2   3\n      4   5   6\n\n3. XML::\n\n    $ showtable astropy/io/votable/tests/data/names.xml --max-width 70\n\n               col1             col2     col3  ... col15 col16 col17\n               ---              deg      deg   ...  mag   mag   ---\n    ------------------------- -------- ------- ... ----- ----- -----\n    SSTGLMC G000.0000+00.1611   0.0000  0.1611 ...    --    --    AA\n\n\n\n4. Print all the FITS tables in the current directory::\n\n    $ showtable *.fits\n\n\"\"\"\n\nimport argparse\nimport textwrap\nimport warnings\n\nfrom astropy import log\nfrom astropy.table import Table\nfrom astropy.utils.exceptions import AstropyUserWarning", "metadata": {"file_name": "astropy/table/scripts/showtable.py", "File Name": "astropy/table/scripts/showtable.py", "Functions": "showtable, main"}}, {"code": "reverse : bool\n            Sort in reverse order (default=False)\n\n        Examples\n        --------\n        Create a table with 3 columns::\n\n            >>> t = Table([['Max', 'Jo', 'John'], ['Miller', 'Miller', 'Jackson'],\n            ...            [12, 15, 18]], names=('firstname', 'name', 'tel'))\n            >>> print(t)\n            firstname   name  tel\n            --------- ------- ---\n                  Max  Miller  12\n                   Jo  Miller  15\n                 John Jackson  18\n\n        Sorting according to standard sorting rules, first 'name' then 'firstname'::\n\n            >>> t.sort(['name', 'firstname'])\n            >>> print(t)\n            firstname   name  tel\n            --------- ------- ---\n                 John Jackson  18\n                   Jo  Miller  15\n                  Max  Miller  12\n\n        Sorting according to standard sorting rules, first 'firstname' then 'tel',\n        in reverse order::\n\n            >>> t.sort(['firstname', 'tel'], reverse=True)\n            >>> print(t)\n            firstname   name  tel\n            --------- ------- ---\n                  Max  Miller  12\n                 John Jackson  18\n                   Jo  Miller  15\n        \"\"\"\n        if keys is None:\n            if not self.indices:\n                raise ValueError(\"Table sort requires input keys or a table index\")\n            keys = [x.info.name for x in self.indices[0].columns]\n\n        if isinstance(keys, str):\n            keys = [keys]\n\n        indexes = self.argsort(keys, kind=kind, reverse=reverse)\n\n        with self.index_mode(\"freeze\"):\n            for name, col in self.columns.items():\n                # Make a new sorted column.  This requires that take() also copies\n                # relevant info attributes for mixin columns.\n                new_col = col.take(indexes, axis=0)\n\n                # First statement in try: will succeed if the column supports an in-place\n                # update, and matches the legacy behavior of astropy Table.  However,\n                # some mixin classes may not support this, so in that case just drop\n                # in the entire new column. See #9553 and #9536 for discussion.", "metadata": {"file_name": "astropy/table/table.py", "File Name": "astropy/table/table.py", "Classes": "TableReplaceWarning, TableColumns, TableAttribute, PprintIncludeExclude, Table, QTable, _Context", "Functions": "descr, has_info_class, _get_names_from_list_of_dict, _encode_mixins"}}, {"code": "If data are not list of dict then this is None.\n        names_from_list_of_dict = None\n\n        # Row-oriented input, e.g. list of lists or list of tuples, list of\n        # dict, Row instance.  Set data to something that the subsequent code\n        # will parse correctly.\n        if rows is not None:\n            if data is not None:\n                raise ValueError(\"Cannot supply both `data` and `rows` values\")\n            if isinstance(rows, types.GeneratorType):\n                # Without this then the all(..) test below uses up the generator\n                rows = list(rows)\n\n            # Get column names if `rows` is a list of dict, otherwise this is None\n            names_from_list_of_dict = _get_names_from_list_of_dict(rows)\n            if names_from_list_of_dict:\n                data = rows\n            elif isinstance(rows, self.Row):\n                data = rows\n            else:\n                data = list(zip(*rows))\n\n        # Infer the type of the input data and set up the initialization\n        # function, number of columns, and potentially the default col names\n\n        default_names = None\n\n        # Handle custom (subclass) table attributes that are stored in meta.\n        # These are defined as class attributes using the TableAttribute\n        # descriptor.  Any such attributes get removed from kwargs here and\n        # stored for use after the table is otherwise initialized. Any values\n        # provided via kwargs will have precedence over existing values from\n        # meta (e.g. from data as a Table or meta via kwargs).\n        meta_table_attrs = {}\n        if kwargs:\n            for attr in list(kwargs):\n                descr = getattr(self.__class__, attr, None)\n                if isinstance(descr, TableAttribute):\n                    meta_table_attrs[attr] = kwargs.pop(attr)\n\n        if hasattr(data, \"__astropy_table__\"):\n            # Data object implements the __astropy_table__ interface method.\n            # Calling that method returns an appropriate instance of\n            # self.__class__ and respects the `copy` arg.  The returned\n            # Table object should NOT then be copied.", "metadata": {"file_name": "astropy/table/table.py", "File Name": "astropy/table/table.py", "Classes": "TableReplaceWarning, TableColumns, TableAttribute, PprintIncludeExclude, Table, QTable, _Context", "Functions": "descr, has_info_class, _get_names_from_list_of_dict, _encode_mixins"}}, {"code": "Note that\n        the columns are inserted before the given position::\n\n            >>> t = Table([[1, 2], [0.1, 0.2]], names=('a', 'b'))\n            >>> t.add_columns([['x', 'y'], ['u', 'v']], names=['c', 'd'],\n            ...               indexes=[0, 1])\n            >>> print(t)\n             c   a   d   b\n            --- --- --- ---\n              x   1   u 0.1\n              y   2   v 0.2\n\n        Add second column 'b' and column 'c' with ``rename_duplicate``::\n\n            >>> t = Table([[1, 2], [0.1, 0.2]], names=('a', 'b'))\n            >>> t.add_columns([[1.1, 1.2], ['x', 'y']], names=('b', 'c'),\n            ...               rename_duplicate=True)\n            >>> print(t)\n             a   b  b_1  c\n            --- --- --- ---\n              1 0.1 1.1  x\n              2 0.2 1.2  y\n\n        Add unnamed columns or mixin objects in the table using default names\n        or by specifying explicit names with ``names``.", "metadata": {"file_name": "astropy/table/table.py", "File Name": "astropy/table/table.py", "Classes": "TableReplaceWarning, TableColumns, TableAttribute, PprintIncludeExclude, Table, QTable, _Context", "Functions": "descr, has_info_class, _get_names_from_list_of_dict, _encode_mixins"}}, {"code": "# Licensed under a 3-clause BSD style license - see LICENSE.rst\n\"\"\"\nAn extensible ASCII table reader and writer.\n\nClasses to read DAOphot table format\n\n:Copyright: Smithsonian Astrophysical Observatory (2011)\n:Author: Tom Aldcroft (aldcroft@head.cfa.harvard.edu)\n\"\"\"\n\n\nimport itertools as itt\nimport re\nfrom collections import OrderedDict, defaultdict\n\nimport numpy as np\n\nfrom . import core, fixedwidth\nfrom .misc import first_false_index, first_true_index, groupmore\n\n\nclass DaophotHeader(core.BaseHeader):\n    \"\"\"\n    Read the header from a file produced by the IRAF DAOphot routine.\n    \"\"\"\n\n    comment = r\"\\s*#K\"\n\n    # Regex for extracting the format strings\n    re_format = re.compile(r\"%-?(\\d+)\\.?\\d?[sdfg]\")\n    re_header_keyword = re.compile(\n        r\"[#]K\" r\"\\s+ (?P<name> \\w+)\" r\"\\s* = (?P<stuff> .+) $\", re.VERBOSE\n    )\n    aperture_values = ()\n\n    def __init__(self):\n        core.BaseHeader.__init__(self)\n\n    def parse_col_defs(self, grouped_lines_dict):\n        \"\"\"Parse a series of column definition lines.\n\n        Examples\n        --------\n        When parsing, there may be several such blocks in a single file\n        (where continuation characters have already been stripped).", "metadata": {"file_name": "astropy/io/ascii/daophot.py", "File Name": "astropy/io/ascii/daophot.py", "Classes": "DaophotHeader, DaophotData, DaophotInputter, Daophot"}}, {"code": ">>> readme = \"data/vizier/ReadMe\"\n      >>> r = ascii.get_reader(ascii.Cds, readme=readme)\n      >>> table = r.read(\"data/vizier/table1.dat\")\n      >>> # table5.dat has the same ReadMe file\n      >>> table = r.read(\"data/vizier/table5.dat\")\n\n    If no ``readme`` parameter is specified, then the header\n    information is assumed to be at the top of the given table.\n\n      >>> r = ascii.get_reader(ascii.Cds)\n      >>> table = r.read(\"data/cds.dat\")\n      >>> #The following gives InconsistentTableError, since no\n      >>> #readme file was given and table1.dat does not have a header.\n      >>> table = r.read(\"data/vizier/table1.dat\")\n      Traceback (most recent call last):\n        ...\n      InconsistentTableError: No CDS section delimiter found\n\n    Caveats:\n\n    * The Units and Explanations are available in the column ``unit`` and\n      ``description`` attributes, respectively.\n    * The other metadata defined by this format is not available in the output table.\n    \"\"\"\n\n    _format_name = \"cds\"\n    _io_registry_format_aliases = [\"cds\"]\n    _io_registry_can_write = False\n    _description = \"CDS format table\"\n\n    data_class = CdsData\n    header_class = CdsHeader\n\n    def __init__(self, readme=None):\n        super().__init__()\n        self.header.readme = readme\n\n    def write(self, table=None):\n        \"\"\"Not available for the CDS class (raises NotImplementedError).\"\"\"\n        raise NotImplementedError\n\n    def read(self, table):\n        # If the read kwarg `data_start` is 'guess' then the table may have extraneous\n        # lines between the end of the header and the beginning of data.\n        if self.data.start_line == \"guess\":\n            # Replicate the first part of BaseReader.read up to the point where\n            # the table lines are initially read in.", "metadata": {"file_name": "astropy/io/ascii/cds.py", "File Name": "astropy/io/ascii/cds.py", "Classes": "CdsHeader, CdsData, Cds"}}, {"code": "# Licensed under a 3-clause BSD style license - see LICENSE.rst\n\nfrom astropy.io import registry\n\nfrom .info import serialize_method_as\n\n__all__ = [\"TableRead\", \"TableWrite\"]\n__doctest_skip__ = [\"TableRead\", \"TableWrite\"]\n\n\nclass TableRead(registry.UnifiedReadWrite):\n    \"\"\"Read and parse a data table and return as a Table.\n\n    This function provides the Table interface to the astropy unified I/O\n    layer.  This allows easily reading a file in many supported data formats\n    using syntax such as::\n\n      >>> from astropy.table import Table\n      >>> dat = Table.read('table.dat', format='ascii')\n      >>> events = Table.read('events.fits', format='fits')\n\n    Get help on the available readers for ``Table`` using the``help()`` method::\n\n      >>> Table.read.help()  # Get help reading Table and list supported formats\n      >>> Table.read.help('fits')  # Get detailed help on Table FITS reader\n      >>> Table.read.list_formats()  # Print list of available formats\n\n    See also: https://docs.astropy.org/en/stable/io/unified.html\n\n    Parameters\n    ----------\n    *args : tuple, optional\n        Positional arguments passed through to data reader. If supplied the\n        first argument is typically the input filename.\n    format : str\n        File format specifier.\n    units : list, dict, optional\n        List or dict of units to apply to columns\n    descriptions : list, dict, optional\n        List or dict of descriptions to apply to columns\n    **kwargs : dict, optional\n        Keyword arguments passed through to data reader.", "metadata": {"file_name": "astropy/table/connect.py", "File Name": "astropy/table/connect.py", "Classes": "TableRead, TableWrite"}}, {"code": "_pformat_docs = \"\"\"\n    {__doc__}\n\n    Parameters\n    ----------\n    max_lines : int or None\n        Maximum number of rows to output\n\n    max_width : int or None\n        Maximum character width of output\n\n    show_name : bool\n        Include a header row for column names. Default is True.\n\n    show_unit : bool\n        Include a header row for unit.  Default is to show a row\n        for units only if one or more columns has a defined value\n        for the unit.\n\n    show_dtype : bool\n        Include a header row for column dtypes. Default is True.\n\n    html : bool\n        Format the output as an HTML table. Default is False.\n\n    tableid : str or None\n        An ID tag for the table; only used if html is set.  Default is\n        \"table{id}\", where id is the unique integer id of the table object,\n        id(self)\n\n    align : str or list or tuple or None\n        Left/right alignment of columns. Default is right (None) for all\n        columns. Other allowed values are '>', '<', '^', and '0=' for\n        right, left, centered, and 0-padded, respectively. A list of\n        strings can be provided for alignment of tables with multiple\n        columns.\n\n    tableclass : str or list of str or None\n        CSS classes for the table; only used if html is set.  Default is\n        None.\n\n    Returns\n    -------\n    lines : list\n        Formatted table as a list of strings.\n    \"\"\"\n\n\nclass TableReplaceWarning(UserWarning):\n    \"\"\"\n    Warning class for cases when a table column is replaced via the\n    Table.__setitem__ syntax e.g. t['a'] = val.\n\n    This does not inherit from AstropyWarning because we want to use\n    stacklevel=3 to show the user where the issue occurred in their code.\n    \"\"\"\n\n    pass", "metadata": {"file_name": "astropy/table/table.py", "File Name": "astropy/table/table.py", "Classes": "TableReplaceWarning, TableColumns, TableAttribute, PprintIncludeExclude, Table, QTable, _Context", "Functions": "descr, has_info_class, _get_names_from_list_of_dict, _encode_mixins"}}, {"code": "Returns\n    -------\n    writer : `~astropy.io.ascii.BaseReader` subclass\n        ASCII format writer instance\n    \"\"\"\n    if Writer is None:\n        Writer = basic.Basic\n    if \"strip_whitespace\" not in kwargs:\n        kwargs[\"strip_whitespace\"] = True\n    writer = core._get_writer(Writer, fast_writer, **kwargs)\n\n    # Handle the corner case of wanting to disable writing table comments for the\n    # commented_header format.  This format *requires* a string for `write_comment`\n    # because that is used for the header column row, so it is not possible to\n    # set the input `comment` to None.  Without adding a new keyword or assuming\n    # a default comment character, there is no other option but to tell user to\n    # simply remove the meta['comments'].\n    if isinstance(\n        writer, (basic.CommentedHeader, fastbasic.FastCommentedHeader)\n    ) and not isinstance(kwargs.get(\"comment\", \"\"), str):\n        raise ValueError(\n            \"for the commented_header writer you must supply a string\\n\"\n            \"value for the `comment` keyword.  In order to disable writing\\n\"\n            \"table comments use `del t.meta['comments']` prior to writing.\"\n        )\n\n    return writer\n\n\ndef write(\n    table,\n    output=None,\n    format=None,\n    Writer=None,\n    fast_writer=True,\n    *,\n    overwrite=False,\n    **kwargs,\n):\n    # Docstring inserted below\n\n    _validate_read_write_kwargs(\n        \"write\", format=format, fast_writer=fast_writer, overwrite=overwrite, **kwargs\n    )\n\n    if isinstance(output, (str, bytes, os.PathLike)):\n        output = os.path.expanduser(output)\n        if not overwrite and os.path.lexists(output):\n            raise OSError(NOT_OVERWRITING_MSG.format(output))\n\n    if output is None:\n        output = sys.stdout\n\n    # Ensure that `table` is a Table subclass.", "metadata": {"file_name": "astropy/io/ascii/ui.py", "File Name": "astropy/io/ascii/ui.py", "Functions": "_probably_html, set_guess, get_reader, _get_format_class, _get_fast_reader_dict, _validate_read_write_kwargs, _expand_user_if_path, read, _guess, _get_guess_kwargs_list, _read_in_chunks, _read_in_chunks_generator, get_writer, write, get_read_trace, is_ducktype, passthrough_fileobj"}}, {"code": "# Licensed under a 3-clause BSD style license - see LICENSE.rst\n\"\"\"An extensible ASCII table reader and writer.\n\nfixedwidth.py:\n  Read or write a table with fixed width columns.\n\n:Copyright: Smithsonian Astrophysical Observatory (2011)\n:Author: Tom Aldcroft (aldcroft@head.cfa.harvard.edu)\n\"\"\"\n\n\nfrom . import basic, core\nfrom .core import DefaultSplitter, InconsistentTableError", "metadata": {"file_name": "astropy/io/ascii/fixedwidth.py", "File Name": "astropy/io/ascii/fixedwidth.py", "Classes": "FixedWidthSplitter, FixedWidthHeaderSplitter, FixedWidthHeader, FixedWidthData, FixedWidth, FixedWidthNoHeaderHeader, FixedWidthNoHeaderData, FixedWidthNoHeader, FixedWidthTwoLineHeader, FixedWidthTwoLineDataSplitter, FixedWidthTwoLineData, FixedWidthTwoLine"}}, {"code": "value = []\n            node = yaml.MappingNode(tag, value, flow_style=flow_style)\n            if self.alias_key is not None:\n                self.represented_objects[self.alias_key] = node\n            best_style = True\n            if hasattr(mapping, \"items\"):\n                mapping = mapping.items()\n                if hasattr(mapping, \"sort\"):\n                    mapping.sort()\n                else:\n                    mapping = list(mapping)\n                    try:\n                        mapping = sorted(mapping)\n                    except TypeError:\n                        pass\n\n            for item_key, item_value in mapping:\n                node_key = self.represent_data(item_key)\n                node_value = self.represent_data(item_value)\n                if not (isinstance(node_key, yaml.ScalarNode) and not node_key.style):\n                    best_style = False\n                if not (\n                    isinstance(node_value, yaml.ScalarNode) and not node_value.style\n                ):\n                    best_style = False\n                value.append((node_key, node_value))\n            if flow_style is None:\n                if self.default_flow_style is not None:\n                    node.flow_style = self.default_flow_style\n                else:\n                    node.flow_style = best_style\n            return node\n\n    TableDumper.add_representer(OrderedDict, _repr_odict)\n    TableDumper.add_representer(ColumnDict, _repr_column_dict)\n\n    header = copy.copy(header)  # Don't overwrite original\n    header[\"datatype\"] = [_get_col_attributes(col) for col in header[\"cols\"]]\n    del header[\"cols\"]\n\n    lines = yaml.dump(\n        header, default_flow_style=None, Dumper=TableDumper, width=130\n    ).splitlines()\n    return lines\n\n\nclass YamlParseError(Exception):\n    pass", "metadata": {"file_name": "astropy/table/meta.py", "File Name": "astropy/table/meta.py", "Classes": "ColumnOrderList, ColumnDict, YamlParseError, ConvertError, TableDumper, TableLoader", "Functions": "_construct_odict, _repr_pairs, _repr_odict, _repr_column_dict, _get_variable_length_array_shape, _get_datatype_from_dtype, _get_col_attributes, get_yaml_from_table, get_yaml_from_header, get_header_from_yaml"}}, {"code": "if \"fill_values\" in kwargs and kwargs[\"fill_values\"] is None:\n        del kwargs[\"fill_values\"]\n\n    if issubclass(Writer, FastBasic):  # Fast writers handle args separately\n        return Writer(**kwargs)\n    elif fast_writer and f\"fast_{Writer._format_name}\" in FAST_CLASSES:\n        # Switch to fast writer\n        kwargs[\"fast_writer\"] = fast_writer\n        return FAST_CLASSES[f\"fast_{Writer._format_name}\"](**kwargs)\n\n    writer_kwargs = {k: v for k, v in kwargs.items() if k not in extra_writer_pars}\n    writer = Writer(**writer_kwargs)\n\n    if \"delimiter\" in kwargs:\n        writer.header.splitter.delimiter = kwargs[\"delimiter\"]\n        writer.data.splitter.delimiter = kwargs[\"delimiter\"]\n    if \"comment\" in kwargs:\n        writer.header.write_comment = kwargs[\"comment\"]\n        writer.data.write_comment = kwargs[\"comment\"]\n    if \"quotechar\" in kwargs:\n        writer.header.splitter.quotechar = kwargs[\"quotechar\"]\n        writer.data.splitter.quotechar = kwargs[\"quotechar\"]\n    if \"formats\" in kwargs:\n        writer.data.formats = kwargs[\"formats\"]\n    if \"strip_whitespace\" in kwargs:\n        if kwargs[\"strip_whitespace\"]:\n            # Restore the default SplitterClass process_val method which strips\n            # whitespace.  This may have been changed in the Writer\n            # initialization (e.g.", "metadata": {"file_name": "astropy/io/ascii/core.py", "File Name": "astropy/io/ascii/core.py", "Classes": "CsvWriter, MaskedConstant, InconsistentTableError, OptionalTableImportError, ParameterError, FastOptionsError, NoType, StrType, NumType, FloatType, BoolType, IntType, AllType, Column, BaseInputter, BaseSplitter, DefaultSplitter, BaseHeader, BaseData, BaseOutputter, TableOutputter, MetaBaseReader, BaseReader, ContinuationLinesInputter, WhitespaceSplitter", "Functions": "_check_multidim_table, _replace_tab_with_space, _get_line_index, convert_numpy, _deduplicate_names, _is_number, _apply_include_exclude_names, _get_reader, _get_writer, bool_converter, generic_converter, rename_columns"}}, {"code": "class WhitespaceSplitter(DefaultSplitter):\n    def process_line(self, line):\n        \"\"\"Replace tab with space within ``line`` while respecting quoted substrings.\"\"\"\n        newline = []\n        in_quote = False\n        lastchar = None\n        for char in line:\n            if char == self.quotechar and (\n                self.escapechar is None or lastchar != self.escapechar\n            ):\n                in_quote = not in_quote\n            if char == \"\\t\" and not in_quote:\n                char = \" \"\n            lastchar = char\n            newline.append(char)\n\n        return \"\".join(newline)\n\n\nextra_reader_pars = (\n    \"Reader\",\n    \"Inputter\",\n    \"Outputter\",\n    \"delimiter\",\n    \"comment\",\n    \"quotechar\",\n    \"header_start\",\n    \"data_start\",\n    \"data_end\",\n    \"converters\",\n    \"encoding\",\n    \"data_Splitter\",\n    \"header_Splitter\",\n    \"names\",\n    \"include_names\",\n    \"exclude_names\",\n    \"strict_names\",\n    \"fill_values\",\n    \"fill_include_names\",\n    \"fill_exclude_names\",\n)\n\n\ndef _get_reader(Reader, Inputter=None, Outputter=None, **kwargs):\n    \"\"\"Initialize a table reader allowing for common customizations.  See ui.get_reader()\n    for param docs.  This routine is for internal (package) use only and is useful\n    because it depends only on the \"core\" module.\n    \"\"\"", "metadata": {"file_name": "astropy/io/ascii/core.py", "File Name": "astropy/io/ascii/core.py", "Classes": "CsvWriter, MaskedConstant, InconsistentTableError, OptionalTableImportError, ParameterError, FastOptionsError, NoType, StrType, NumType, FloatType, BoolType, IntType, AllType, Column, BaseInputter, BaseSplitter, DefaultSplitter, BaseHeader, BaseData, BaseOutputter, TableOutputter, MetaBaseReader, BaseReader, ContinuationLinesInputter, WhitespaceSplitter", "Functions": "_check_multidim_table, _replace_tab_with_space, _get_line_index, convert_numpy, _deduplicate_names, _is_number, _apply_include_exclude_names, _get_reader, _get_writer, bool_converter, generic_converter, rename_columns"}}, {"code": "Any column values of INDEF are interpreted as a missing value and will be\n    masked out in the resultant table.\n\n    In case of multi-aperture daophot files containing repeated entries for the last\n    row of fields, extra unique column names will be created by suffixing\n    corresponding field names with numbers starting from 2 to N (where N is the\n    total number of apertures).\n    For example,\n    first aperture radius will be RAPERT and corresponding magnitude will be MAG,\n    second aperture radius will be RAPERT2 and corresponding magnitude will be MAG2,\n    third aperture radius will be RAPERT3 and corresponding magnitude will be MAG3,\n    and so on.\n\n    \"\"\"\n\n    _format_name = \"daophot\"\n    _io_registry_format_aliases = [\"daophot\"]\n    _io_registry_can_write = False\n    _description = \"IRAF DAOphot format table\"\n\n    header_class = DaophotHeader\n    data_class = DaophotData\n    inputter_class = DaophotInputter\n\n    table_width = 80\n\n    def __init__(self):\n        core.BaseReader.__init__(self)\n        # The inputter needs to know about the data (see DaophotInputter.process_lines)\n        self.inputter.data = self.data\n\n    def write(self, table=None):\n        raise NotImplementedError", "metadata": {"file_name": "astropy/io/ascii/daophot.py", "File Name": "astropy/io/ascii/daophot.py", "Classes": "DaophotHeader, DaophotData, DaophotInputter, Daophot"}}, {"code": "data = data.__astropy_table__(self.__class__, copy, **kwargs)\n            copy = False\n        elif kwargs:\n            raise TypeError(\n                f\"__init__() got unexpected keyword argument {list(kwargs.keys())[0]!r}\"\n            )\n\n        if isinstance(data, np.ndarray) and data.shape == (0,) and not data.dtype.names:\n            data = None\n\n        if isinstance(data, self.Row):\n            data = data._table[data._index : data._index + 1]\n\n        if isinstance(data, (list, tuple)):\n            # Get column names from `data` if it is a list of dict, otherwise this is None.\n            # This might be previously defined if `rows` was supplied as an init arg.\n            names_from_list_of_dict = (\n                names_from_list_of_dict or _get_names_from_list_of_dict(data)\n            )\n            if names_from_list_of_dict:\n                init_func = self._init_from_list_of_dicts\n                n_cols = len(names_from_list_of_dict)\n            else:\n                init_func = self._init_from_list\n                n_cols = len(data)\n\n        elif isinstance(data, np.ndarray):\n            if data.dtype.names:\n                init_func = self._init_from_ndarray  # _struct\n                n_cols = len(data.dtype.names)\n                default_names = data.dtype.names\n            else:\n                init_func = self._init_from_ndarray  # _homog\n                if data.shape == ():\n                    raise ValueError(\"Can not initialize a Table with a scalar\")\n                elif len(data.shape) == 1:\n                    data = data[np.newaxis, :]\n                n_cols = data.shape[1]\n\n        elif isinstance(data, Mapping):\n            init_func = self._init_from_dict\n            default_names = list(data)\n            n_cols = len(default_names)\n\n        elif isinstance(data, Table):\n            # If user-input meta is None then use data.meta (if non-trivial)\n            if meta is None and data.meta:\n                # At this point do NOT deepcopy data.meta as this will happen after\n                # table init_func() is called.", "metadata": {"file_name": "astropy/table/table.py", "File Name": "astropy/table/table.py", "Classes": "TableReplaceWarning, TableColumns, TableAttribute, PprintIncludeExclude, Table, QTable, _Context", "Functions": "descr, has_info_class, _get_names_from_list_of_dict, _encode_mixins"}}, {"code": "By default, any character\n    below a ``|`` will be ignored (since this is the current standard),\n    but if you need to read files that assume characters below the ``|``\n    symbols belong to the column before or after the ``|``, you can specify\n    ``definition='left'`` or ``definition='right'`` respectively when reading\n    the table (the default is ``definition='ignore'``). The following examples\n    demonstrate the different conventions:\n\n    * ``definition='ignore'``::\n\n        |   ra  |  dec  |\n        | float | float |\n          1.2345  6.7890\n\n    * ``definition='left'``::\n\n        |   ra  |  dec  |\n        | float | float |\n           1.2345  6.7890\n\n    * ``definition='right'``::\n\n        |   ra  |  dec  |\n        | float | float |\n        1.2345  6.7890\n\n    IPAC tables can specify a null value in the header that is shown in place\n    of missing or bad data. On writing, this value defaults to ``null``.", "metadata": {"file_name": "astropy/io/ascii/ipac.py", "File Name": "astropy/io/ascii/ipac.py", "Classes": "IpacFormatErrorDBMS, IpacFormatError, IpacHeaderSplitter, IpacHeader, IpacDataSplitter, IpacData, Ipac", "Functions": "process_keyword_value"}}, {"code": "increase search depth return self.search_multiline( lines,\n        # depth=2*depth )\n\n        # indexing now relative to line[0]\n        markers = np.cumsum([data_start, first_special, last_special])\n        # multiline portion of first data block\n        multiline_block = lines[markers[1] : markers[-1]]\n\n        return markers, multiline_block, header_lines\n\n    def process_lines(self, lines):\n        markers, block, header = self.search_multiline(lines)\n        self.data.is_multiline = markers is not None\n        self.data.markers = markers\n        self.data.first_block = block\n        # set the header lines returned by the search as a attribute of the header\n        self.data.header.lines = header\n\n        if markers is not None:\n            lines = lines[markers[0] :]\n\n        continuation_char = self.continuation_char\n        multiline_char = self.multiline_char\n        replace_char = self.replace_char\n\n        parts = []\n        outlines = []\n        for i, line in enumerate(lines):\n            mo = self.re_multiline.search(line)\n            if mo:\n                comment, special, cont = mo.groups()\n                if comment or cont:\n                    line = line.replace(continuation_char, replace_char)\n                if special:\n                    line = line.replace(multiline_char, replace_char)\n                if cont and not comment:\n                    parts.append(line)\n                if not cont:\n                    parts.append(line)\n                    outlines.append(\"\".join(parts))\n                    parts = []\n            else:\n                raise core.InconsistentTableError(\n                    f\"multiline re could not match line {i}: {line}\"\n                )\n\n        return outlines\n\n\nclass Daophot(core.BaseReader):\n    \"\"\"\n    DAOphot format table.\n\n    Example::\n\n      #K MERGERAD   = INDEF                   scaleunit  %-23.7g\n      #K IRAF = NOAO/IRAFV2.", "metadata": {"file_name": "astropy/io/ascii/daophot.py", "File Name": "astropy/io/ascii/daophot.py", "Classes": "DaophotHeader, DaophotData, DaophotInputter, Daophot"}}, {"code": "# We have to do it this late, because the fill_value\n        # defined in the class can be overwritten by ui.write\n        self.data.fill_values.append((core.masked, \"null\"))\n\n        # Check column names before altering\n        self.header.cols = list(table.columns.values())\n        self.header.check_column_names(self.names, self.strict_names, self.guessing)\n\n        core._apply_include_exclude_names(\n            table, self.names, self.include_names, self.exclude_names\n        )\n\n        # Check that table has only 1-d columns.\n        self._check_multidim_table(table)\n\n        # Now use altered columns\n        new_cols = list(table.columns.values())\n        # link information about the columns to the writer object (i.e. self)\n        self.header.cols = new_cols\n        self.data.cols = new_cols\n\n        # Write header and data to lines list\n        lines = []\n        # Write meta information\n        if \"comments\" in table.meta:\n            for comment in table.meta[\"comments\"]:\n                if len(str(comment)) > 78:\n                    warn(\n                        \"Comment string > 78 characters was automatically wrapped.\",\n                        AstropyUserWarning,\n                    )\n                for line in wrap(\n                    str(comment), 80, initial_indent=\"\\\\ \", subsequent_indent=\"\\\\ \"\n                ):\n                    lines.append(line)\n        if \"keywords\" in table.meta:\n            keydict = table.meta[\"keywords\"]\n            for keyword in keydict:\n                try:\n                    val = keydict[keyword][\"value\"]\n                    lines.append(f\"\\\\{keyword.strip()}={val!r}\")\n                    # meta is not standardized: Catch some common Errors.\n                except TypeError:\n                    warn(\n                        f\"Table metadata keyword {keyword} has been skipped.  \"\n                        \"IPAC metadata must be in the form {{'keywords':\"\n                        \"{{'keyword': {{'value': value}} }}\",\n                        AstropyUserWarning,\n                    )\n        ignored_keys = [\n            key for key in table.meta if key not in (\"keywords\", \"comments\")\n        ]\n        if any(ignored_keys):\n            warn(\n                f\"Table metadata keyword(s) {ignored_keys} were not written.  \"", "metadata": {"file_name": "astropy/io/ascii/ipac.py", "File Name": "astropy/io/ascii/ipac.py", "Classes": "IpacFormatErrorDBMS, IpacFormatError, IpacHeaderSplitter, IpacHeader, IpacDataSplitter, IpacData, Ipac", "Functions": "process_keyword_value"}}, {"code": "data_Splitter : `~astropy.io.ascii.BaseSplitter`\n        Splitter class to split data columns\n    header_Splitter : `~astropy.io.ascii.BaseSplitter`\n        Splitter class to split header columns\n    names : list\n        List of names corresponding to each data column\n    include_names : list\n        List of names to include in output.\n    exclude_names : list\n        List of names to exclude from output (applied after ``include_names``)\n    fill_values : tuple, list of tuple\n        specification of fill values for bad or missing table values\n    fill_include_names : list\n        List of names to include in fill_values.\n    fill_exclude_names : list\n        List of names to exclude from fill_values (applied after ``fill_include_names``)\n    fast_reader : bool, str or dict\n        Whether to use the C engine, can also be a dict with options which\n        defaults to `False`; parameters for options dict:\n\n        use_fast_converter: bool\n            enable faster but slightly imprecise floating point conversion method\n        parallel: bool or int\n            multiprocessing conversion using ``cpu_count()`` or ``'number'`` processes\n        exponent_style: str\n            One-character string defining the exponent or ``'Fortran'`` to auto-detect\n            Fortran-style scientific notation like ``'3.14159D+00'`` (``'E'``, ``'D'``, ``'Q'``),\n            all case-insensitive; default ``'E'``, all other imply ``use_fast_converter``\n        chunk_size : int\n            If supplied with a value > 0 then read the table in chunks of\n            approximately ``chunk_size`` bytes. Default is reading table in one pass.\n        chunk_generator : bool\n            If True and ``chunk_size > 0`` then return an iterator that returns a\n            table for each chunk.  The default is to return a single stacked table\n            for all the chunks.\n\n    encoding : str\n        Allow to specify encoding to read the file (default= ``None``).\n\n    Returns\n    -------\n    dat : `~astropy.table.Table` or <generator>\n        Output table\n\n    \"\"\"\n\n# Specify allowed types for core write() keyword arguments.  Each entry\n# corresponds to the name of an argument and either a type (e.g.", "metadata": {"file_name": "astropy/io/ascii/docs.py", "File Name": "astropy/io/ascii/docs.py"}}, {"code": "The behavior of ``copy`` for Column objects is:\n        - copy=True: new class instance with a copy of data and deep copy of meta\n        - copy=False: new class instance with same data and a key-only copy of meta\n\n        For mixin columns:\n        - copy=True: new class instance with copy of data and deep copy of meta\n        - copy=False: original instance (no copy at all)\n\n        Parameters\n        ----------\n        data : object (column-like sequence)\n            Input column data\n        copy : bool\n            Make a copy\n        default_name : str\n            Default name\n        dtype : np.dtype or None\n            Data dtype\n        name : str or None\n            Column name\n\n        Returns\n        -------\n        col : Column, MaskedColumn, mixin-column type\n            Object that can be used as a column in self\n        \"\"\"\n        data_is_mixin = self._is_mixin_for_table(data)\n        masked_col_cls = (\n            self.ColumnClass\n            if issubclass(self.ColumnClass, self.MaskedColumn)\n            else self.MaskedColumn\n        )\n\n        try:\n            data0_is_mixin = self._is_mixin_for_table(data[0])\n        except Exception:\n            # Need broad exception, cannot predict what data[0] raises for arbitrary data\n            data0_is_mixin = False\n\n        # If the data is not an instance of Column or a mixin class, we can\n        # check the registry of mixin 'handlers' to see if the column can be\n        # converted to a mixin class\n        if (handler := get_mixin_handler(data)) is not None:\n            original_data = data\n            data = handler(data)\n            if not (data_is_mixin := self._is_mixin_for_table(data)):\n                fully_qualified_name = (\n                    original_data.__class__.__module__\n                    + \".\"\n                    + original_data.__class__.__name__\n                )\n                raise TypeError(\n                    \"Mixin handler for object of type \"\n                    f\"{fully_qualified_name} \"\n                    \"did not return a valid mixin column\"\n                )\n\n        # Get the final column name using precedence.", "metadata": {"file_name": "astropy/table/table.py", "File Name": "astropy/table/table.py", "Classes": "TableReplaceWarning, TableColumns, TableAttribute, PprintIncludeExclude, Table, QTable, _Context", "Functions": "descr, has_info_class, _get_names_from_list_of_dict, _encode_mixins"}}, {"code": "", "metadata": {"file_name": "astropy/table/scripts/__init__.py", "File Name": "astropy/table/scripts/__init__.py"}}, {"code": "But for table input the table meta\n                # gets a key copy here if copy=False because later a direct object ref\n                # is used.\n                meta = data.meta if copy else data.meta.copy()\n\n            # Handle indices on input table. Copy primary key and don't copy indices\n            # if the input Table is in non-copy mode.\n            self.primary_key = data.primary_key\n            self._init_indices = self._init_indices and data._copy_indices\n\n            # Extract default names, n_cols, and then overwrite ``data`` to be the\n            # table columns so we can use _init_from_list.\n            default_names = data.colnames\n            n_cols = len(default_names)\n            data = list(data.columns.values())\n\n            init_func = self._init_from_list\n\n        elif data is None:\n            if names is None:\n                if dtype is None:\n                    # Table was initialized as `t = Table()`. Set up for empty\n                    # table with names=[], data=[], and n_cols=0.\n                    # self._init_from_list() will simply return, giving the\n                    # expected empty table.\n                    names = []\n                else:\n                    try:\n                        # No data nor names but dtype is available.  This must be\n                        # valid to initialize a structured array.\n                        dtype = np.dtype(dtype)\n                        names = dtype.names\n                        dtype = [dtype[name] for name in names]\n                    except Exception:\n                        raise ValueError(\n                            \"dtype was specified but could not be \"\n                            \"parsed for column names\"\n                        )\n            # names is guaranteed to be set at this point\n            init_func = self._init_from_list\n            n_cols = len(names)\n            data = [[]] * n_cols\n\n        else:\n            raise ValueError(f\"Data type {type(data)} not allowed to init Table\")\n\n        # Set up defaults if names and/or dtype are not specified.\n        # A value of None means the actual value will be inferred\n        # within the appropriate initialization routine, either from\n        # existing specification or auto-generated.", "metadata": {"file_name": "astropy/table/table.py", "File Name": "astropy/table/table.py", "Classes": "TableReplaceWarning, TableColumns, TableAttribute, PprintIncludeExclude, Table, QTable, _Context", "Functions": "descr, has_info_class, _get_names_from_list_of_dict, _encode_mixins"}}, {"code": "def _check_multidim_table(table, max_ndim):\n    \"\"\"Check that ``table`` has only columns with ndim <= ``max_ndim``.\n\n    Currently ECSV is the only built-in format that supports output of arbitrary\n    N-d columns, but HTML supports 2-d.\n    \"\"\"\n    # No limit?\n    if max_ndim is None:\n        return\n\n    # Check for N-d columns\n    nd_names = [col.info.name for col in table.itercols() if len(col.shape) > max_ndim]\n    if nd_names:\n        raise ValueError(\n            f\"column(s) with dimension > {max_ndim} \"\n            \"cannot be be written with this format, try using 'ecsv' \"\n            \"(Enhanced CSV) format\"\n        )\n\n\nclass CsvWriter:\n    \"\"\"\n    Internal class to replace the csv writer ``writerow`` and ``writerows``\n    functions so that in the case of ``delimiter=' '`` and\n    ``quoting=csv.QUOTE_MINIMAL``, the output field value is quoted for empty\n    fields (when value == '').\n\n    This changes the API slightly in that the writerow() and writerows()\n    methods return the output written string instead of the length of\n    that string.\n\n    Examples\n    --------\n    >>> from astropy.io.ascii.core import CsvWriter\n    >>> writer = CsvWriter(delimiter=' ')\n    >>> print(writer.writerow(['hello', '', 'world']))\n    hello \"\" world\n    \"\"\"\n\n    # Random 16-character string that gets injected instead of any\n    # empty fields and is then replaced post-write with doubled-quotechar.", "metadata": {"file_name": "astropy/io/ascii/core.py", "File Name": "astropy/io/ascii/core.py", "Classes": "CsvWriter, MaskedConstant, InconsistentTableError, OptionalTableImportError, ParameterError, FastOptionsError, NoType, StrType, NumType, FloatType, BoolType, IntType, AllType, Column, BaseInputter, BaseSplitter, DefaultSplitter, BaseHeader, BaseData, BaseOutputter, TableOutputter, MetaBaseReader, BaseReader, ContinuationLinesInputter, WhitespaceSplitter", "Functions": "_check_multidim_table, _replace_tab_with_space, _get_line_index, convert_numpy, _deduplicate_names, _is_number, _apply_include_exclude_names, _get_reader, _get_writer, bool_converter, generic_converter, rename_columns"}}, {"code": "import serialize\n\n            # Convert any Time or TimeDelta columns and pay attention to masking\n            time_cols = [col for col in tbl.itercols() if isinstance(col, TimeBase)]\n            if time_cols:\n                # Make a light copy of table and clear any indices\n                new_cols = []\n                for col in tbl.itercols():\n                    new_col = (\n                        col_copy(col, copy_indices=False) if col.info.indices else col\n                    )\n                    new_cols.append(new_col)\n                tbl = tbl.__class__(new_cols, copy=False)\n\n                # Certain subclasses (e.g. TimeSeries) may generate new indices on\n                # table creation, so make sure there are no indices on the table.\n                for col in tbl.itercols():\n                    col.info.indices.clear()\n\n                for col in time_cols:\n                    if isinstance(col, TimeDelta):\n                        # Convert to nanoseconds (matches astropy datetime64 support)\n                        new_col = (col.sec * 1e9).astype(\"timedelta64[ns]\")\n                        nat = np.timedelta64(\"NaT\")\n                    else:\n                        new_col = col.datetime64.copy()\n                        nat = np.datetime64(\"NaT\")\n                    if col.masked:\n                        new_col[col.mask] = nat\n                    tbl[col.info.name] = new_col\n\n            # Convert the table to one with no mixins, only Column objects.\n            encode_tbl = serialize.represent_mixins_as_columns(tbl)\n            return encode_tbl\n\n        tbl = _encode_mixins(self)\n\n        badcols = [name for name, col in self.columns.items() if len(col.shape) > 1]\n        if badcols:\n            # fmt: off\n            raise ValueError(\n                f'Cannot convert a table with multidimensional columns to a '\n                f'pandas DataFrame.", "metadata": {"file_name": "astropy/table/table.py", "File Name": "astropy/table/table.py", "Classes": "TableReplaceWarning, TableColumns, TableAttribute, PprintIncludeExclude, Table, QTable, _Context", "Functions": "descr, has_info_class, _get_names_from_list_of_dict, _encode_mixins"}}, {"code": "See help(np.core.numerictypes)\n            rdb_type = \"S\" if col.info.dtype.kind in (\"S\", \"U\") else \"N\"\n            rdb_types.append(rdb_type)\n\n        lines.append(self.splitter.join(rdb_types))\n\n\nclass RdbData(TabData):\n    \"\"\"\n    Data reader for RDB data. Starts reading at line 2.\n    \"\"\"\n\n    start_line = 2\n\n\nclass Rdb(Tab):\n    \"\"\"Tab-separated file with an extra line after the column definition line that\n    specifies either numeric (N) or string (S) data.\n\n    See: https://www.drdobbs.com/rdb-a-unix-command-line-database/199101326\n\n    Example::\n\n      col1 <tab> col2 <tab> col3\n      N <tab> S <tab> N\n      1 <tab> 2 <tab> 5\n\n    \"\"\"\n\n    _format_name = \"rdb\"\n    _io_registry_format_aliases = [\"rdb\"]\n    _io_registry_suffix = \".rdb\"\n    _description = \"Tab-separated with a type definition header line\"\n\n    header_class = RdbHeader\n    data_class = RdbData", "metadata": {"file_name": "astropy/io/ascii/basic.py", "File Name": "astropy/io/ascii/basic.py", "Classes": "BasicHeader, BasicData, Basic, NoHeaderHeader, NoHeaderData, NoHeader, CommentedHeaderHeader, CommentedHeader, TabHeaderSplitter, TabDataSplitter, TabHeader, TabData, Tab, CsvSplitter, CsvHeader, CsvData, Csv, RdbHeader, RdbData, Rdb"}}, {"code": "class Conf(_config.ConfigNamespace):\n    \"\"\"\n    Configuration parameters for `astropy.table`.\n    \"\"\"\n\n    auto_colname = _config.ConfigItem(\n        \"col{0}\",\n        \"The template that determines the name of a column if it cannot be \"\n        \"determined. Uses new-style (format method) string formatting.\",\n        aliases=[\"astropy.table.column.auto_colname\"],\n    )\n    default_notebook_table_class = _config.ConfigItem(\n        \"table-striped table-bordered table-condensed\",\n        \"The table class to be used in Jupyter notebooks when displaying \"\n        \"tables (and not overridden). See <https://getbootstrap.com/css/#tables \"\n        \"for a list of useful bootstrap classes.\",\n    )\n    replace_warnings = _config.ConfigItem(\n        [],\n        \"List of conditions for issuing a warning when replacing a table \"\n        \"column using setitem, e.g. t['a'] = value.  Allowed options are \"\n        \"'always', 'slice', 'refcount', 'attributes'.\",\n        \"string_list\",\n    )\n    replace_inplace = _config.ConfigItem(\n        False,\n        \"Always use in-place update of a table column when using setitem, \"\n        \"e.g. t['a'] = value.  This overrides the default behavior of \"\n        \"replacing the column entirely with the new value when possible. \"\n        \"This configuration option will be deprecated and then removed in \"\n        \"subsequent major releases.\",\n    )", "metadata": {"file_name": "astropy/table/__init__.py", "File Name": "astropy/table/__init__.py", "Classes": "Conf"}}, {"code": "table_class : str or None\n            A string with a list of HTML classes used to style the table.\n            The special default string ('astropy-default') means that the string\n            will be retrieved from the configuration item\n            ``astropy.table.default_notebook_table_class``. Note that these\n            table classes may make use of bootstrap, as this is loaded with the\n            notebook.  See `this page <https://getbootstrap.com/css/#tables>`_\n            for the list of classes.\n        css : str\n            A valid CSS string declaring the formatting for the table. Defaults\n            to ``astropy.table.jsviewer.DEFAULT_CSS_NB``.\n        display_length : int, optional\n            Number or rows to show. Defaults to 50.\n        show_row_index : str or False\n            If this does not evaluate to False, a column with the given name\n            will be added to the version of the table that gets displayed.\n            This new column shows the index of the row in the table itself,\n            even when the displayed table is re-sorted by another column. Note\n            that if a column with this name already exists, this option will be\n            ignored. Defaults to \"idx\".\n\n        Notes\n        -----\n        Currently, unlike `show_in_browser` (with ``jsviewer=True``), this\n        method needs to access online javascript code repositories.  This is due\n        to modern browsers' limitations on accessing local files.  Hence, if you\n        call this method while offline (and don't have a cached version of\n        jquery and jquery.dataTables), you will not get the jsviewer features.\n        \"\"\"", "metadata": {"file_name": "astropy/table/table.py", "File Name": "astropy/table/table.py", "Classes": "TableReplaceWarning, TableColumns, TableAttribute, PprintIncludeExclude, Table, QTable, _Context", "Functions": "descr, has_info_class, _get_names_from_list_of_dict, _encode_mixins"}}], "traj": {"usage": {"embedding_tokens": 0}}}
